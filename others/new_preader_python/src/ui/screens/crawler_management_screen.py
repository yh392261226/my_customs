"""
çˆ¬å–ç®¡ç†å±å¹•
"""

import os
import glob
import subprocess
import platform
import asyncio
from send2trash import send2trash
from typing import Dict, Any, Optional, List, ClassVar, Set
from urllib.parse import unquote
from textual.screen import Screen
from textual.containers import Container, Vertical, Horizontal
from textual.widgets import Static, Button, Label, Input, Link, Header, Footer, LoadingIndicator, Select
from textual.widgets import DataTable, Log
from textual.app import ComposeResult
from textual import events, on
from textual.screen import ModalScreen
from textual.containers import Center, Middle, Horizontal, Vertical

from src.locales.i18n_manager import get_global_i18n
from src.themes.theme_manager import ThemeManager
from src.core.database_manager import DatabaseManager
from src.utils.logger import get_logger
from src.ui.dialogs.note_dialog import NoteDialog
from src.ui.dialogs.select_books_dialog import SelectBooksDialog
from src.utils.browser_tab_monitor import BrowserTabMonitor, BrowserType
from src.config.config_manager import ConfigManager
import threading
import time

logger = get_logger(__name__)

class LogViewerPopup(ModalScreen):
    """æ—¥å¿—æŸ¥çœ‹å™¨å¼¹çª—"""
    
    BINDINGS = [("escape", "close", get_global_i18n().t('crawler.close_popup')),
            ("q", "force_close", get_global_i18n().t('crawler.force_close')),
            ("e", "scroll_to_bottom", get_global_i18n().t('crawler.scroll_to_bottom')),
            ("c", "sync_close", get_global_i18n().t('crawler.sync_close'))]  # å°†åœ¨__init__ä¸­åŠ¨æ€è®¾ç½®
    
    DEFAULT_CSS = """
    LogViewerPopup {
        height: 80%;
        width: 80%;
        align: center middle;
    }
    
    LogViewerPopup > Container {
        width: 90%;  /* ä»85%å¢åŠ åˆ°90% */
        height: 90%;  /* ä»85%å¢åŠ åˆ°90% */
        align: center middle;
        background: $surface;
        border: solid $accent;
    }
    
    .log-popup-container {
        width: 85%;
        height: 85%;
        align: center middle;
        layout: vertical;
    }
    
    .log-popup-header {
        height: 1;  /* ä»3è¡Œå‡å°‘åˆ°2è¡Œ */
        background: $accent;
        color: $text;
        content-align: center middle;
    }
    
    .log-popup-controls {
        height: 4;  /* ä»3è¡Œå‡å°‘åˆ°2è¡Œ */
        margin: 0 0 1 0;
    }

    #close-log-btn, #toggle-scroll-btn, #clear-log-btn, #refresh-log-btn {
        margin: 0;
        padding: 0;
        border: none;
        height: 3;
    }
    
    .log-popup-content {
        height: 55%;  /* ä½¿ç”¨å¼¹æ€§å¸ƒå±€å æ®å‰©ä½™ç©ºé—´ */
        background: $background;
    }
    
    #log-viewer {
        width: 100%;
        height: 40%;
        background: $background;
        /* å¢åŠ å†…è¾¹è·ç¡®ä¿æ˜¾ç¤ºå®Œæ•´å†…å®¹ */
        padding: 5 1 5 1;  /* ä¸Š å³ ä¸‹ å·¦ - å¢åŠ åº•éƒ¨å†…è¾¹è· */
        border: solid $border;
        overflow: auto;
    }
    """
    
    def __init__(self, log_file_path: str):
        super().__init__()
        self.log_file_path = log_file_path
        self.auto_scroll = True
        self.last_position = 0
        self.file_watcher_task = None
        self.stop_watching = False
        
    def compose(self) -> ComposeResult:
        """ç»„åˆæ—¥å¿—æŸ¥çœ‹å™¨ç•Œé¢"""
        yield Header()
        yield Container(
            Vertical(
                # æ ‡é¢˜æ 
                Horizontal(
                    Label(f"ğŸ“‹ {get_global_i18n().t('crawler.viewing_logs')}: {os.path.basename(self.log_file_path)}", classes="log-popup-title"),
                    classes="log-popup-header"
                ),
                
                # æ§åˆ¶æŒ‰é’®
                Horizontal(
                    Button(get_global_i18n().t('common.close'), id="close-log-btn", variant="primary"),
                    Button(get_global_i18n().t('crawler.toggle_auto_scroll'), id="toggle-scroll-btn", variant="default"),
                    Button(get_global_i18n().t('crawler.clear_display'), id="clear-log-btn", variant="warning"),
                    Button(get_global_i18n().t('crawler.refresh_log'), id="refresh-log-btn", variant="success"),
                    classes="log-popup-controls"
                ),
                
                # æ—¥å¿—å†…å®¹åŒºåŸŸ
                
                Log(id="log-viewer", auto_scroll=True, max_lines=1000, classes="log-popup-content"),
                
                classes="log-popup-container"
            )
        )
        yield Footer()
    
    def on_mount(self) -> None:
        """å¼¹çª—æŒ‚è½½æ—¶çš„å›è°ƒ"""
        self._load_initial_log_content()
        self._start_file_watching()
        
        # è®¾ç½®ç„¦ç‚¹åˆ°å…³é—­æŒ‰é’®ï¼Œæ–¹ä¾¿æ“ä½œ
        try:
            self.query_one("#close-log-btn", Button).focus()
        except Exception:
            pass
        
        # ç¡®ä¿æ»šåŠ¨åˆ°åº•éƒ¨ï¼ˆè€ƒè™‘æ»šåŠ¨æ¡å½±å“ï¼‰ï¼Œæ›´å¿«å“åº”
        self.set_timer(0.05, self._ensure_scroll_to_bottom)
    
    async def on_unmount(self) -> None:
        """å¼¹çª—å¸è½½æ—¶çš„å›è°ƒ"""
        await self._stop_file_watching()
        
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """æŒ‰é’®æŒ‰ä¸‹äº‹ä»¶å¤„ç†"""
        if event.button.id == "close-log-btn":
            # å¼‚æ­¥å…³é—­
            asyncio.create_task(self._async_close())
        elif event.button.id == "toggle-scroll-btn":
            self._toggle_auto_scroll()
        elif event.button.id == "clear-log-btn":
            self._clear_log_content()
        elif event.button.id == "refresh-log-btn":
            self._refresh_log_content()
    
    async def _async_close(self) -> None:
        """å¼‚æ­¥å…³é—­å¼¹çª—"""
        try:
            await self._stop_file_watching()
            self.dismiss()
        except Exception as e:
            logger.error(f"å…³é—­æ—¥å¿—æŸ¥çœ‹å™¨å¤±è´¥: {e}")
            # å¼ºåˆ¶å…³é—­
            self._force_close()
    
    def _load_initial_log_content(self) -> None:
        """åŠ è½½åˆå§‹æ—¥å¿—å†…å®¹"""
        try:
            if os.path.exists(self.log_file_path):
                with open(self.log_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    log_viewer = self.query_one("#log-viewer", Log)
                    log_viewer.clear()
                    # åªæ˜¾ç¤ºæœ€å1000è¡Œ
                    lines = content.split('\n')
                    if len(lines) > 1000:
                        lines = lines[-1000:]
                    log_viewer.write('\n'.join(lines))
                    
                    # è®°å½•å½“å‰ä½ç½®
                    self.last_position = len(content.encode('utf-8'))
                    
                    # æ»šåŠ¨åˆ°åº•éƒ¨
                    if self.auto_scroll:
                        # å‡å°‘å»¶è¿Ÿï¼Œæ›´å¿«å“åº”ï¼Œå¹¶æ»šåŠ¨åˆ°çœŸæ­£çš„åº•éƒ¨
                        self.set_timer(0.02, lambda: log_viewer.scroll_end(animate=False))
                        # å†æ¬¡ç¡®ä¿æ»šåŠ¨åˆ°ç»å¯¹åº•éƒ¨
                        self.set_timer(0.05, lambda: log_viewer.scroll_end(animate=False))
            else:
                log_viewer = self.query_one("#log-viewer", Log)
                log_viewer.write(f"ğŸ“ {get_global_i18n().t('crawler.log_file_not_found')}: {self.log_file_path}")
        except Exception as e:
            logger.error(f"åŠ è½½æ—¥å¿—å†…å®¹å¤±è´¥: {e}")
            log_viewer = self.query_one("#log-viewer", Log)
            log_viewer.write(f"âŒ {get_global_i18n().t('crawler.load_log_failed')}: {e}")
    
    def _start_file_watching(self) -> None:
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§ä»»åŠ¡"""
        self.stop_watching = False
        self.file_watcher_task = asyncio.create_task(self._watch_file_changes())
    
    async def _stop_file_watching(self) -> None:
        """åœæ­¢æ–‡ä»¶ç›‘æ§"""
        try:
            self.stop_watching = True
            if self.file_watcher_task and not self.file_watcher_task.done():
                # å–æ¶ˆä»»åŠ¡ï¼Œç­‰å¾…å®ƒå®Œæˆ
                self.file_watcher_task.cancel()
                try:
                    await asyncio.wait_for(self.file_watcher_task, timeout=0.1)
                except asyncio.CancelledError:
                    pass  # ä»»åŠ¡è¢«æ­£å¸¸å–æ¶ˆ
                except asyncio.TimeoutError:
                    pass  # è¶…æ—¶ä¹Ÿæ²¡å…³ç³»ï¼Œä»»åŠ¡ä¼šè¢«å–æ¶ˆ
        except Exception as e:
            logger.error(f"åœæ­¢æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­å…³é—­æµç¨‹
    
    async def _watch_file_changes(self) -> None:
        """å¼‚æ­¥ç›‘æ§æ–‡ä»¶å˜åŒ–"""
        while not self.stop_watching:
            try:
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if self.stop_watching:
                    break
                    
                if os.path.exists(self.log_file_path):
                    current_size = os.path.getsize(self.log_file_path)
                    if current_size > self.last_position:
                        # æ–‡ä»¶æœ‰æ–°å†…å®¹
                        with open(self.log_file_path, 'r', encoding='utf-8') as f:
                            f.seek(self.last_position)
                            new_content = f.read()
                            
                            # æ›´æ–°UIï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­ï¼‰
                            try:
                                log_viewer = self.query_one("#log-viewer", Log)
                                log_viewer.write(new_content)
                                if self.auto_scroll:
                                    # å‡å°‘å»¶è¿Ÿï¼Œæ›´å¿«å“åº”ï¼Œæ»šåŠ¨åˆ°ç»å¯¹åº•éƒ¨
                                    self.set_timer(0.02, lambda: log_viewer.scroll_end(animate=False))
                                    # å†æ¬¡ç¡®ä¿æ»šåŠ¨åˆ°çœŸæ­£çš„åº•éƒ¨
                                    self.set_timer(0.05, lambda: log_viewer.scroll_end(animate=False))
                            except Exception:
                                # å¼¹çª—å¯èƒ½å·²ç»å…³é—­ï¼Œåœæ­¢ç›‘æ§
                                self.stop_watching = True
                                break
                            
                            self.last_position = current_size
                else:
                    self.last_position = 0
                    
            except Exception as e:
                logger.error(f"ç›‘æ§æ—¥å¿—æ–‡ä»¶å˜åŒ–å¤±è´¥: {e}")
                # å‡ºé”™æ—¶ä¹Ÿåœæ­¢ç›‘æ§ï¼Œé¿å…æ— é™å¾ªç¯
                self.stop_watching = True
                break
            
            # ä½¿ç”¨æ›´çŸ­çš„å¼‚æ­¥ç¡çœ ï¼Œå¯ä»¥è¢«å–æ¶ˆï¼Œæé«˜å“åº”é€Ÿåº¦
            try:
                await asyncio.sleep(0.2)  # ä»0.5ç§’å‡å°‘åˆ°0.2ç§’
            except asyncio.CancelledError:
                break
    
    def _toggle_auto_scroll(self) -> None:
        """åˆ‡æ¢è‡ªåŠ¨æ»šåŠ¨"""
        self.auto_scroll = not self.auto_scroll
        log_viewer = self.query_one("#log-viewer", Log)
        log_viewer.auto_scroll = self.auto_scroll
        
        # æ›´æ–°æŒ‰é’®æ–‡æœ¬
        toggle_btn = self.query_one("#toggle-scroll-btn", Button)
        toggle_btn.label = f"{get_global_i18n().t('crawler.auto_scroll')}: {'å¼€' if self.auto_scroll else 'å…³'}"
        
        if self.auto_scroll:
            self.app.notify(get_global_i18n().t('crawler.auto_scroll_enabled'))
            # ç«‹å³æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œè€ƒè™‘æ»šåŠ¨æ¡å½±å“ï¼Œæ›´å¿«å“åº”
            self.set_timer(0.02, lambda: log_viewer.scroll_end(animate=False))
            # å†æ¬¡ç¡®ä¿æ»šåŠ¨åˆ°çœŸæ­£çš„åº•éƒ¨
            self.set_timer(0.05, lambda: log_viewer.scroll_end(animate=False))
        else:
            self.app.notify(get_global_i18n().t('crawler.auto_scroll_disabled'))
        
        # å¦‚æœå¼€å¯è‡ªåŠ¨æ»šåŠ¨ï¼Œç«‹å³æ»šåŠ¨åˆ°åº•éƒ¨
        if self.auto_scroll:
            log_viewer.scroll_end(animate=False)
    
    def _clear_log_content(self) -> None:
        """æ¸…ç©ºæ—¥å¿—å†…å®¹æ˜¾ç¤º"""
        log_viewer = self.query_one("#log-viewer", Log)
        log_viewer.clear()
        log_viewer.write(f"ğŸ“ {get_global_i18n().t('crawler.log_cleared_message')}")
    
    def _refresh_log_content(self) -> None:
        """åˆ·æ–°æ—¥å¿—å†…å®¹"""
        self.last_position = 0  # é‡ç½®ä½ç½®ï¼Œé‡æ–°åŠ è½½å…¨éƒ¨å†…å®¹
        self._load_initial_log_content()
        self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_refreshed')}")
        
    async def action_close(self) -> None:
        """å…³é—­å¼¹çª—"""
        try:
            await self._stop_file_watching()
            self.dismiss()
            # æ˜¾ç¤ºå…³é—­æç¤º
            self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_closed')}")
        except Exception as e:
            logger.error(f"å…³é—­æ—¥å¿—æŸ¥çœ‹å™¨å¤±è´¥: {e}")
            # å¼ºåˆ¶å…³é—­
            await self._force_close()
    
    async def action_force_close(self) -> None:
        """å¼ºåˆ¶å…³é—­å¼¹çª—"""
        await self._force_close()
    
    async def _force_close(self) -> None:
        """å¼ºåˆ¶å…³é—­å¼¹çª—çš„å†…éƒ¨æ–¹æ³•"""
        try:
            self.stop_watching = True
            if self.file_watcher_task and not self.file_watcher_task.done():
                # å–æ¶ˆå¼‚æ­¥ä»»åŠ¡
                self.file_watcher_task.cancel()
                # ç­‰å¾…ä»»åŠ¡çœŸæ­£å–æ¶ˆ
                try:
                    await asyncio.wait_for(self.file_watcher_task, timeout=0.1)
                except asyncio.TimeoutError:
                    pass  # è¶…æ—¶ä¹Ÿæ²¡å…³ç³»ï¼Œç»§ç»­å…³é—­
            self.dismiss()
            # æ˜¾ç¤ºå¼ºåˆ¶å…³é—­æç¤º
            self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_closed')}")
        except Exception as e:
            logger.error(f"{get_global_i18n().t('crawler.force_close_failed')}: {e}")
            # æœ€åçš„æ‰‹æ®µï¼šç›´æ¥ç§»é™¤
            try:
                self.remove()
                # å³ä½¿ç§»é™¤ä¹Ÿæ˜¾ç¤ºæç¤º
                self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_closed')}")
            except Exception:
                pass  # æœ€åçš„æ‰‹æ®µå¤±è´¥ï¼Œä»€ä¹ˆéƒ½ä¸åš
    
    def action_sync_close(self) -> None:
        """åŒæ­¥å…³é—­å¼¹çª—ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            self.stop_watching = True
            # ä¸ç­‰å¾…å¼‚æ­¥ä»»åŠ¡ï¼Œç›´æ¥å…³é—­
            self.dismiss()
            # æ˜¾ç¤ºåŒæ­¥å…³é—­æç¤º
            self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_closed')}")
        except Exception as e:
            logger.error(f"{get_global_i18n().t('crawler.sync_close_failed')}: {e}")
            try:
                self.remove()
                # å³ä½¿ç§»é™¤ä¹Ÿæ˜¾ç¤ºæç¤º
                self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_closed')}")
            except Exception:
                pass
    
    def _ensure_scroll_to_bottom(self) -> None:
        """ç¡®ä¿æ»šåŠ¨åˆ°åº•éƒ¨çš„å†…éƒ¨æ–¹æ³•"""
        try:
            log_viewer = self.query_one("#log-viewer", Log)
            log_viewer.scroll_end(animate=False)
        except Exception:
            pass
    
    def action_scroll_to_bottom(self) -> None:
            """æ‰‹åŠ¨æ»šåŠ¨åˆ°åº•éƒ¨"""
            try:
                log_viewer = self.query_one("#log-viewer", Log)
                # ç«‹å³æ»šåŠ¨ï¼Œç„¶åå†æ¬¡ç¡®ä¿æ»šåŠ¨åˆ°ç»å¯¹åº•éƒ¨ï¼Œæ›´å¿«å“åº”
                log_viewer.scroll_end(animate=False)
                self.set_timer(0.02, lambda: log_viewer.scroll_end(animate=False))
                self.app.notify(get_global_i18n().t('crawler.scrolled_to_bottom'))
            except Exception as e:
                logger.error(f"{get_global_i18n().t('crawler.scroll_to_bottom_failed')}: {e}")


class CrawlerManagementScreen(Screen[None]):
    """çˆ¬å–ç®¡ç†å±å¹•"""
    
    CSS_PATH = ["../styles/utilities.tcss", "../styles/crawler_management_overrides.tcss"]
    TITLE: ClassVar[Optional[str]] = None
    # ç»Ÿä¸€å¿«æ·é”®ç»‘å®šï¼ˆå« ESC è¿”å›ï¼‰
    BINDINGS: ClassVar[list[tuple[str, str, str]]] = [
        ("o", "open_browser", get_global_i18n().t('crawler.shortcut_o')),
        ("r", "view_history", get_global_i18n().t('crawler.shortcut_r')),
        ("b", "note", get_global_i18n().t('crawler.shortcut_b')),
        ("l", "view_logs", get_global_i18n().t('crawler.view_logs')),
        ("escape", "back", get_global_i18n().t('common.back')),
        ("X", "select_books", get_global_i18n().t('crawler.select_books')),
        ("s", "start_crawl", get_global_i18n().t('crawler.shortcut_s')),
        ("v", "stop_crawl", get_global_i18n().t('crawler.shortcut_v')),
        ("p", "prev_page", get_global_i18n().t('crawler.shortcut_p')),
        ("n", "next_page", get_global_i18n().t('crawler.shortcut_n')),
        ("x", "clear_search_params", get_global_i18n().t('crawler.clear_search_params')),
        ("j", "jump_to", get_global_i18n().t('bookshelf.jump_to')),
        ("space", "toggle_row", get_global_i18n().t('batch_ops.toggle_row')),
    ]

    def action_open_browser(self) -> None:
        self._open_browser()
        self._update_status(get_global_i18n().t('crawler.browser_opened'), "warning")

    def action_view_history(self) -> None:
        self._view_history()
        self._update_status(get_global_i18n().t('crawler.history_loaded'), "warning")

    def action_start_crawl(self) -> None:
        self._start_crawl()
        self._update_status(get_global_i18n().t('crawler.crawling'), "warning")

    def action_stop_crawl(self) -> None:
        self._stop_crawl()
        self._update_status(get_global_i18n().t('crawler.crawl_stopped'), "warning")

    def action_note(self) -> None:
        self._open_note_dialog()

    def action_view_logs(self) -> None:
        """æ‰“å¼€æ—¥å¿—æŸ¥çœ‹å™¨å¼¹çª—"""
        self._open_log_viewer()

    def action_prev_page(self) -> None:
        self._go_to_prev_page()

    def action_next_page(self) -> None:
        self._go_to_next_page()

    def action_back(self) -> None:
        self.app.pop_screen()

    def action_select_books(self) -> None:
        # å¦‚æœæœªå¼€å¯æ”¯æŒé€‰æ‹©ä¹¦ç±ï¼Œåˆ™ä¸åšä»»ä½•å¤„ç†
        if self.novel_site.get("selectable_enabled", True):
            self._open_select_books_dialog()
        else:
            # å¼¹çª—æç¤ºæœªå¼€å¯æ”¯æŒé€‰æ‹©ä¹¦ç±
            self._update_status(get_global_i18n().t('crawler.disabled_selectable'), "error")
    
    def action_toggle_row(self) -> None:
        """ç©ºæ ¼é”® - é€‰ä¸­æˆ–å–æ¶ˆé€‰ä¸­å½“å‰è¡Œ"""
        # ç›´æ¥å¤„ç†ç©ºæ ¼é”®ï¼Œä¸ä¾èµ–BINDINGSç³»ç»Ÿ
        table = self.query_one("#crawl-history-table", DataTable)
        
        # è·å–å½“å‰å…‰æ ‡ä½ç½®
        current_row_index = None
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨cursor_row
        if hasattr(table, 'cursor_row') and table.cursor_row is not None:
            current_row_index = table.cursor_row
        # å…¶æ¬¡å°è¯•ä½¿ç”¨cursor_coordinate
        elif hasattr(table, 'cursor_coordinate') and table.cursor_coordinate:
            coord = table.cursor_coordinate
            current_row_index = coord.row
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¡Œç´¢å¼•
        if current_row_index is None:
            # æ˜¾ç¤ºæç¤ºä¿¡æ¯ï¼Œè¦æ±‚ç”¨æˆ·å…ˆé€‰æ‹©ä¸€è¡Œ
            self._update_status(get_global_i18n().t('novel_sites.select_site_first'))
            return
        
        # æ£€æŸ¥è¡Œç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        current_page_row_count = min(self.items_per_page, len(self.crawler_history) - (self.current_page - 1) * self.items_per_page)
        if current_row_index < 0 or current_row_index >= current_page_row_count:
            self._update_status(get_global_i18n().t('novel_sites.select_site_first'))
            return
        
        # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•
        start_index = (self.current_page - 1) * self.items_per_page
        
        # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦æœ‰æ•°æ®
        if start_index + current_row_index >= len(self.crawler_history):
            self._update_status(get_global_i18n().t('novel_sites.select_site_first'))
            return
        
        # è·å–å½“å‰è¡Œçš„å†å²è®°å½•é¡¹
        history_item = self.crawler_history[start_index + current_row_index]
        if not history_item:
            return
        
        # è·å–è®°å½•ID
        record_id = str(history_item["id"])
        
        # åˆ‡æ¢é€‰ä¸­çŠ¶æ€
        if record_id in self.selected_history:
            self.selected_history.remove(record_id)
        else:
            self.selected_history.add(record_id)
        
        # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
        self._update_history_table()
        
        # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
        selected_count = len(self.selected_history)
        self._update_status(get_global_i18n().t('crawler.already_selected', count=selected_count), "information")
        
        # ç¡®ä¿è¡¨æ ¼ä¿æŒç„¦ç‚¹
        try:
            table.focus()
        except Exception:
            pass

    def action_clear_search_params(self) -> None:
        """æ¸…é™¤æœç´¢å‚æ•°"""
        self.query_one("#search-input-field", Input).value = ""
        self.query_one("#search-input-field", Input).placeholder = get_global_i18n().t('crawler.search_placeholder')

    def action_jump_to(self) -> None:
        self._show_jump_dialog()

    def _toggle_site_selection(self, table: DataTable, current_row_index: int) -> None:
        """åˆ‡æ¢ç½‘ç«™é€‰ä¸­çŠ¶æ€ï¼ˆå‚è€ƒæ‰¹é‡æ“ä½œé¡µé¢çš„å®ç°ï¼‰"""
        try:
            # è®¡ç®—å½“å‰é¡µé¢çš„èµ·å§‹ç´¢å¼•å’Œå…¨å±€ç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            
            # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦æœ‰æ•°æ®
            if start_index + current_row_index >= len(self.crawler_history):
                return
            
            # è·å–å½“å‰è¡Œçš„å†å²è®°å½•é¡¹
            history_item = self.crawler_history[start_index + current_row_index]
            if not history_item:
                return
            
            # è·å–è®°å½•ID
            record_id = str(history_item["id"])
            
            # åˆ‡æ¢é€‰ä¸­çŠ¶æ€
            if record_id in self.selected_history:
                self.selected_history.remove(record_id)
            else:
                self.selected_history.add(record_id)
            
            # é‡æ–°æ¸²æŸ“è¡¨æ ¼ä»¥æ›´æ–°é€‰ä¸­çŠ¶æ€æ˜¾ç¤º
            self._update_history_table()
            
            # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
            selected_count = len(self.selected_history)
            self._update_status(get_global_i18n().t('crawler.already_selected', count=selected_count), "information")
                
        except Exception:
            # å¦‚æœå‡ºé”™ï¼Œé‡æ–°æ¸²æŸ“æ•´ä¸ªè¡¨æ ¼
            self._update_history_table()

    def __init__(self, theme_manager: ThemeManager, novel_site: Dict[str, Any]):
        """
        åˆå§‹åŒ–çˆ¬å–ç®¡ç†å±å¹•
        
        Args:
            theme_manager: ä¸»é¢˜ç®¡ç†å™¨
            novel_site: ä¹¦ç±ç½‘ç«™ä¿¡æ¯
        """
        super().__init__()
        self.theme_manager = theme_manager
        self.novel_site = novel_site
        self.crawler_history = []  # çˆ¬å–å†å²è®°å½•
        self.current_page = 1
        self.items_per_page = 10
        self.total_pages = 0
        self.db_manager = DatabaseManager()  # æ•°æ®åº“ç®¡ç†å™¨
        
        # åå°çˆ¬å–ç®¡ç†å™¨
        from src.core.crawler_manager import CrawlerManager
        self.crawler_manager = CrawlerManager()
        
        # çˆ¬å–çŠ¶æ€
        self.current_task_id: Optional[str] = None  # å½“å‰ä»»åŠ¡ID
        self.is_crawling = False  # çˆ¬å–çŠ¶æ€æ ‡å¿—ï¼ˆç”¨äºUIæ˜¾ç¤ºï¼‰
        # å½“å‰æ­£åœ¨çˆ¬å–çš„IDï¼ˆç”¨äºçŠ¶æ€æ˜¾ç¤ºï¼‰
        self.current_crawling_id: Optional[str] = None
        self.loading_animation = None  # åŠ è½½åŠ¨ç”»ç»„ä»¶
        self.loading_indicator = None  # åŸç”Ÿ LoadingIndicator å¼•ç”¨
        self.is_mounted_flag = False  # ç»„ä»¶æŒ‚è½½æ ‡å¿—
        self.title = get_global_i18n().t('crawler.title')
        
        # å¤šé€‰ç›¸å…³å±æ€§
        self.selected_history: Set[str] = set()  # é€‰ä¸­çš„å†å²è®°å½•ID
        
        # æœç´¢ç›¸å…³å±æ€§
        self._search_keyword = ""  # æœç´¢å…³é”®è¯
        
        # æ’åºç›¸å…³å±æ€§
        self._sorted_history: List[str] = []  # æ’åºåçš„å†å²è®°å½•IDé¡ºåº
        self._sort_column: Optional[str] = None  # å½“å‰æ’åºçš„åˆ—
        self._sort_reverse: bool = True  # æ’åºæ–¹å‘ï¼ŒTrueè¡¨ç¤ºå€’åº

        # æµè§ˆå™¨é€‰æ‹©ç›¸å…³å±æ€§
        self.selected_browser = "chrome"  # é»˜è®¤é€‰æ‹©Chrome
        self.browser_options = ["chrome", "safari", "brave", "firefox"]
        
        # æµè§ˆå™¨æ ‡ç­¾é¡µç›‘å¬å™¨ï¼ˆAppleScriptæ¨¡å¼ï¼‰
        self.browser_monitor: Optional[BrowserTabMonitor] = None
        self.browser_monitor_active = False  # ç›‘å¬å™¨çŠ¶æ€
        
        # æ³¨å†Œå›è°ƒå‡½æ•°
        self.crawler_manager.register_status_callback(self._on_crawl_status_change)
        self.crawler_manager.register_notification_callback(self._on_crawl_success_notify)

    def _load_browser_config(self) -> None:
        """åŠ è½½æµè§ˆå™¨é…ç½®"""
        try:
            # ä»é…ç½®ç®¡ç†å™¨ä¸­è¯»å–é»˜è®¤æµè§ˆå™¨
            try:
                config_manager = ConfigManager.get_instance()
                config = config_manager.get_config()
                default_browser = config.get("browser", {}).get("default_browser", "chrome")
                if default_browser in self.browser_options:
                    self.selected_browser = default_browser
                    logger.info(f"ä»é…ç½®ç®¡ç†å™¨åŠ è½½é»˜è®¤æµè§ˆå™¨: {default_browser}")
                else:
                    logger.warning(f"è®¾ç½®çš„æµè§ˆå™¨ {default_browser} ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤Chrome")
                    self.selected_browser = "chrome"
            except Exception as e:
                logger.error(f"ä»é…ç½®ç®¡ç†å™¨è¯»å–æµè§ˆå™¨é…ç½®å¤±è´¥: {e}")
                self.selected_browser = "chrome"
        except Exception as e:
            logger.error(f"åŠ è½½æµè§ˆå™¨é…ç½®å¤±è´¥: {e}")

    def _save_browser_config(self) -> None:
        """ä¿å­˜æµè§ˆå™¨é…ç½®"""
        try:
            # ä¿å­˜åˆ°é…ç½®ç®¡ç†å™¨ä¸­
            try:
                config_manager = ConfigManager.get_instance()
                config = config_manager.get_config()
                
                # ç¡®ä¿ browser é…ç½®èŠ‚å­˜åœ¨
                if "browser" not in config:
                    config["browser"] = {}
                
                config["browser"]["default_browser"] = self.selected_browser
                
                if config_manager.save_config(config):
                    logger.info(f"ä¿å­˜æµè§ˆå™¨é…ç½®åˆ°é…ç½®ç®¡ç†å™¨: {self.selected_browser}")
                else:
                    logger.error(f"ä¿å­˜æµè§ˆå™¨é…ç½®å¤±è´¥: ä¿å­˜æ“ä½œè¿”å› False")
            except Exception as e:
                logger.error(f"ä¿å­˜æµè§ˆå™¨é…ç½®åˆ°é…ç½®ç®¡ç†å™¨å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"ä¿å­˜æµè§ˆå™¨é…ç½®å¤±è´¥: {e}")

    def _get_rating_display(self, rating: int) -> str:
        """
        æ ¹æ®æ˜Ÿçº§è¯„åˆ†ç”Ÿæˆæ˜¾ç¤ºå­—ç¬¦ä¸²
        
        Args:
            rating: æ˜Ÿçº§è¯„åˆ† (0-5)
            
        Returns:
            str: æ˜Ÿçº§æ˜¾ç¤ºå­—ç¬¦ä¸²ï¼Œå¦‚ "â˜†â˜†â˜†â˜†â˜†" æˆ– "â˜…â˜…â˜…â˜…â˜…"
        """
        # ç¡®ä¿è¯„åˆ†åœ¨0-5èŒƒå›´å†…
        rating = max(0, min(5, rating))
        
        # ä½¿ç”¨å®å¿ƒæ˜Ÿæ˜Ÿè¡¨ç¤ºè¯„åˆ†ï¼Œç©ºå¿ƒæ˜Ÿæ˜Ÿè¡¨ç¤ºå‰©ä½™
        filled_stars = "â˜…" * rating
        empty_stars = "â˜†" * (5 - rating)
        
        return f"{filled_stars}{empty_stars}"

    def _has_permission(self, permission_key: str) -> bool:
        """æ£€æŸ¥æƒé™ï¼ˆå…¼å®¹å•/å¤šç”¨æˆ·ï¼‰"""
        try:
            db_manager = self.db_manager if hasattr(self, "db_manager") else DatabaseManager()
            # è·å–å½“å‰ç”¨æˆ·ID
            current_user_id = getattr(self.app, 'current_user_id', None)
            if current_user_id is None:
                # å¦‚æœæ²¡æœ‰å½“å‰ç”¨æˆ·ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¤šç”¨æˆ·æ¨¡å¼
                if not getattr(self.app, 'multi_user_enabled', False):
                    # å•ç”¨æˆ·æ¨¡å¼é»˜è®¤å…è®¸æ‰€æœ‰æƒé™
                    return True
                else:
                    # å¤šç”¨æˆ·æ¨¡å¼ä½†æ²¡æœ‰å½“å‰ç”¨æˆ·ï¼Œé»˜è®¤æ‹’ç»
                    return False
            # ä¼ å…¥ç”¨æˆ·IDä¸æƒé™é”®
            return db_manager.has_permission(current_user_id, permission_key)  # type: ignore[misc]
        except Exception as e:
            logger.error(f"æ£€æŸ¥æƒé™å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å…è®¸
    
    def compose(self) -> ComposeResult:
        """
        ç»„åˆçˆ¬å–ç®¡ç†å±å¹•ç•Œé¢
        
        Returns:
            ComposeResult: ç»„åˆç»“æœ
        """
        yield Header()
        yield Container(
            Vertical(
                # Label(f"{get_global_i18n().t('crawler.title')} - {self.novel_site['name']}", id="crawler-title", classes="section-title"),
                Link(f"{self.novel_site['url']}", url=f"{self.novel_site['url']}", id="crawler-url", tooltip=f"{get_global_i18n().t('crawler.click_me')}"),
                # æ˜¾ç¤ºæ˜Ÿçº§è¯„åˆ†
                Label(self._get_rating_display(self.novel_site.get('rating', 2)), id="rating-label", classes="rating-display"),
                # å¯¹ä¹¦ç±IDç¤ºä¾‹è¿›è¡ŒURLè§£ç ï¼Œé¿å…æ˜¾ç¤ºä¹±ç 
                Label(f"{get_global_i18n().t('crawler.book_id_example')}: {unquote(self.novel_site.get('book_id_example', ''))}", id="book-id-example-label"),

                # é¡¶éƒ¨æ“ä½œæŒ‰é’®ï¼ˆå›ºå®šï¼‰
                Horizontal(
                    Button(get_global_i18n().t('crawler.open_browser'), id="open-browser-btn"),
                    Button(get_global_i18n().t('crawler.view_history'), id="view-history-btn"),
                    Button(get_global_i18n().t('crawler.note'), id="note-btn"),
                    Button(get_global_i18n().t('crawler.view_logs'), id="view-logs-btn", variant="success"),
                    Button(get_global_i18n().t('crawler.clear_invalid'), id="clear-invalid-btn", variant="error"),
                    # å¤šé€‰æ“ä½œæŒ‰é’®
                    Button(get_global_i18n().t('bookshelf.batch_ops.select_all'), id="select-all-btn"),
                    Button(get_global_i18n().t('bookshelf.batch_ops.invert_selection'), id="invert-selection-btn"),
                    Button(get_global_i18n().t('bookshelf.batch_ops.deselect_all'), id="deselect-all-btn"),
                    Button(get_global_i18n().t('batch_ops.move_up'), id="move-up-btn"),
                    Button(get_global_i18n().t('batch_ops.move_down'), id="move-down-btn"),
                    Button(get_global_i18n().t('batch_ops.merge'), id="merge-btn", variant="warning"),
                    Button(get_global_i18n().t('crawler.delete_file'), id="delete-file-btn", variant="warning"),
                    Button(get_global_i18n().t('crawler.delete_record'), id="delete-record-btn", variant="warning"),
                    Button(get_global_i18n().t('crawler.back'), id="back-btn"),
                    id="crawler-buttons", classes="btn-row"
                ),

                # ä¸­éƒ¨å¯æ»šåŠ¨åŒºåŸŸï¼šæœç´¢åŒº + è¾“å…¥åŒº + å†å²è¡¨æ ¼
                Vertical(
                    # æœç´¢åŒºåŸŸ
                    Vertical(
                        Horizontal(
                            Input(placeholder=get_global_i18n().t('crawler.search_placeholder'), id="search-input-field"),
                            Button(get_global_i18n().t('common.search'), id="search-btn"),
                            Button(get_global_i18n().t('crawler.clear_search'), id="clear-search-btn"),
                            id="search-container", classes="form-row"
                        ),
                        id="search-section"
                    ),
                    
                    # å°è¯´IDè¾“å…¥åŒºåŸŸ
                    Vertical(
                        Horizontal(
                            # æ ¹æ®ä¹¦ç±ç½‘ç«™çš„"æ˜¯å¦æ”¯æŒé€‰æ‹©ä¹¦ç±"è®¾ç½®æ˜¾ç¤ºé€‰æ‹©ä¹¦ç±æŒ‰é’®
                            *([Button(get_global_i18n().t('crawler.select_books'), id="choose-books-btn")] if self.novel_site.get("selectable_enabled", True) else []),
                            Input(placeholder=get_global_i18n().t('crawler.novel_id_placeholder_multi'), id="novel-id-input"),
                            Button(get_global_i18n().t('crawler.start_crawl'), id="start-crawl-btn", variant="primary"),
                            Button(get_global_i18n().t('crawler.stop_crawl'), id="stop-crawl-btn", variant="error", disabled=True),
                            Button(get_global_i18n().t('crawler.copy_ids'), id="copy-ids-btn"),
                            Button(get_global_i18n().t('crawler.toggle_monitor'), id="toggle-monitor-btn", variant="success"),
                            # Label(get_global_i18n().t('crawler.browser_label'), id="browser-label", classes="browser-label"),
                            Select(
                                id="browser-select",
                                options=[
                                    (get_global_i18n().t('crawler.browser_label'), "chrome"),
                                    ("Chrome", "chrome"),
                                    ("Safari", "safari"),
                                    ("Brave", "brave")
                                ],
                                value="chrome",
                                classes="browser-select"
                            ),
                            id="novel-id-container", classes="form-row"
                        ),
                        id="novel-id-section"
                    ),

                    # çˆ¬å–å†å²åŒºåŸŸ
                    Vertical(
                        # Label(get_global_i18n().t('crawler.crawl_history'), id="crawl-history-title"),
                        DataTable(id="crawl-history-table"),
                        id="crawl-history-section"
                    ),
                    id="crawler-scroll", classes="scroll-y"
                ),

                # åˆ†é¡µå¯¼èˆª
                Horizontal(
                    Button("â—€â—€", id="first-page-btn", classes="pagination-btn"),
                    Button("â—€", id="prev-page-btn", classes="pagination-btn"),
                    Label("", id="page-info", classes="page-info"),
                    Button("â–¶", id="next-page-btn", classes="pagination-btn"),
                    Button("â–¶â–¶", id="last-page-btn", classes="pagination-btn"),
                    Button(get_global_i18n().t('bookshelf.jump_to'), id="jump-page-btn", classes="pagination-btn"),
                    id="pagination-bar",
                    classes="pagination-bar"
                ),

                # çŠ¶æ€ä¿¡æ¯
                Label("", id="crawler-status"),

                # åŠ è½½åŠ¨ç”»åŒºåŸŸ
                Static("", id="loading-animation"),

                # å¿«æ·é”®çŠ¶æ€æ 
                # Horizontal(
                #     Label(get_global_i18n().t('crawler.shortcut_o'), id="shortcut-o"),
                #     Label(get_global_i18n().t('crawler.shortcut_r'), id="shortcut-r"),
                #     Label(get_global_i18n().t('crawler.shortcut_s'), id="shortcut-s"),
                #     Label(get_global_i18n().t('crawler.shortcut_v'), id="shortcut-v"),
                #     Label(get_global_i18n().t('crawler.shortcut_b'), id="shortcut-b"),
                #     Label(get_global_i18n().t('crawler.shortcut_p'), id="shortcut-p"),
                #     Label(get_global_i18n().t('crawler.shortcut_n'), id="shortcut-n"),
                #     Label(get_global_i18n().t('crawler.shortcut_esc'), id="shortcut-esc"),
                #     id="shortcuts-bar", classes="status-bar"
                # ),
                id="crawler-container"
            )
        )
        yield Footer()
    
    def on_mount(self) -> None:
        """å±å¹•æŒ‚è½½æ—¶çš„å›è°ƒ"""
        # è®¾ç½®æŒ‚è½½æ ‡å¿—
        self.is_mounted_flag = True
        
        # åº”ç”¨ä¸»é¢˜
        self.theme_manager.apply_theme_to_screen(self)
        
        # æƒé™æç¤ºä¸æŒ‰é’®çŠ¶æ€
        try:
            start_btn = self.query_one("#start-crawl-btn", Button)
            if not getattr(self.app, "has_permission", lambda k: True)("crawler.run"):
                start_btn.disabled = True
                self._update_status(get_global_i18n().t('crawler.np_crawler'), "warning")
        except Exception:
            pass
        
        # åˆå§‹åŒ–æ•°æ®è¡¨
        table = self.query_one("#crawl-history-table", DataTable)
        table.clear(columns=True)
        
        # æ·»åŠ åˆ—å®šä¹‰
        table.add_column(get_global_i18n().t('batch_ops.selected'), key="selected")
        table.add_column(get_global_i18n().t('crawler.sequence'), key="sequence")
        table.add_column(get_global_i18n().t('crawler.novel_id'), key="novel_id")
        table.add_column(get_global_i18n().t('crawler.novel_title'), key="novel_title")
        table.add_column(get_global_i18n().t('bookshelf.size'), key="file_size")
        table.add_column(get_global_i18n().t('crawler.crawl_time'), key="crawl_time")
        table.add_column(get_global_i18n().t('crawler.status'), key="status")
        table.add_column(get_global_i18n().t('crawler.view_file'), key="view_file")
        table.add_column(get_global_i18n().t('crawler.read_book'), key="read_book")
        table.add_column("æµè§ˆå™¨é˜…è¯»", key="browser_read_book")
        table.add_column(get_global_i18n().t('crawler.delete_file'), key="delete_file")
        table.add_column(get_global_i18n().t('crawler.delete_record'), key="delete_record")
        table.add_column(get_global_i18n().t('crawler.view_reason'), key="view_reason")
        table.add_column(get_global_i18n().t('crawler.retry'), key="retry")
        
        table.zebra_stripes = True
        
        # åˆå§‹åŒ–åŠ è½½åŠ¨ç”»
        self._initialize_loading_animation()

        # åŠ è½½æµè§ˆå™¨é…ç½® - å¿…é¡»åœ¨åˆå§‹åŒ–ç›‘å¬å™¨ä¹‹å‰
        self._load_browser_config()

        # æ›´æ–°æµè§ˆå™¨é€‰æ‹©ä¸‹æ‹‰æ¡†çš„å€¼ä¸ºåŠ è½½çš„é…ç½®
        try:
            browser_select = self.query_one("#browser-select", Select)
            browser_select.value = self.selected_browser
            logger.info(f"æµè§ˆå™¨é€‰æ‹©ä¸‹æ‹‰æ¡†å·²æ›´æ–°ä¸º: {self.selected_browser}")
        except Exception as e:
            logger.warning(f"æ›´æ–°æµè§ˆå™¨é€‰æ‹©ä¸‹æ‹‰æ¡†å¤±è´¥: {e}")

        # åˆå§‹åŒ–Chromeç›‘å¬å™¨ - ä½¿ç”¨åŠ è½½åçš„é…ç½®
        self._init_browser_monitor()

        # åŠ è½½çˆ¬å–å†å²
        self._load_crawl_history()

        # è®¾ç½®ç„¦ç‚¹åˆ°è¡¨æ ¼ï¼Œç¡®ä¿å…‰æ ‡ä½ç½®èƒ½å¤Ÿæ­£ç¡®æ¢å¤
        try:
            table = self.query_one("#crawl-history-table", DataTable)
            table.focus()
            # ç¡®ä¿è¡¨æ ¼èƒ½å¤Ÿæ¥æ”¶é”®ç›˜äº‹ä»¶
            table.can_focus = True
        except Exception:
            # å¦‚æœè¡¨æ ¼ç„¦ç‚¹è®¾ç½®å¤±è´¥ï¼Œå›é€€åˆ°è¾“å…¥æ¡†
            self.query_one("#novel-id-input", Input).focus()
    
    def _on_crawl_status_change(self, task_id: str, task: Any) -> None:
        """çˆ¬å–çŠ¶æ€å˜åŒ–å›è°ƒ"""
        try:
            from src.core.crawler_manager import CrawlStatus
            
            # æ›´æ–°UIçŠ¶æ€
            if task.status == CrawlStatus.RUNNING:
                self.is_crawling = True
                self.current_crawling_id = task.current_novel_id
                
                # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
                if task.current_novel_id:
                    status_text = f"{get_global_i18n().t('crawler.crawling')} ({task.progress}/{task.total}): {task.current_novel_id}"
                    self.app.call_later(self._update_status, status_text)
                
            elif task.status in [CrawlStatus.COMPLETED, CrawlStatus.FAILED, CrawlStatus.STOPPED]:
                self.is_crawling = False
                self.current_crawling_id = None
                self.current_task_id = None
                
                # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                if task.status == CrawlStatus.COMPLETED:
                    status_text = f"{get_global_i18n().t('crawler.crawl_completed')}: {task.success_count} {get_global_i18n().t('crawler.success')}, {task.failed_count} {get_global_i18n().t('crawler.failed')}"
                elif task.status == CrawlStatus.FAILED:
                    status_text = f"{get_global_i18n().t('crawler.crawl_failed')}: {task.error_message}"
                else:
                    status_text = get_global_i18n().t('crawler.crawl_stopped')
                
                self.app.call_later(self._update_status, status_text)
                self.app.call_later(self._update_crawl_button_state)
                self.app.call_later(self._hide_loading_animation)
                
                # åˆ·æ–°å†å²è®°å½•
                self.app.call_later(self._load_crawl_history)
                
                # è‡ªåŠ¨éªŒè¯ï¼šå¦‚æœè¾“å…¥æ¡†ä¸­è¿˜æœ‰IDï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€ä¸ª
                if task.status == CrawlStatus.COMPLETED:
                    self.app.call_later(self._check_and_continue_crawl)
                
                # è‡ªåŠ¨éªŒè¯ï¼šå¦‚æœè¾“å…¥æ¡†ä¸­è¿˜æœ‰IDï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€ä¸ª
                if task.status == CrawlStatus.COMPLETED:
                    self.app.call_later(self._check_and_continue_crawl)
            
        except Exception as e:
            logger.error(f"çˆ¬å–çŠ¶æ€å›è°ƒå¤„ç†å¤±è´¥: {e}")
    
    def _on_crawl_success_notify(self, task_id: str, novel_id: str, novel_title: str, already_exists: bool = False) -> None:
        """çˆ¬å–æˆåŠŸé€šçŸ¥å›è°ƒ
        
        Args:
            task_id: ä»»åŠ¡ID
            novel_id: å°è¯´ID
            novel_title: å°è¯´æ ‡é¢˜
            already_exists: æ˜¯å¦æ–‡ä»¶å·²å­˜åœ¨
        """
        try:
            # æ¸…ç†è¾“å…¥æ¡†ä¸­çš„IDï¼ˆæ— è®ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨éƒ½è¦æ¸…ç†ï¼‰
            self.app.call_later(self._remove_id_from_input, novel_id)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸éœ€è¦æ·»åŠ æ•°æ®åº“è®°å½•å’Œå‘é€å…¨å±€é€šçŸ¥
            if already_exists:
                logger.info(f"å°è¯´æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ·»åŠ æ•°æ®åº“è®°å½•: {novel_title}")
                # åªæ˜¾ç¤ºæ¶ˆæ¯ï¼Œä¸å‘é€å…¨å±€é€šçŸ¥ï¼Œä¸åˆ·æ–°å†å²è®°å½•
                self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.novel_exists')}: {novel_title}", "information")
                return
            
            # å‘é€å…¨å±€é€šçŸ¥ï¼ˆè·¨é¡µé¢ï¼‰
            def send_global_notification():
                try:
                    # å‘é€å…¨å±€é€šçŸ¥åˆ°ä¹¦æ¶é¡µé¢ï¼Œæ›´æ–°ä¹¦æ¶åˆ—è¡¨
                    if hasattr(self.app, 'post_message'):
                        try:
                            from src.ui.messages import RefreshBookshelfMessage
                            self.app.post_message(RefreshBookshelfMessage())
                        except ImportError:
                            logger.debug("RefreshBookshelfMessage å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€šçŸ¥æ–¹å¼")
                    
                    # åœ¨ä¸»ç•Œé¢æ˜¾ç¤ºæˆåŠŸé€šçŸ¥
                    if hasattr(self.app, 'post_message'):
                        try:
                            from src.ui.messages import CrawlCompleteNotification
                            self.app.post_message(CrawlCompleteNotification(
                                success=True,
                                novel_title=novel_title,
                                message=f"æˆåŠŸçˆ¬å–å°è¯´: {novel_title}"
                            ))
                        except ImportError:
                            logger.debug("CrawlCompleteNotification å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€šçŸ¥æ–¹å¼")
                except Exception as e:
                    logger.debug(f"å‘é€å…¨å±€é€šçŸ¥å¤±è´¥: {e}")
            
            self.app.call_later(send_global_notification)
            
            # åˆ·æ–°å½“å‰é¡µé¢çš„å†å²è®°å½•
            self.app.call_later(self._load_crawl_history)
            
        except Exception as e:
            logger.error(f"çˆ¬å–æˆåŠŸé€šçŸ¥å›è°ƒå¤„ç†å¤±è´¥: {e}")
    
    def _load_crawl_history(self, from_search: bool = False) -> None:
        """åŠ è½½çˆ¬å–å†å²è®°å½•
        
        Args:
            from_search: æ˜¯å¦æ¥è‡ªæœç´¢æ“ä½œï¼ˆæœç´¢æ—¶ä¸è®¾ç½®è¡¨æ ¼ç„¦ç‚¹ï¼‰
        """
        try:
            # ä»æ•°æ®åº“åŠ è½½çˆ¬å–å†å²
            site_id = self.novel_site.get('id')
            if site_id:
                # ä¸ºäº†æ”¯æŒåˆ†é¡µï¼Œä¸é™åˆ¶æŸ¥è¯¢æ•°é‡ï¼Œç”±UIåˆ†é¡µæ§åˆ¶æ˜¾ç¤º
                db_history = self.db_manager.get_crawl_history_by_site(site_id, limit=None)
                
                # è½¬æ¢æ•°æ®åº“æ ¼å¼ä¸ºæ˜¾ç¤ºæ ¼å¼
                self.crawler_history = []
                for item in db_history:
                    # è½¬æ¢çŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬
                    status_text = get_global_i18n().t('crawler.status_success') if item['status'] == 'success' else get_global_i18n().t('crawler.status_failed')
                    
                    # è½¬æ¢æ—¶é—´æ ¼å¼
                    try:
                        from datetime import datetime
                        crawl_time = datetime.fromisoformat(item['crawl_time']).strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        crawl_time = item['crawl_time']
                    
                    # è·å–æ–‡ä»¶å¤§å°
                    file_size = 0
                    if item['file_path']:
                        try:
                            from src.utils.file_utils import FileUtils
                            file_size = FileUtils.get_file_size(item['file_path'])
                        except Exception as e:
                            # logger.debug(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
                            file_size = 0
                    
                    self.crawler_history.append({
                        "id": item['id'],
                        "novel_id": item['novel_id'],
                        "novel_title": item['novel_title'],
                        "crawl_time": crawl_time,
                        "status": status_text,
                        "file_path": item['file_path'] or "",
                        "file_size": file_size,
                        "error_message": item.get('error_message', '')
                    })
            else:
                self.crawler_history = []
        except Exception as e:
            logger.error(f"åŠ è½½çˆ¬å–å†å²è®°å½•å¤±è´¥: {e}")
            self.crawler_history = []
        
        # åº”ç”¨æœç´¢è¿‡æ»¤
        self.crawler_history = self._filter_history(self.crawler_history)
        self._update_history_table(from_search=from_search)
    
    def _filter_history(self, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ¹æ®æœç´¢å…³é”®è¯è¿‡æ»¤å†å²è®°å½•"""
        if not self._search_keyword:
            return history
        
        # å¯¹æœç´¢å…³é”®è¯è¿›è¡ŒURLè§£ç ï¼Œä»¥ä¾¿èƒ½æ­£ç¡®åŒ¹é…
        keyword = unquote(self._search_keyword).lower()
        filtered_history = []
        
        for item in history:
            # è·å–è§£ç åçš„novel_id
            item_novel_id = item.get('novel_id', '')
            decoded_novel_id = unquote(item_novel_id) if item_novel_id else ''
            
            # æœç´¢å°è¯´æ ‡é¢˜ã€å°è¯´IDï¼ˆè§£ç åï¼‰ã€çŠ¶æ€
            if (keyword in item.get('novel_title', '').lower() or 
                keyword in decoded_novel_id.lower() or 
                keyword in item.get('status', '').lower()):
                filtered_history.append(item)
        
        return filtered_history
    
    def _update_history_table(self, from_search: bool = False) -> None:
        """æ›´æ–°å†å²è®°å½•è¡¨æ ¼
        
        Args:
            from_search: æ˜¯å¦æ¥è‡ªæœç´¢æ“ä½œï¼ˆæœç´¢æ—¶ä¸è®¾ç½®è¡¨æ ¼ç„¦ç‚¹ï¼‰
        """
        try:
            # ç¡®ä¿ç»„ä»¶å·²ç»æŒ‚è½½
            if not self.is_mounted_flag:
                logger.debug("ç»„ä»¶å°šæœªæŒ‚è½½ï¼Œå»¶è¿Ÿæ›´æ–°å†å²è®°å½•è¡¨æ ¼")
                # å»¶è¿Ÿ100msåé‡è¯•
                self.set_timer(0.1, self._update_history_table)
                return

            table = self.query_one("#crawl-history-table", DataTable)
            
            # ä¿å­˜å½“å‰å…‰æ ‡ä½ç½®
            current_cursor_row = table.cursor_row
            
            table.clear()
            
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            end_index = min(start_index + self.items_per_page, len(self.crawler_history))
            
            # æ·»åŠ å½“å‰é¡µçš„æ•°æ®è¡Œ
            for i in range(start_index, end_index):
                item = self.crawler_history[i]
                
                # æ£€æŸ¥æ˜¯å¦é€‰ä¸­
                # æ³¨æ„ï¼šselected_history ä¸­å­˜å‚¨çš„æ˜¯å­—ç¬¦ä¸²ç±»å‹çš„IDï¼Œéœ€è¦å°†item["id"]è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
                is_selected = "âœ“" if str(item["id"]) in self.selected_history else ""
                
                # å¯¹novel_idè¿›è¡ŒURLè§£ç ï¼Œé¿å…æ˜¾ç¤ºä¹±ç 
                novel_id = item.get("novel_id", "")
                decoded_novel_id = unquote(novel_id) if novel_id else ""
                
                # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º
                size_display = ""
                if "file_size" in item and item["file_size"]:
                    try:
                        from src.utils.file_utils import FileUtils
                        size_display = FileUtils.format_file_size(item["file_size"])
                    except Exception as e:
                        logger.debug(f"æ ¼å¼åŒ–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
                        size_display = ""
                
                row_data = {
                    "selected": is_selected,
                    "sequence": str(i + 1),
                    "novel_id": decoded_novel_id,
                    "novel_title": item["novel_title"],
                    "file_size": size_display,
                    "crawl_time": item["crawl_time"],
                    "status": item["status"],
                    "view_file": get_global_i18n().t('crawler.view_file') if item["status"] == get_global_i18n().t('crawler.status_success') else "",
                    "read_book": get_global_i18n().t('crawler.read_book') if item["status"] == get_global_i18n().t('crawler.status_success') else "",
                    "browser_read_book": get_global_i18n().t('crawler.browser_read_book') if item["status"] == get_global_i18n().t('crawler.status_success') else "",
                    "delete_file": get_global_i18n().t('crawler.delete_file') if item["status"] == get_global_i18n().t('crawler.status_success') else "",
                    "delete_record": get_global_i18n().t('crawler.delete_record'),
                    "view_reason": get_global_i18n().t('crawler.view_reason') if item["status"] == get_global_i18n().t('crawler.status_failed') else "",
                    "retry": get_global_i18n().t('crawler.retry') if item["status"] == get_global_i18n().t('crawler.status_failed') else ""
                }
                
                table.add_row(*row_data.values(), key=str(item["id"]))
            
            # æ›´æ–°åˆ†é¡µä¿¡æ¯
            self._update_pagination_info()
            
            # æ›´æ–°é€‰æ‹©çŠ¶æ€
            self._update_selection_status()
            
            # æ¢å¤å…‰æ ‡ä½ç½®ï¼Œç¡®ä¿å…‰æ ‡ä¸ä¼šè·³å›ç¬¬ä¸€è¡Œ
            if current_cursor_row is not None and current_cursor_row >= 0:
                # ç¡®ä¿å…‰æ ‡ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if current_cursor_row < min(self.items_per_page, len(self.crawler_history) - start_index):
                    if hasattr(table, 'move_cursor'):
                        table.move_cursor(row=current_cursor_row)
                    # å¦‚æœmove_cursorä¸å­˜åœ¨ï¼Œä½¿ç”¨é”®ç›˜æ“ä½œæ¥ç§»åŠ¨å…‰æ ‡
                    else:
                        # å°†å…‰æ ‡ç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®
                        # å…ˆå°†å…‰æ ‡ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œ
                        while table.cursor_row > 0:
                            table.action_cursor_up()
                        # ç„¶åå‘ä¸‹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                        for _ in range(current_cursor_row):
                            table.action_cursor_down()
            
            # åªæœ‰åœ¨ä¸æ˜¯æ¥è‡ªæœç´¢æ—¶æ‰è®¾ç½®è¡¨æ ¼ç„¦ç‚¹
            if not from_search:
                table.focus()
            
        except Exception as e:
            logger.debug(f"æ›´æ–°å†å²è®°å½•è¡¨æ ¼å¤±è´¥: {e}")
            # å»¶è¿Ÿé‡è¯•
            self.set_timer(0.1, self._update_history_table)
    
    def _update_pagination_info(self) -> None:
        """æ›´æ–°åˆ†é¡µä¿¡æ¯"""
        try:
            # ç¡®ä¿ç»„ä»¶å·²ç»æŒ‚è½½
            if not self.is_mounted_flag:
                logger.debug("ç»„ä»¶å°šæœªæŒ‚è½½ï¼Œå»¶è¿Ÿæ›´æ–°åˆ†é¡µä¿¡æ¯")
                # å»¶è¿Ÿ100msåé‡è¯•
                self.set_timer(0.1, self._update_pagination_info)
                return

            total_pages = max(1, (len(self.crawler_history) + self.items_per_page - 1) // self.items_per_page)
            page_info = get_global_i18n().t('page_info', total=len(self.crawler_history), current=self.current_page, pages=total_pages)
            
            page_label = self.query_one("#page-info", Label)
            page_label.update(page_info)
            
            # æ›´æ–°åˆ†é¡µæŒ‰é’®çŠ¶æ€
            self.query_one("#first-page-btn", Button).disabled = self.current_page <= 1
            self.query_one("#prev-page-btn", Button).disabled = self.current_page <= 1
            self.query_one("#next-page-btn", Button).disabled = self.current_page >= total_pages
            self.query_one("#last-page-btn", Button).disabled = self.current_page >= total_pages
            
        except Exception as e:
            logger.error(f"æ›´æ–°åˆ†é¡µä¿¡æ¯å¤±è´¥: {e}")
    
    def _update_status(self, message: str, severity: str = "information") -> None:
        """æ›´æ–°çŠ¶æ€ä¿¡æ¯"""
        try:
            status_label = self.query_one("#crawler-status", Label)
            status_label.update(message)
            
            # è®¾ç½®æ ·å¼ç±»
            status_label.remove_class("status-info")
            status_label.remove_class("status-warning")
            status_label.remove_class("status-error")
            
            if severity == "warning":
                status_label.add_class("status-warning")
            elif severity == "error":
                status_label.add_class("status-error")
            else:
                status_label.add_class("status-info")
                
        except Exception as e:
            logger.error(f"æ›´æ–°çŠ¶æ€ä¿¡æ¯å¤±è´¥: {e}")
    
    def _initialize_loading_animation(self) -> None:
        """åˆå§‹åŒ–åŠ è½½åŠ¨ç”»"""
        try:
            # åˆ›å»ºåŸç”ŸLoadingIndicator
            self.loading_indicator = LoadingIndicator()
            self.loading_indicator.styles.display = "none"  # é»˜è®¤éšè—
            
            # å°†åŠ è½½æŒ‡ç¤ºå™¨æ·»åŠ åˆ°åŠ è½½åŠ¨ç”»åŒºåŸŸ
            loading_container = self.query_one("#loading-animation", Static)
            loading_container.mount(self.loading_indicator)
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åŠ è½½åŠ¨ç”»å¤±è´¥: {e}")
    
    # ==================== æœç´¢åŠŸèƒ½ ====================
    
    def _focus_search_input(self) -> None:
        """å°†ç„¦ç‚¹è®¾ç½®å›æœç´¢æ¡†"""
        try:
            search_input = self.query_one("#search-input-field", Input)
            search_input.focus()
        except Exception as e:
            logger.debug(f"è®¾ç½®æœç´¢æ¡†ç„¦ç‚¹å¤±è´¥: {e}")
    
    def _perform_search(self) -> None:
        """æ‰§è¡Œæœç´¢"""
        try:
            search_input = self.query_one("#search-input-field", Input)
            self._search_keyword = search_input.value.strip()
            
            # é‡æ–°åŠ è½½å†å²è®°å½•å¹¶åº”ç”¨æœç´¢è¿‡æ»¤
            self._load_crawl_history(from_search=True)
            
            if self._search_keyword:
                self._update_status(get_global_i18n().t('crawler.search_complete', count=len(self.crawler_history)))
            else:
                self._update_status(get_global_i18n().t('crawler.all_records_shown'))
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            self._update_status(get_global_i18n().t('crawler.search_failed'), "error")
    
    def _clear_search(self) -> None:
        """æ¸…é™¤æœç´¢"""
        try:
            search_input = self.query_one("#search-input-field", Input)
            search_input.value = ""
            self._search_keyword = ""
            
            # é‡æ–°åŠ è½½å†å²è®°å½•
            self._load_crawl_history(from_search=True)
            self._update_status(get_global_i18n().t('crawler.search_cleared'))
        except Exception as e:
            logger.error(f"æ¸…é™¤æœç´¢å¤±è´¥: {e}")
            self._update_status(get_global_i18n().t('crawler.clear_search_failed'), "error")
    
    # ==================== å¤šé€‰æ“ä½œæ–¹æ³• ====================
    
    def _handle_selection_click(self, row_index: int) -> None:
        """å¤„ç†é€‰æ‹©åˆ—çš„ç‚¹å‡»"""
        try:
            # è·å–å½“å‰é¡µçš„æ•°æ®
            start_index = (self.current_page - 1) * self.items_per_page
            if row_index is not None and row_index < len(self.crawler_history) - start_index:
                history_item = self.crawler_history[start_index + row_index]
                
                if not history_item:
                    return
                
                record_id = history_item["id"]
                
                # åˆ‡æ¢é€‰æ‹©çŠ¶æ€
                if record_id in self.selected_history:
                    self.selected_history.remove(record_id)
                else:
                    self.selected_history.add(record_id)
                
                # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
                self._update_history_table()
                
                # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
                self._update_selection_status()
                
        except Exception as e:
            logger.error(f"å¤„ç†é€‰æ‹©ç‚¹å‡»å¤±è´¥: {e}")
    
    def _handle_cell_selection(self, row_key: str) -> None:
        """å¤„ç†å•å…ƒæ ¼é€‰æ‹©ï¼ˆç©ºæ ¼é”®æˆ–é¼ æ ‡ç‚¹å‡»ï¼‰"""
        try:
            # row_key å°±æ˜¯å†å²è®°å½•IDï¼Œç›´æ¥ä½¿ç”¨
            record_id = row_key
            
            # æ£€æŸ¥è®°å½•IDæ˜¯å¦å­˜åœ¨
            # æ³¨æ„ï¼šrecord_idæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œéœ€è¦ä¸å†å²è®°å½•IDè¿›è¡Œæ¯”è¾ƒæ—¶è¿›è¡Œç±»å‹è½¬æ¢
            record_exists = any(str(item["id"]) == record_id for item in self.crawler_history)
            if not record_exists:
                logger.debug(f"æ— æ³•æ‰¾åˆ°å¯¹åº”çš„å†å²è®°å½•: {record_id}")
                return
            
            # åˆ‡æ¢é€‰æ‹©çŠ¶æ€
            if record_id in self.selected_history:
                self.selected_history.remove(record_id)
            else:
                self.selected_history.add(record_id)
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            
            # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
            self._update_selection_status()
            
        except Exception as e:
            logger.error(f"å¤„ç†å•å…ƒæ ¼é€‰æ‹©å¤±è´¥: {e}")
    
    def _update_selection_status(self) -> None:
        """æ›´æ–°é€‰æ‹©çŠ¶æ€æ˜¾ç¤º"""
        selected_count = len(self.selected_history)
        self._update_status(get_global_i18n().t('batch_ops.selected_count', count=selected_count))
    
    def _select_all(self) -> None:
        """å…¨é€‰"""
        try:
            # é€‰æ‹©å½“å‰æ˜¾ç¤ºçš„æ‰€æœ‰è®°å½•
            for item in self.crawler_history:
                # ç¡®ä¿ç±»å‹ä¸€è‡´ï¼šå°†item["id"]è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                self.selected_history.add(str(item["id"]))
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            self._update_selection_status()
            
        except Exception as e:
            logger.error(f"å…¨é€‰å¤±è´¥: {e}")
    
    def _select_all_rows(self) -> None:
        """å…¨é€‰å½“å‰é¡µ"""
        try:
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•å’Œç»“æŸç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            end_index = min(start_index + self.items_per_page, len(self.crawler_history))
            
            # åªé€‰æ‹©å½“å‰é¡µçš„è®°å½•
            for i in range(start_index, end_index):
                item = self.crawler_history[i]
                # ç¡®ä¿ç±»å‹ä¸€è‡´ï¼šå°†item["id"]è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                record_id = str(item["id"])
                self.selected_history.add(record_id)
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            self._update_selection_status()
            
        except Exception as e:
            logger.error(f"å…¨é€‰å¤±è´¥: {e}")
    
    def _invert_selection(self) -> None:
        """åé€‰å½“å‰é¡µ"""
        try:
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•å’Œç»“æŸç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            end_index = min(start_index + self.items_per_page, len(self.crawler_history))
            
            # åªåé€‰å½“å‰é¡µçš„è®°å½•
            for i in range(start_index, end_index):
                item = self.crawler_history[i]
                # ç¡®ä¿ç±»å‹ä¸€è‡´ï¼šå°†item["id"]è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                record_id = str(item["id"])
                if record_id in self.selected_history:
                    self.selected_history.remove(record_id)
                else:
                    self.selected_history.add(record_id)
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            self._update_selection_status()
            
        except Exception as e:
            logger.error(f"åé€‰å¤±è´¥: {e}")
    
    def _deselect_all_rows(self) -> None:
        """å–æ¶ˆå…¨é€‰"""
        try:
            self.selected_history.clear()
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            self._update_selection_status()
            
        except Exception as e:
            logger.error(f"å–æ¶ˆå…¨é€‰å¤±è´¥: {e}")
    
    def _move_selected_up(self) -> None:
        """ä¸Šç§»å…‰æ ‡æ‰€åœ¨è¡Œ"""
        try:
            # è·å–å½“å‰å…‰æ ‡æ‰€åœ¨è¡Œ
            table = self.query_one("#crawl-history-table", DataTable)
            cursor_row = table.cursor_row
            
            if cursor_row is None or cursor_row < 0:
                return
            
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            
            # è®¡ç®—å®é™…ç´¢å¼•
            actual_index = start_index + cursor_row
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if actual_index >= len(self.crawler_history):
                return
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸Šç§»
            if actual_index <= 0:
                return
            
            # äº¤æ¢ä½ç½®
            self.crawler_history[actual_index], self.crawler_history[actual_index-1] = self.crawler_history[actual_index-1], self.crawler_history[actual_index]
            
            # ä¿å­˜æ–°çš„å…‰æ ‡ä½ç½®ï¼ˆä¸Šç§»åå…‰æ ‡åº”è¯¥å‘ä¸Šç§»åŠ¨ä¸€è¡Œï¼‰
            new_cursor_row = max(0, cursor_row - 1)
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            
            # æ¢å¤å…‰æ ‡åˆ°æ­£ç¡®ä½ç½®
            if hasattr(table, 'move_cursor'):
                table.move_cursor(row=new_cursor_row)
            else:
                # ä½¿ç”¨é”®ç›˜æ“ä½œæ¥ç§»åŠ¨å…‰æ ‡
                # å…ˆå°†å…‰æ ‡ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œ
                while table.cursor_row > 0:
                    table.action_cursor_up()
                # ç„¶åå‘ä¸‹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                for _ in range(new_cursor_row):
                    table.action_cursor_down()
            
            # åªæœ‰åœ¨ä¸æ˜¯æ¥è‡ªæœç´¢æ—¶æ‰è®¾ç½®è¡¨æ ¼ç„¦ç‚¹
            if not from_search:
                table.focus()
            
            
        except Exception as e:
            logger.error(f"ä¸Šç§»å¤±è´¥: {e}")
    
    def _move_selected_down(self) -> None:
        """ä¸‹ç§»å…‰æ ‡æ‰€åœ¨è¡Œ"""
        try:
            # è·å–å½“å‰å…‰æ ‡æ‰€åœ¨è¡Œ
            table = self.query_one("#crawl-history-table", DataTable)
            cursor_row = table.cursor_row
            
            if cursor_row is None or cursor_row < 0:
                return
            
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            
            # è®¡ç®—å®é™…ç´¢å¼•
            actual_index = start_index + cursor_row
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if actual_index >= len(self.crawler_history):
                return
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸‹ç§»
            if actual_index >= len(self.crawler_history) - 1:
                return
            
            # äº¤æ¢ä½ç½®
            self.crawler_history[actual_index], self.crawler_history[actual_index+1] = self.crawler_history[actual_index+1], self.crawler_history[actual_index]
            
            # ä¿å­˜æ–°çš„å…‰æ ‡ä½ç½®ï¼ˆä¸‹ç§»åå…‰æ ‡åº”è¯¥å‘ä¸‹ç§»åŠ¨ä¸€è¡Œï¼‰
            new_cursor_row = min(cursor_row + 1, self.items_per_page - 1)
            
            # è®¡ç®—å½“å‰é¡µçš„å®é™…è¡Œæ•°
            current_page_rows = min(self.items_per_page, len(self.crawler_history) - start_index)
            
            # ç¡®ä¿æ–°å…‰æ ‡ä½ç½®ä¸è¶…è¿‡å½“å‰é¡µçš„å®é™…è¡Œæ•°
            if new_cursor_row >= current_page_rows:
                new_cursor_row = current_page_rows - 1
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            
            # æ¢å¤å…‰æ ‡åˆ°æ­£ç¡®ä½ç½®
            if hasattr(table, 'move_cursor'):
                table.move_cursor(row=new_cursor_row)
            else:
                # ä½¿ç”¨é”®ç›˜æ“ä½œæ¥ç§»åŠ¨å…‰æ ‡
                # å…ˆå°†å…‰æ ‡ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œ
                while table.cursor_row > 0:
                    table.action_cursor_up()
                # ç„¶åå‘ä¸‹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                for _ in range(new_cursor_row):
                    table.action_cursor_down()
            
            # åªæœ‰åœ¨ä¸æ˜¯æ¥è‡ªæœç´¢æ—¶æ‰è®¾ç½®è¡¨æ ¼ç„¦ç‚¹
            if not from_search:
                table.focus()
            
            
        except Exception as e:
            logger.error(f"ä¸‹ç§»å¤±è´¥: {e}")
    
    def _move_to_position(self, target_position: int) -> None:
        """å°†å½“å‰å…‰æ ‡æ‰€åœ¨çš„é¡¹ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®"""
        try:
            # è·å–å½“å‰å…‰æ ‡æ‰€åœ¨è¡Œ
            table = self.query_one("#crawl-history-table", DataTable)
            cursor_row = table.cursor_row
            
            if cursor_row is None or cursor_row < 0:
                self._update_status(get_global_i18n().t('crawler.no_selection'))
                return
            
            # è®¡ç®—å½“å‰é¡µçš„èµ·å§‹ç´¢å¼•
            start_index = (self.current_page - 1) * self.items_per_page
            
            # è®¡ç®—å½“å‰é¡¹çš„å®é™…ç´¢å¼•
            current_index = start_index + cursor_row
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if current_index >= len(self.crawler_history):
                self._update_status(get_global_i18n().t('crawler.id_error'))
                return
            
            # æ£€æŸ¥é€‰ä¸­é¡¹æ•°é‡
            selected_count = len(self.selected_history)
            
            # å¦‚æœæ²¡æœ‰é€‰ä¸­é¡¹ï¼Œæç¤ºç”¨æˆ·
            if selected_count == 0:
                self._update_status(get_global_i18n().t('crawler.no_selection'))
                return
            
            # è·å–å½“å‰é¡¹
            current_item = self.crawler_history[current_index]
            
            # æ£€æŸ¥å½“å‰é¡¹æ˜¯å¦ä¸ºé€‰ä¸­é¡¹
            current_item_id = str(current_item.get("id"))
            if current_item_id not in self.selected_history:
                self._update_status(get_global_i18n().t('crawler.sort_only_selected'))
                return
            
            # å¦‚æœç›®æ ‡ä½ç½®è¶…å‡ºé€‰ä¸­é¡¹æ•°é‡ï¼Œè°ƒæ•´åˆ°æœ«å°¾
            if target_position >= selected_count:
                target_position = selected_count - 1
            
            # é‡æ–°æ’åºé€‰ä¸­é¡¹ï¼šå…ˆè·å–æ‰€æœ‰é€‰ä¸­é¡¹
            selected_items = []
            other_items = []
            
            for i, item in enumerate(self.crawler_history):
                item_id = str(item.get("id"))
                if item_id in self.selected_history:
                    selected_items.append((i, item))
                else:
                    other_items.append((i, item))
            
            # æ‰¾åˆ°å½“å‰é¡¹åœ¨é€‰ä¸­é¡¹ä¸­çš„ä½ç½®
            current_selected_index = -1
            for i, (orig_idx, item) in enumerate(selected_items):
                if str(item.get("id")) == current_item_id:
                    current_selected_index = i
                    break
            
            if current_selected_index == -1:
                return
            
            # ä»é€‰ä¸­é¡¹åˆ—è¡¨ä¸­ç§»é™¤å½“å‰é¡¹
            current_selected_item = selected_items.pop(current_selected_index)[1]
            
            # å°†å½“å‰é¡¹æ’å…¥åˆ°ç›®æ ‡ä½ç½®
            selected_items.insert(target_position, (None, current_selected_item))  # ä½ç½®ç”¨Noneä¸´æ—¶å ä½
            
            # é‡å»ºå®Œæ•´çš„åˆ—è¡¨ï¼šä¿æŒéé€‰ä¸­é¡¹çš„ç›¸å¯¹ä½ç½®ï¼Œåªè°ƒæ•´é€‰ä¸­é¡¹çš„é¡ºåº
            new_crawler_history = []
            selected_iter = iter(selected_items)
            
            for item in self.crawler_history:
                item_id = str(item.get("id"))
                if item_id in self.selected_history:
                    # ä½¿ç”¨é€‰ä¸­é¡¹ä¸­çš„ä¸‹ä¸€ä¸ªé¡¹
                    _, selected_item = next(selected_iter)
                    new_crawler_history.append(selected_item)
                else:
                    # ä¿æŒéé€‰ä¸­é¡¹ä¸å˜
                    new_crawler_history.append(item)
            
            # æ›´æ–°å†å²è®°å½•
            self.crawler_history = new_crawler_history
            
            # è®¡ç®—ç§»åŠ¨åå½“å‰é¡¹çš„æ–°ç´¢å¼•
            new_current_index = -1
            for i, item in enumerate(self.crawler_history):
                if str(item.get("id")) == current_item_id:
                    new_current_index = i
                    break
            
            if new_current_index == -1:
                return
            
            # è®¡ç®—ç§»åŠ¨åæ–°çš„å…‰æ ‡ä½ç½®
            if new_current_index < start_index:
                # ç§»åŠ¨åˆ°å½“å‰é¡µä¹‹å‰ï¼Œå°†å…‰æ ‡ç§»åŠ¨åˆ°å½“å‰é¡µç¬¬ä¸€è¡Œ
                new_cursor_row = 0
            elif new_current_index >= start_index + self.items_per_page:
                # ç§»åŠ¨åˆ°å½“å‰é¡µä¹‹åï¼Œå¯èƒ½éœ€è¦ç¿»é¡µ
                new_cursor_row = min(self.items_per_page - 1, len(self.crawler_history) - start_index - 1)
            else:
                # ç§»åŠ¨åˆ°å½“å‰é¡µå†…ï¼Œè®¡ç®—æ–°çš„å…‰æ ‡ä½ç½®
                new_cursor_row = new_current_index - start_index
            
            # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
            self._update_history_table()
            
            # å¦‚æœç§»åŠ¨åˆ°å…¶ä»–é¡µï¼Œè®¡ç®—æ–°é¡µç 
            new_page = new_current_index // self.items_per_page + 1
            if new_page != self.current_page:
                self.current_page = new_page
                self._update_history_table()
            
            # æ¢å¤å…‰æ ‡åˆ°æ­£ç¡®ä½ç½®
            if hasattr(table, 'move_cursor'):
                table.move_cursor(row=new_cursor_row)
            else:
                # ä½¿ç”¨é”®ç›˜æ“ä½œæ¥ç§»åŠ¨å…‰æ ‡
                # å…ˆå°†å…‰æ ‡ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œ
                while table.cursor_row > 0:
                    table.action_cursor_up()
                # ç„¶åå‘ä¸‹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                for _ in range(new_cursor_row):
                    table.action_cursor_down()
            
            # åªæœ‰åœ¨ä¸æ˜¯æ¥è‡ªæœç´¢æ—¶æ‰è®¾ç½®è¡¨æ ¼ç„¦ç‚¹
            if not from_search:
                table.focus()
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            display_position = target_position + 1
            if display_position == 10:
                display_key = "0"
            else:
                display_key = str(display_position)
            
        except Exception as e:
            logger.error(f"ç§»åŠ¨åˆ°æŒ‡å®šä½ç½®å¤±è´¥: {e}")
    
    def _move_cursor_to_position(self, target_position: int) -> None:
        """å°†å…‰æ ‡ç§»åŠ¨åˆ°å½“å‰é¡µçš„æŒ‡å®šè¡Œ"""
        try:
            # è·å–è¡¨æ ¼
            table = self.query_one("#crawl-history-table", DataTable)
            
            # è®¡ç®—å½“å‰é¡µçš„å®é™…è¡Œæ•°
            start_index = (self.current_page - 1) * self.items_per_page
            current_page_rows = min(self.items_per_page, len(self.crawler_history) - start_index)
            
            # æ£€æŸ¥ç›®æ ‡ä½ç½®æ˜¯å¦è¶…å‡ºå½“å‰é¡µçš„è¡Œæ•°
            if target_position >= current_page_rows:
                target_position = current_page_rows - 1
            
            # ç§»åŠ¨å…‰æ ‡åˆ°ç›®æ ‡è¡Œ
            if hasattr(table, 'move_cursor'):
                table.move_cursor(row=target_position)
            else:
                # ä½¿ç”¨é”®ç›˜æ“ä½œæ¥ç§»åŠ¨å…‰æ ‡
                # å…ˆå°†å…‰æ ‡ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œ
                while table.cursor_row > 0:
                    table.action_cursor_up()
                # ç„¶åå‘ä¸‹ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                for _ in range(target_position):
                    table.action_cursor_down()
            
            # åªæœ‰åœ¨ä¸æ˜¯æ¥è‡ªæœç´¢æ—¶æ‰è®¾ç½®è¡¨æ ¼ç„¦ç‚¹
            if not from_search:
                table.focus()
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            display_position = target_position + 1
            if display_position == 10:
                display_key = "0"
            else:
                display_key = str(display_position)
            
        except Exception as e:
            logger.error(f"ç§»åŠ¨å…‰æ ‡å¤±è´¥: {e}")
    
    def _merge_selected(self) -> None:
        """åˆå¹¶é€‰ä¸­é¡¹"""
        try:
            if not self.selected_history:
                self._update_status(get_global_i18n().t('crawler.merge_selection'))
                return
            
            if len(self.selected_history) < 2:
                self._update_status(get_global_i18n().t('crawler.merge_at_least_two'))
                return
            
            # è·å–é€‰ä¸­é¡¹
            selected_items = []
            for item in self.crawler_history:
                # ç¡®ä¿ç±»å‹ä¸€è‡´ï¼šå°†item["id"]è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
                if str(item["id"]) in self.selected_history:
                    selected_items.append(item)
            
            # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯æˆåŠŸçŠ¶æ€
            for item in selected_items:
                if item["status"] != get_global_i18n().t('crawler.status_success'):
                    self._update_status(get_global_i18n().t('crawler.merge_successful_records'))
                    return
            
            # æ‰“å¼€åˆå¹¶å¯¹è¯æ¡†
            from src.ui.dialogs.crawler_merge_dialog import CrawlerMergeDialog
            
            def handle_merge_result(result: Optional[Dict[str, Any]]) -> None:
                if not result:
                    return  # å¦‚æœç»“æœä¸ºNoneï¼Œç›´æ¥è¿”å›
                
                if result.get('success'):
                    new_title = result.get('new_title', '')
                    selected_items = result.get('selected_items', [])
                    
                    try:
                        # æ‰§è¡Œå®é™…çš„åˆå¹¶æ“ä½œ
                        if self._perform_actual_merge(selected_items, new_title):
                            self._update_status(f"{get_global_i18n().t('crawler.merge_success')}: {new_title}")
                            # æ¸…é™¤å·²åˆå¹¶çš„é€‰ä¸­é¡¹
                            for item in selected_items:
                                item_id = item.get("id")
                                if item_id and str(item_id) in self.selected_history:
                                    self.selected_history.remove(str(item_id))
                            # åˆ·æ–°å†å²è®°å½•
                            self._load_crawl_history()
                        else:
                            self._update_status(get_global_i18n().t('crawler.merge_failed'), "error")
                    except Exception as e:
                        logger.error(f"åˆå¹¶æ“ä½œå¼‚å¸¸: {e}")
                        self._update_status(get_global_i18n().t('crawler.merge_exception', e=e), "error")
                else:
                    message = result.get('message', 'æœªçŸ¥é”™è¯¯')
                    if message != get_global_i18n().t('batch_ops.cancel_merge'):  # ä¸æ˜¾ç¤ºå–æ¶ˆåˆå¹¶çš„é”™è¯¯
                        self._update_status(get_global_i18n().t('crawler.merge_failed_with_message', message=message), "error")
            
            self.app.push_screen(
                CrawlerMergeDialog(
                    self.theme_manager,
                    selected_items
                ),
                handle_merge_result
            )
            
        except Exception as e:
            logger.error(f"åˆå¹¶å¤±è´¥: {e}")
            self._update_status(get_global_i18n().t('crawler.merge_failed'), "error")
    
    def _perform_actual_merge(self, selected_items: List[Dict[str, Any]], new_title: str) -> bool:
        """
        æ‰§è¡Œå®é™…çš„åˆå¹¶æ“ä½œ
        
        Args:
            selected_items: é€‰ä¸­çš„çˆ¬å–å†å²è®°å½•
            new_title: æ–°ä¹¦ç±æ ‡é¢˜
            
        Returns:
            bool: åˆå¹¶æ˜¯å¦æˆåŠŸ
        """
        try:
            if not selected_items or len(selected_items) < 2:
                logger.error("åˆå¹¶å¤±è´¥ï¼šè‡³å°‘éœ€è¦é€‰æ‹©2æ¡è®°å½•")
                return False
            
            # æ”¶é›†éœ€è¦åˆå¹¶çš„æ–‡ä»¶è·¯å¾„å’Œè®°å½•ä¿¡æ¯
            file_paths = []
            record_ids = []
            for item in selected_items:
                if item.get("file_path"):
                    file_paths.append(item["file_path"])
                    record_ids.append(item["id"])
            
            if not file_paths:
                logger.error("åˆå¹¶å¤±è´¥ï¼šæ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„æ–‡ä»¶")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for file_path in file_paths:
                if not os.path.exists(file_path):
                    logger.error(f"åˆå¹¶å¤±è´¥ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
                    return False
            
            # åˆ›å»ºæ–°çš„åˆå¹¶æ–‡ä»¶
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç›®å½•ä½œä¸ºåˆå¹¶æ–‡ä»¶çš„ä¿å­˜ç›®å½•
            first_file_dir = os.path.dirname(file_paths[0])
            merged_filename = f"{new_title}_{timestamp}.txt"
            merged_file_path = os.path.join(first_file_dir, merged_filename)
            
            # åˆå¹¶æ–‡ä»¶å†…å®¹
            with open(merged_file_path, 'w', encoding='utf-8') as merged_file:
                for i, file_path in enumerate(file_paths):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as source_file:
                            content = source_file.read().strip()
                            if content:
                                # æ·»åŠ ç« èŠ‚åˆ†éš”ç¬¦ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼‰
                                if i > 0:
                                    merged_file.write("\n\n" + "="*50 + "\n\n")
                                merged_file.write(content)
                    except Exception as e:
                        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        continue
            
            # ä¿å­˜åˆå¹¶è®°å½•åˆ°æ•°æ®åº“
            site_id = self.novel_site.get('id')
            if not isinstance(site_id, int):
                site_id = 0  # é»˜è®¤å€¼
            
            self.db_manager.add_crawl_history(
                site_id=site_id,
                novel_id=f"merged_{timestamp}",
                novel_title=new_title,
                status="success",
                file_path=merged_file_path,
                error_message=""
            )
            
            # å°†åˆå¹¶åçš„ä¹¦ç±æ·»åŠ åˆ°ä¹¦åº“
            try:
                from src.core.book import Book
                
                # åˆ›å»ºä¹¦ç±å¯¹è±¡
                # ä½¿ç”¨å½“å‰æ“ä½œç½‘ç«™çš„æ•°æ®åº“åç§°ä½œä¸ºauthorå­—æ®µ
                author = self.novel_site.get('name', 'æœªçŸ¥ä½œè€…')
                site_tags = self.novel_site.get('tags', '')
                
                book = Book(merged_file_path, new_title, author, tags=site_tags)
                
                # æ£€æŸ¥ä¹¦ç±æ˜¯å¦å·²ç»å­˜åœ¨
                existing_books = self.db_manager.get_all_books()
                book_exists = any(book.path == merged_file_path for book in existing_books)
                
                if not book_exists:
                    # æ·»åŠ åˆ°ä¹¦åº“
                    if self.db_manager.add_book(book):
                        logger.info(f"åˆå¹¶ä¹¦ç±å·²æ·»åŠ åˆ°ä¹¦åº“: {new_title}")
                    else:
                        logger.warning(f"åˆå¹¶ä¹¦ç±æ·»åŠ åˆ°ä¹¦åº“å¤±è´¥: {new_title}")
                else:
                    logger.info(f"åˆå¹¶ä¹¦ç±å·²å­˜åœ¨äºä¹¦åº“: {new_title}")
                    
            except Exception as e:
                logger.error(f"æ·»åŠ åˆå¹¶ä¹¦ç±åˆ°ä¹¦åº“å¤±è´¥: {e}")
            
            # åˆ é™¤æºæ–‡ä»¶å’Œæºæ•°æ®
            for i, (file_path, record_id) in enumerate(zip(file_paths, record_ids)):
                try:
                    # åˆ é™¤æºæ–‡ä»¶
                    if os.path.exists(file_path):
                        send2trash(file_path)
                        logger.info(f"æºæ–‡ä»¶å·²ç§»è‡³å›æ”¶ç«™: {file_path}")
                    
                    # åˆ é™¤ä¹¦æ¶ä¸­çš„å¯¹åº”ä¹¦ç±
                    books = self.db_manager.get_all_books()
                    for book in books:
                        if hasattr(book, 'path') and book.path == file_path:
                            if self.db_manager.delete_book(book.path):
                                logger.info(f"åˆ é™¤ä¹¦æ¶ä¸­çš„ä¹¦ç±: {book.title}")
                            else:
                                logger.warning(f"åˆ é™¤ä¹¦æ¶ä¹¦ç±å¤±è´¥: {book.title}")
                            break
                    
                    # åˆ é™¤çˆ¬å–å†å²è®°å½•
                    if record_id:
                        self.db_manager.delete_crawl_history(record_id)
                        logger.info(f"åˆ é™¤çˆ¬å–å†å²è®°å½•: {record_id}")
                        
                except Exception as e:
                    logger.error(f"åˆ é™¤æºæ–‡ä»¶æˆ–æ•°æ®å¤±è´¥ {i}: {e}")
            
            # å‘é€ä¹¦æ¶åˆ·æ–°æ¶ˆæ¯
            try:
                from src.ui.messages import RefreshBookshelfMessage
                self.app.post_message(RefreshBookshelfMessage())
                logger.info("å·²å‘é€ä¹¦æ¶åˆ·æ–°æ¶ˆæ¯")
            except Exception as msg_error:
                logger.debug(f"å‘é€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯å¤±è´¥: {msg_error}")
            
            # è®°å½•åˆå¹¶æ“ä½œæ—¥å¿—
            logger.info(f"åˆå¹¶æˆåŠŸï¼š{len(selected_items)}ä¸ªæ–‡ä»¶åˆå¹¶ä¸º {new_title}")
            
            return True
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ“ä½œå¼‚å¸¸: {e}")
            return False
    


    @on(DataTable.HeaderSelected, "#crawl-history-table")
    def on_data_table_header_selected(self, event: DataTable.HeaderSelected) -> None:
        """æ•°æ®è¡¨æ ¼è¡¨å¤´ç‚¹å‡»äº‹ä»¶ - å¤„ç†æ’åº"""
        try:
            column_key = event.column_key.value or ""

            logger.debug(f"è¡¨å¤´ç‚¹å‡»äº‹ä»¶: column={column_key}")

            # åªå¯¹ç‰¹å®šåˆ—è¿›è¡Œæ’åºï¼šåºå·ã€ä¹¦ç±IDã€ä¹¦ç±æ ‡é¢˜ã€å¤§å°ã€çˆ¬å–æ—¶é—´ã€çŠ¶æ€
            sortable_columns = ["sequence", "novel_id", "novel_title", "file_size", "crawl_time", "status"]

            if column_key in sortable_columns:
                # åˆ‡æ¢æ’åºæ–¹å‘
                if self._sort_column == column_key:
                    self._sort_reverse = not self._sort_reverse
                else:
                    self._sort_column = column_key
                    self._sort_reverse = True  # æ–°åˆ—é»˜è®¤å€’åº

                # æ‰§è¡Œæ’åº
                self._sort_history(column_key, self._sort_reverse)

                # æ›´æ–°è¡¨æ ¼æ˜¾ç¤º
                self._update_history_table()

                # æ˜¾ç¤ºæ’åºæç¤º
                sort_direction = "å€’åº" if self._sort_reverse else "æ­£åº"
                column_names = {
                    "sequence": "åºå·",
                    "novel_id": "ä¹¦ç±ID",
                    "novel_title": "ä¹¦ç±æ ‡é¢˜",
                    "file_size": "å¤§å°",
                    "crawl_time": "çˆ¬å–æ—¶é—´",
                    "status": "çŠ¶æ€"
                }
                column_name = column_names.get(column_key, column_key)
                self._update_status(f"å·²æŒ‰ {column_name} {sort_direction} æ’åˆ—", "information")

        except Exception as e:
            logger.error(f"è¡¨å¤´ç‚¹å‡»äº‹ä»¶å¤„ç†å¤±è´¥: {e}")

    def _sort_history(self, column_key: str, reverse: bool) -> None:
        """æ ¹æ®æŒ‡å®šåˆ—å¯¹å†å²è®°å½•è¿›è¡Œæ’åº

        Args:
            column_key: æ’åºçš„åˆ—é”®
            reverse: æ˜¯å¦å€’åº
        """
        try:
            def get_sort_key(item: Dict[str, Any]) -> Any:
                """è·å–æ’åºé”®å€¼"""
                if column_key == "sequence":
                    # åºå·æ— æ³•ç›´æ¥æ’åºï¼Œä½¿ç”¨åŸå§‹æ•°æ®é¡ºåº
                    # ç”±äºæ’åºååºå·ä¼šé‡æ–°ç¼–å·ï¼Œè¿™é‡Œä½¿ç”¨IDä½œä¸ºæ›¿ä»£
                    return item.get("id", "")
                elif column_key == "novel_id":
                    # ä¹¦ç±IDæ’åºï¼Œéœ€è¦URLè§£ç 
                    novel_id = item.get("novel_id", "")
                    return unquote(novel_id) if novel_id else ""
                elif column_key == "novel_title":
                    # ä¹¦ç±æ ‡é¢˜æ’åº
                    return item.get("novel_title", "")
                elif column_key == "file_size":
                    # å¤§å°æ’åºï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å¤§å°æ•°å€¼
                    return item.get("file_size", 0)
                elif column_key == "crawl_time":
                    # çˆ¬å–æ—¶é—´æ’åºï¼Œéœ€è¦è§£ææ—¶é—´å­—ç¬¦ä¸²
                    try:
                        from datetime import datetime
                        time_str = item.get("crawl_time", "")
                        return datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
                    except:
                        return datetime.min
                elif column_key == "status":
                    # çŠ¶æ€æ’åº
                    return item.get("status", "")
                return None

            # ä½¿ç”¨ sort å‡½æ•°è¿›è¡Œæ’åº
            self.crawler_history.sort(key=get_sort_key, reverse=reverse)

        except Exception as e:
            logger.error(f"æ’åºå¤±è´¥: {e}")

    @on(DataTable.CellSelected, "#crawl-history-table")
    def on_data_table_cell_selected(self, event: DataTable.CellSelected) -> None:
        """æ•°æ®è¡¨æ ¼å•å…ƒæ ¼é€‰æ‹©äº‹ä»¶"""
        try:
            cell_key = event.cell_key
            column = cell_key.column_key.value or ""
            row_key = cell_key.row_key.value or ""
            
            logger.debug(f"å•å…ƒæ ¼é€‰æ‹©äº‹ä»¶: column={column}, row_key={row_key}")
            
            # å¤„ç†é€‰æ‹©åˆ—ç‚¹å‡»
            if column == "selected":
                self._handle_cell_selection(row_key)
                
            # å¤„ç†å…¶ä»–åˆ—çš„æŒ‰é’®ç‚¹å‡»
            elif column in ["view_file", "read_book", "browser_read_book", "delete_file", "delete_record", "view_reason", "retry"]:
                self._handle_button_click(column, row_key)
                
            # å¤„ç†ç©ºæ ¼é”®é€‰æ‹©ï¼šå½“ç‚¹å‡»ä»»ä½•éæŒ‰é’®åˆ—æ—¶ï¼Œè§¦å‘é€‰æ‹©åˆ‡æ¢
            elif column not in ["selected", "view_file", "read_book", "browser_read_book", "delete_file", "delete_record", "view_reason", "retry"]:
                self._handle_cell_selection(row_key)
                    
        except Exception as e:
            logger.error(f"å•å…ƒæ ¼é€‰æ‹©äº‹ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _handle_button_click(self, column: str, row_key: str) -> None:
        """å¤„ç†æŒ‰é’®ç‚¹å‡»"""
        try:
            # row_key å°±æ˜¯å†å²è®°å½•IDï¼Œç›´æ¥æŸ¥æ‰¾å¯¹åº”çš„å†å²è®°å½•
            # æ³¨æ„ï¼šrow_keyæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œéœ€è¦ä¸å†å²è®°å½•IDè¿›è¡Œæ¯”è¾ƒæ—¶è¿›è¡Œç±»å‹è½¬æ¢
            history_item = None
            for item in self.crawler_history:
                # å°†å†å²è®°å½•IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä¸row_keyè¿›è¡Œæ¯”è¾ƒ
                if str(item.get("id")) == row_key:
                    history_item = item
                    break
            
            if not history_item:
                logger.debug(f"æ— æ³•æ‰¾åˆ°å¯¹åº”çš„å†å²è®°å½•: {row_key}")
                return
            
            # æ ¹æ®åˆ—åè°ƒç”¨ç›¸åº”çš„å¤„ç†æ–¹æ³•
            if column == "view_file":
                self._view_file(history_item)
            elif column == "read_book":
                self._read_book(history_item)
            elif column == "browser_read_book":
                self._read_book_in_browser(history_item)
            elif column == "delete_file":
                self._delete_file(history_item)
            elif column == "delete_record":
                self._delete_record_only(history_item)
            elif column == "view_reason":
                self._view_reason(history_item)
            elif column == "retry":
                self._retry_crawl(history_item)
                
        except Exception as e:
            logger.debug(f"å¤„ç†æŒ‰é’®ç‚¹å‡»å¤±è´¥: {e}")
    
    @on(DataTable.RowSelected, "#crawl-history-table")
    def on_data_table_row_selected(self, event: DataTable.RowSelected) -> None:
        """æ•°æ®è¡¨æ ¼è¡Œé€‰æ‹©äº‹ä»¶ - æ”¯æŒç©ºæ ¼é”®é€‰æ‹©"""
        try:
            row_key = event.row_key.value or ""
            logger.debug(f"è¡Œé€‰æ‹©äº‹ä»¶: row_key={row_key}")
            
            # åˆ‡æ¢é€‰æ‹©çŠ¶æ€
            self._handle_cell_selection(row_key)
                    
        except Exception as e:
            logger.error(f"è¡Œé€‰æ‹©äº‹ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _go_to_first_page(self) -> None:
        """è·³è½¬åˆ°ç¬¬ä¸€é¡µ"""
        if self.current_page != 1:
            self.current_page = 1
            self._update_history_table()
    
    def _go_to_prev_page(self) -> None:
        """è·³è½¬åˆ°ä¸Šä¸€é¡µ"""
        if self.current_page > 1:
            self.current_page -= 1
            self._update_history_table()
    
    def _go_to_next_page(self) -> None:
        """è·³è½¬åˆ°ä¸‹ä¸€é¡µ"""
        total_pages = max(1, (len(self.crawler_history) + self.items_per_page - 1) // self.items_per_page)
        if self.current_page < total_pages:
            self.current_page += 1
            self._update_history_table()
    
    def _go_to_last_page(self) -> None:
        """è·³è½¬åˆ°æœ€åä¸€é¡µ"""
        total_pages = max(1, (len(self.crawler_history) + self.items_per_page - 1) // self.items_per_page)
        if self.current_page != total_pages:
            self.current_page = total_pages
            self._update_history_table()
    
    def _show_jump_dialog(self) -> None:
        """æ˜¾ç¤ºè·³è½¬é¡µç å¯¹è¯æ¡†"""
        def handle_jump_result(result: Optional[str]) -> None:
            if result and result.strip():
                try:
                    total_pages = max(1, (len(self.crawler_history) + self.items_per_page - 1) // self.items_per_page)
                    page_num = int(result.strip())
                    if 1 <= page_num <= total_pages:
                        if page_num != self.current_page:
                            self.current_page = page_num
                            self._update_history_table()
                    else:
                        self._update_status(get_global_i18n().t('crawler.page_out_of_range', total_pages=total_pages), "error")
                except ValueError:
                    self._update_status(get_global_i18n().t('crawler.invalid_page_number'), "error")
        
        from src.ui.dialogs.input_dialog import InputDialog
        dialog = InputDialog(
            self.theme_manager,
            title=get_global_i18n().t('crawler.goto_page'),
            prompt=get_global_i18n().t('crawler.goto_page_message'),
            placeholder=get_global_i18n().t('crawler.page_number')
        )
        self.app.push_screen(dialog, handle_jump_result)
    
    # ==================== åŸºç¡€åŠŸèƒ½æ–¹æ³• ====================
    
    def _open_browser(self) -> None:
        """åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç½‘ç«™"""
        try:
            # ä½¿ç”¨BrowserManageræ‰“å¼€URL
            try:
                from src.utils.browser_manager import BrowserManager
                
                success = BrowserManager.open_url(self.novel_site['url'])
                if success:
                    browser_name = BrowserManager.get_default_browser()
                    self._update_status(get_global_i18n().t('crawler.browser_opened_with', browser=browser_name), "information")
                else:
                    # å¦‚æœBrowserManagerå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æµè§ˆå™¨
                    import webbrowser
                    webbrowser.open(self.novel_site['url'])
                    self._update_status(get_global_i18n().t('crawler.browser_opened'))
            except Exception as e:
                # å¦‚æœBrowserManagerå¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æµè§ˆå™¨
                import webbrowser
                webbrowser.open(self.novel_site['url'])
                self._update_status(get_global_i18n().t('crawler.browser_opened'))
                logger.warning(f"BrowserManagerå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æµè§ˆå™¨: {e}")
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.open_browser_failed')}: {str(e)}", "error")
    
    def _view_history(self) -> None:
        """æŸ¥çœ‹çˆ¬å–å†å²"""
        # åˆ·æ–°å†å²è®°å½•
        self._load_crawl_history()
        self._update_status(get_global_i18n().t('crawler.history_loaded'))
    
    def _open_note_dialog(self) -> None:
        """æ‰“å¼€å¤‡æ³¨å¯¹è¯æ¡†"""
        try:
            # è·å–å½“å‰ç½‘ç«™çš„å¤‡æ³¨å†…å®¹
            site_id = self.novel_site.get('id')
            if not site_id:
                self._update_status(get_global_i18n().t('crawler.no_site_id'), "error")
                return
            
            # ä»æ•°æ®åº“åŠ è½½ç°æœ‰å¤‡æ³¨
            current_note = self.db_manager.get_novel_site_note(site_id) or ""
            
            # æ‰“å¼€å¤‡æ³¨å¯¹è¯æ¡†
            def handle_note_dialog_result(result: Optional[str]) -> None:
                if result is not None:
                    # ä¿å­˜å¤‡æ³¨åˆ°æ•°æ®åº“
                    if self.db_manager.save_novel_site_note(site_id, result):
                        self._update_status(get_global_i18n().t('crawler.note_saved'), "success")
                    else:
                        self._update_status(get_global_i18n().t('crawler.note_save_failed'), "error")
                # å¦‚æœresultä¸ºNoneï¼Œè¡¨ç¤ºç”¨æˆ·å–æ¶ˆäº†æ“ä½œ
            
            self.app.push_screen(
                NoteDialog(
                    self.theme_manager,
                    self.novel_site['name'],
                    current_note
                ),
                handle_note_dialog_result
            )
            
        except Exception as e:
            logger.error(f"æ‰“å¼€å¤‡æ³¨å¯¹è¯æ¡†å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.open_note_dialog_failed')}: {str(e)}", "error")

    def _open_select_books_dialog(self) -> None:
        """æ‰“å¼€é€‰æ‹©ä¹¦ç±å¯¹è¯æ¡†ï¼Œå›å¡«é€‰ä¸­IDåˆ°è¾“å…¥æ¡†"""
        try:
            def handle_selected_ids(result: Optional[str]) -> None:
                if result:
                    try:
                        # å¯¹é€‰æ‹©çš„IDè¿›è¡ŒURLè§£ç 
                        decoded_result = unquote(result)
                        novel_id_input = self.query_one("#novel-id-input", Input)
                        current_value = novel_id_input.value.strip()
                        
                        if current_value:
                            # å¦‚æœè¾“å…¥æ¡†ä¸­å·²æœ‰å†…å®¹ï¼Œåˆ™åœ¨åŸæœ‰å†…å®¹æœ«å°¾æ·»åŠ é€—å·åå†è¿½åŠ æ–°å†…å®¹
                            new_value = f"{current_value},{decoded_result}"
                        else:
                            # å¦‚æœè¾“å…¥æ¡†ä¸ºç©ºï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ–°å†…å®¹
                            new_value = decoded_result
                        
                        novel_id_input.value = new_value
                        novel_id_input.focus()
                        self._update_status(get_global_i18n().t('crawler.filled_ids'))
                    except Exception as e:
                        logger.debug(f"å›å¡«é€‰ä¸­IDå¤±è´¥: {e}")
            self.app.push_screen(
                SelectBooksDialog(self.theme_manager, self.novel_site),
                handle_selected_ids
            )
        except Exception as e:
            logger.error(f"æ‰“å¼€é€‰æ‹©ä¹¦ç±å¯¹è¯æ¡†å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.open_dialog_failed')}: {str(e)}", "error")

    def _stop_crawl(self) -> None:
        """åœæ­¢çˆ¬å–"""
        if not self.is_crawling and not self.current_task_id:
            self._update_status(get_global_i18n().t('crawler.no_crawl_in_progress'))
            return
        
        # ç«‹å³æ›´æ–°UIçŠ¶æ€
        self.is_crawling = False
        self._update_crawl_button_state()
        
        # å¦‚æœæœ‰åå°ä»»åŠ¡ï¼Œåœæ­¢å®ƒ
        if self.current_task_id:
            if self.crawler_manager.stop_crawl_task(self.current_task_id):
                self._update_status(get_global_i18n().t('crawler.crawl_stopped'))
            else:
                self._update_status(get_global_i18n().t('crawler.stop_crawl_failed'), "error")
        else:
            # å¦‚æœæ²¡æœ‰åå°ä»»åŠ¡ï¼Œç›´æ¥æ˜¾ç¤ºåœæ­¢çŠ¶æ€
            self._update_status(get_global_i18n().t('crawler.crawl_stopped'))
    
    def _copy_novel_ids(self) -> None:
        """å¤åˆ¶è¾“å…¥æ¡†ä¸­çš„æ‰€æœ‰ä¹¦ç±ID"""
        try:
            # è·å–è¾“å…¥æ¡†ä¸­çš„å†…å®¹
            novel_id_input = self.query_one("#novel-id-input", Input)
            novel_ids_input = novel_id_input.value.strip()
            
            if not novel_ids_input:
                self._update_status(get_global_i18n().t('crawler.enter_novel_id'))
                return
            
            # åˆ†å‰²å¤šä¸ªå°è¯´IDå¹¶å¤„ç†
            novel_ids = [id.strip() for id in novel_ids_input.split(',') if id.strip()]
            
            if not novel_ids:
                self._update_status(get_global_i18n().t('crawler.enter_novel_id'))
                return
            
            # å°†IDåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨é€—å·åˆ†éš”ï¼Œä¿æŒåŸå§‹æ ¼å¼
            ids_text = ', '.join(novel_ids)
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            count = len(novel_ids)
            
            # ä½¿ç”¨pyperclipå¤åˆ¶åˆ°å‰ªè´´æ¿
            try:
                import pyperclip
                pyperclip.copy(ids_text)
                self._update_status(f"{get_global_i18n().t('crawler.copy_ids_success')}: {count} {get_global_i18n().t('crawler.books_count')}")
            except ImportError:
                # å¦‚æœpyperclipæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤
                import subprocess
                import platform
                
                # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©ä¸åŒçš„å¤åˆ¶å‘½ä»¤
                system = platform.system()
                if system == 'Darwin':  # macOS
                    process = subprocess.run(['pbcopy'], input=ids_text, text=True, check=True)
                elif system == 'Windows':  # Windows
                    process = subprocess.run(['clip'], input=ids_text, text=True, check=True, shell=True)
                else:  # Linux
                    # å°è¯•ä½¿ç”¨xclipæˆ–xsel
                    try:
                        process = subprocess.run(['xclip', '-selection', 'clipboard'], input=ids_text, text=True, check=True)
                    except (subprocess.SubprocessError, FileNotFoundError):
                        process = subprocess.run(['xsel', '--clipboard', '--input'], input=ids_text, text=True, check=True)
                
                self._update_status(f"{get_global_i18n().t('crawler.copy_ids_success')}: {count} {get_global_i18n().t('crawler.books_count')}")
            
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.copy_ids_failed')}: {str(e)}", "error")
    
    def _start_crawl(self) -> None:
        """å¼€å§‹çˆ¬å–å°è¯´"""
        # æƒé™æ ¡éªŒï¼šæ‰§è¡Œçˆ¬å–ä»»åŠ¡éœ€ crawler.run
        if not getattr(self.app, "has_permission", lambda k: True)("crawler.run"):
            self._update_status(get_global_i18n().t('crawler.np_crawler'), "error")
            return
        if self.is_crawling:
            self._update_status(get_global_i18n().t('crawler.crawling_in_progress'), "warning")
            return  # å¦‚æœæ­£åœ¨çˆ¬å–ï¼Œå¿½ç•¥æ–°çš„çˆ¬å–è¯·æ±‚
        
        # è®¾ç½®çˆ¬å–çŠ¶æ€
        self.is_crawling = True
        self._update_crawl_button_state()
        
        novel_id_input = self.query_one("#novel-id-input", Input)
        novel_ids_input = novel_id_input.value.strip()
        
        if not novel_ids_input:
            self._update_status(get_global_i18n().t('crawler.enter_novel_id'))
            return
        
        # åˆ†å‰²å¤šä¸ªå°è¯´ID
        novel_ids = [id.strip() for id in novel_ids_input.split(',') if id.strip()]
        
        # å¯¹æ¯ä¸ªä¹¦ç±IDè¿›è¡ŒURLè§£ç ï¼ˆå¦‚æœæ˜¯URLç¼–ç çš„è¯ï¼‰
        novel_ids = [unquote(id) for id in novel_ids]
        
        if not novel_ids:
            self._update_status(get_global_i18n().t('crawler.enter_novel_id'))
            return
        
        # éªŒè¯æ¯ä¸ªå°è¯´IDæ ¼å¼ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼šæ•°å­—ã€å­—æ¯ã€ä¸­æ–‡ã€æ—¥æœŸè·¯å¾„ç­‰ï¼‰
        invalid_ids = []
        for novel_id in novel_ids:
            # æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
            # 1. 2022/02/blog-post_70 (æ—¥æœŸè·¯å¾„æ ¼å¼)
            # 2. ä¸­æ–‡æ ‡é¢˜å (çº¯ä¸­æ–‡)
            # 3. 2025/06/09/ä¸­æ–‡æ ‡é¢˜ (æ··åˆæ ¼å¼)
            # 4. æ•°å­—å­—æ¯ç»„åˆ (å¦‚68fa7dcff3de0)
            if not novel_id:
                invalid_ids.append(novel_id)
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«éæ³•å­—ç¬¦ï¼ˆç®€åŒ–éªŒè¯ï¼Œä¸»è¦æ’é™¤è‹±æ–‡é€—å·ä½œä¸ºåˆ†éš”ç¬¦ï¼‰
            # å…è®¸çš„å­—ç¬¦ï¼šå­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ã€å¸¸è§æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼ç­‰
            # æ³¨æ„ï¼šè‹±æ–‡é€—å·(,)ç”¨äºåˆ†éš”å¤šä¸ªIDï¼Œæ‰€ä»¥ä¸èƒ½åœ¨å•ä¸ªIDä¸­ä½¿ç”¨
            if ',' in novel_id:
                invalid_ids.append(novel_id)
        
        if invalid_ids:
            self._update_status(f"{get_global_i18n().t('crawler.invalid_novel_id')}: {', '.join(invalid_ids)}")
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡ä¸”æ–‡ä»¶å­˜åœ¨
        site_id = self.novel_site.get('id')
        existing_novels = []
        if site_id:
            for novel_id in novel_ids:
                if self.db_manager.check_novel_exists(site_id, novel_id):
                    existing_novels.append(novel_id)
        
        if existing_novels:
            # è‡ªåŠ¨è·³è¿‡å¹¶æ¸…ç†å·²å­˜åœ¨çš„ID
            try:
                for _eid in existing_novels:
                    # æ¸…ç†è¾“å…¥æ¡†ä¸­çš„å·²å­˜åœ¨ID
                    self.app.call_later(self._remove_id_from_input, _eid)
                    # å•ç‹¬æç¤ºæ¯ä¸ªè¢«è·³è¿‡çš„ID
                    try:
                        self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.skipped')}: {_eid}", "information")
                    except Exception:
                        pass
            except Exception:
                pass
            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„IDï¼Œç»§ç»­çˆ¬å–å‰©ä½™çš„
            novel_ids = [nid for nid in novel_ids if nid not in existing_novels]
            if not novel_ids:
                self._update_status(f"{get_global_i18n().t('crawler.novel_already_exists')}: {', '.join(existing_novels)}")
                # é‡ç½®çˆ¬å–çŠ¶æ€å’ŒæŒ‰é’®çŠ¶æ€
                self.is_crawling = False
                self._update_crawl_button_state()
                return
            else:
                # æ±‡æ€»æç¤ºï¼Œç»§ç»­çˆ¬å–å‰©ä½™ID
                self._update_status(f"{get_global_i18n().t('crawler.novel_already_exists')}: {', '.join(existing_novels)}ï¼Œ{get_global_i18n().t('crawler.skip')}", "information")
        
        # æ£€æŸ¥ä»£ç†è¦æ±‚
        proxy_check_result = self._check_proxy_requirements_sync()
        if not proxy_check_result['can_proceed']:
            self._update_status(proxy_check_result['message'], "error")
            return
        
        proxy_config = proxy_check_result['proxy_config']
        
        # æ¸…ç©ºä¹‹å‰çš„æç¤ºä¿¡æ¯
        self._update_status("")
        
        # ä½¿ç”¨åå°çˆ¬å–ç®¡ç†å™¨å¯åŠ¨ä»»åŠ¡
        site_id = self.novel_site.get('id')
        if not site_id:
            # å›æ»šçˆ¬å–çŠ¶æ€
            self.is_crawling = False
            self._update_crawl_button_state()
            self._update_status(get_global_i18n().t('crawler.no_site_id'), "error")
            return
        
        # å¯åŠ¨åå°çˆ¬å–ä»»åŠ¡
        try:
            task_id = self.crawler_manager.start_crawl_task(site_id, novel_ids, proxy_config)
            if not task_id:
                # ä»»åŠ¡å¯åŠ¨å¤±è´¥ï¼Œå›æ»šçŠ¶æ€
                self.is_crawling = False
                self._update_crawl_button_state()
                self._update_status(get_global_i18n().t('crawler.start_crawl_failed'), "error")
                return
            
            self.current_task_id = task_id
            
            # æ˜¾ç¤ºå¯åŠ¨çŠ¶æ€
            self._update_status(f"{get_global_i18n().t('crawler.starting_crawl')} ({len(novel_ids)} {get_global_i18n().t('crawler.books')})")
            
        except Exception as e:
            # å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œå›æ»šçŠ¶æ€
            self.is_crawling = False
            self._update_crawl_button_state()
            logger.error(f"å¯åŠ¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.start_crawl_failed')}: {str(e)}", "error")
            return
        
        # çŠ¶æ€æ›´æ–°ç”±å›è°ƒå‡½æ•°å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
    
    def _check_proxy_requirements_sync(self) -> Dict[str, Any]:
        """
        åŒæ­¥æ£€æŸ¥ä»£ç†è¦æ±‚
        
        Returns:
            åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        try:
            # æ£€æŸ¥ç½‘ç«™æ˜¯å¦å¯ç”¨äº†ä»£ç†
            proxy_enabled = self.novel_site.get('proxy_enabled', False)
            
            if not proxy_enabled:
                # ç½‘ç«™æœªå¯ç”¨ä»£ç†ï¼Œè¿”å›ç©ºä»£ç†é…ç½®
                return {
                    'can_proceed': True,
                    'proxy_config': {
                        'enabled': False,
                        'proxy_url': ''
                    },
                    'message': get_global_i18n().t('crawler.not_enabled_proxy')
                }
            
            # ç½‘ç«™å¯ç”¨äº†ä»£ç†ï¼Œè·å–å¯ç”¨çš„ä»£ç†è®¾ç½®
            enabled_proxy = self.db_manager.get_enabled_proxy()
            
            if not enabled_proxy:
                # æ²¡æœ‰å¯ç”¨çš„ä»£ç†ï¼Œæç¤ºç”¨æˆ·
                return {
                    'can_proceed': False,
                    'proxy_config': None,
                    'message': get_global_i18n().t('crawler.need_proxy')
                }
            
            # æ„å»ºä»£ç†URL
            proxy_type = enabled_proxy.get('type', 'HTTP').lower()
            host = enabled_proxy.get('host', '')
            port = enabled_proxy.get('port', '')
            username = enabled_proxy.get('username', '')
            password = enabled_proxy.get('password', '')
            
            if not host or not port:
                return {
                    'can_proceed': False,
                    'proxy_config': None,
                    'message': get_global_i18n().t('crawler.proxy_error')
                }
            
            # æ„å»ºä»£ç†URL
            if username and password:
                proxy_url = f"{proxy_type}://{username}:{password}@{host}:{port}"
            else:
                proxy_url = f"{proxy_type}://{host}:{port}"
            
            # æµ‹è¯•ä»£ç†è¿æ¥
            proxy_test_result = self._test_proxy_connection(proxy_url)
            if not proxy_test_result:
                return {
                    'can_proceed': False,
                    'proxy_config': None,
                    'message': get_global_i18n().t("crawler.proxy_error_url", proxy_url=proxy_url)
                }
            
            return {
                'can_proceed': True,
                'proxy_config': {
                    'enabled': True,
                    'proxy_url': proxy_url,
                    'name': enabled_proxy.get('name', get_global_i18n().t("crawler.unnamed_proxy"))
                },
                'message': f"{get_global_i18n().t("crawler.use_proxy")}: {enabled_proxy.get('name', get_global_i18n().t("crawler.unnamed_proxy"))} ({host}:{port})"
            }
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä»£ç†è¦æ±‚å¤±è´¥: {e}")
            return {
                'can_proceed': False,
                'proxy_config': None,
                'message': f'{get_global_i18n().t("crawler.check_proxy_failed")}: {str(e)}'
            }

    def _test_proxy_connection(self, proxy_url: str) -> bool:
        """
        æµ‹è¯•ä»£ç†è¿æ¥æ˜¯å¦å¯ç”¨
        
        Args:
            proxy_url: ä»£ç†URL
            
        Returns:
            bool: ä»£ç†æ˜¯å¦å¯ç”¨
        """
        import requests
        import time
        
        try:
            # è®¾ç½®ä»£ç†
            proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            
            # æµ‹è¯•è¿æ¥ - å…ˆä½¿ç”¨ç®€å•çš„ç½‘ç«™æµ‹è¯•ä»£ç†è¿é€šæ€§
            test_urls = [
                "http://httpbin.org/ip",  # æµ‹è¯•ä»£ç†IP
                "https://www.baidu.com",  # å¤‡ç”¨æµ‹è¯•ç«™ç‚¹
                "https://www.renqixiaoshuo.net"  # ç›®æ ‡ç½‘ç«™
            ]
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´ - å¢åŠ åˆ°30ç§’
            timeout = 30
            connect_timeout = 15  # è¿æ¥è¶…æ—¶
            read_timeout = 15     # è¯»å–è¶…æ—¶
            
            for test_url in test_urls:
                try:
                    start_time = time.time()
                    response = requests.get(
                        test_url, 
                        proxies=proxies, 
                        timeout=(connect_timeout, read_timeout),
                        stream=True,  # ä½¿ç”¨æµå¼ä¸‹è½½ï¼Œé¿å…å¤§æ–‡ä»¶ä¸‹è½½å¡ä½
                        headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                            'Connection': 'keep-alive'
                        }
                    )
                    
                    # åªè¯»å–å‰1KBå†…å®¹æ¥éªŒè¯è¿æ¥
                    content = response.raw.read(1024)
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        logger.info(f"{get_global_i18n().t('crawler.test_success')}: {proxy_url} ({get_global_i18n().t('crawler.response_time')}: {end_time - start_time:.2f}s)")
                        return True
                    else:
                        logger.warning(f"æµ‹è¯•ç«™ç‚¹ {test_url} è¿”å›çŠ¶æ€ç : {response.status_code}")
                        # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªURL
                        
                except requests.exceptions.ConnectTimeout:
                    logger.warning(f"ä»£ç†è¿æ¥è¶…æ—¶ (è¿æ¥è¶…æ—¶ {connect_timeout}s): {proxy_url}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªURL
                except requests.exceptions.ReadTimeout:
                    logger.warning(f"ä»£ç†è¯»å–è¶…æ—¶ (è¯»å–è¶…æ—¶ {read_timeout}s): {proxy_url}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªURL
                except requests.exceptions.ConnectionError as e:
                    logger.warning(f"ä»£ç†è¿æ¥é”™è¯¯: {proxy_url}, é”™è¯¯: {e}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªURL
                except Exception as e:
                    logger.warning(f"ä»£ç†æµ‹è¯•å¼‚å¸¸ (URL: {test_url}): {e}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªURL
            
            # æ‰€æœ‰URLéƒ½å¤±è´¥
            logger.error(f"ä»£ç†æµ‹è¯•å¤±è´¥: æ‰€æœ‰æµ‹è¯•URLéƒ½æ— æ³•è¿æ¥")
            return False
                
        except Exception as e:
            logger.error(f"ä»£ç†æµ‹è¯•å¼‚å¸¸: {e}")
            return False

    async def _actual_crawl_multiple(self, novel_ids: List[str], proxy_config: Dict[str, Any]) -> None:
        """å®é™…çˆ¬å–å¤šä¸ªå°è¯´ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰"""
        import asyncio
        import time
        
        # å¼€å§‹çˆ¬å– - ä½¿ç”¨app.call_lateræ¥å®‰å…¨åœ°æ›´æ–°UI
        self.app.call_later(self._update_status, get_global_i18n().t("crawler.start_to_crawler_books", counts=len(novel_ids)))
        
        try:
            # è·å–è§£æå™¨åç§°
            parser_name = self.novel_site.get('parser')
            if not parser_name:
                self.app.call_later(self._update_status, get_global_i18n().t("crawler.no_parser"), "error")
                return
            
            # å¯¼å…¥è§£æå™¨
            from src.spiders import create_parser
            
            # åˆ›å»ºè§£æå™¨å®ä¾‹ï¼Œä¼ é€’æ•°æ®åº“ä¸­çš„ç½‘ç«™åç§°ä½œä¸ºä½œè€…ä¿¡æ¯å’Œç½‘ç«™URL
            parser = create_parser(parser_name, proxy_config, self.novel_site.get('name'), self.novel_site.get('url'))
            
            if not parser:
                self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.parser_not_found')}: {parser_name}", "error")
                return
            
            # çˆ¬å–æ¯ä¸ªå°è¯´
            success_count = 0
            failed_count = 0
            
            for i, novel_id in enumerate(novel_ids):
                if not self.is_crawling:
                    self.app.call_later(self._update_status, get_global_i18n().t('crawler.crawl_stopped'))
                    break
                
                # æ›´æ–°å½“å‰çˆ¬å–çŠ¶æ€
                self.current_crawling_id = novel_id
                self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.crawling')} ({i+1}/{len(novel_ids)}): {novel_id}")
                
                try:
                    # æ‰§è¡Œçˆ¬å–
                    result = await self._async_parse_novel_detail(parser, novel_id)
                    
                    if result['success']:
                        success_count += 1
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        site_id = self.novel_site.get('id')
                        if site_id:
                            self.db_manager.add_crawl_history(
                                site_id=site_id,
                                novel_id=novel_id,
                                novel_title=result['title'],
                                status="success",
                                file_path=result['file_path'],
                                error_message=""
                            )
                        
                        self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.crawl_success')}: {novel_id}")
                        # çˆ¬å–æˆåŠŸåæ¸…ç†è¾“å…¥æ¡†ä¸­çš„ID
                        self.app.call_later(self._remove_id_from_input, novel_id)
                    else:
                        failed_count += 1
                        
                        # ä¿å­˜å¤±è´¥è®°å½•åˆ°æ•°æ®åº“
                        site_id = self.novel_site.get('id')
                        if site_id:
                            self.db_manager.add_crawl_history(
                                site_id=site_id,
                                novel_id=novel_id,
                                novel_title=novel_id,
                                status="failed",
                                file_path="",
                                error_message=result.get('error_message', get_global_i18n().t('crawler.unknown_error'))
                            )
                        
                        self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.crawl_failed')}: {novel_id} - {result.get('error_message', get_global_i18n().t('crawler.unknown_error'))}", "error")
                
                except Exception as e:
                    failed_count += 1
                    logger.error(f"çˆ¬å–å°è¯´ {novel_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    
                    # ä¿å­˜å¼‚å¸¸è®°å½•åˆ°æ•°æ®åº“
                    site_id = self.novel_site.get('id')
                    if site_id:
                        self.db_manager.add_crawl_history(
                            site_id=site_id,
                            novel_id=novel_id,
                            novel_title=novel_id,
                            status="failed",
                            file_path="",
                            error_message=str(e)
                        )
                    
                    self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.crawl_exception')}: {novel_id} - {str(e)}", "error")
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                await asyncio.sleep(1)
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            if self.is_crawling:
                self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.crawl_completed')}: {success_count} {get_global_i18n().t('crawler.success')}, {failed_count} {get_global_i18n().t('crawler.failed')}")
                
                # åˆ·æ–°å†å²è®°å½•
                self.app.call_later(self._load_crawl_history)
            
        except Exception as e:
            logger.error(f"æ‰¹é‡çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.batch_crawl_exception')}: {str(e)}", "error")
        
        finally:
            # é‡ç½®çˆ¬å–çŠ¶æ€
            self.is_crawling = False
            self.current_crawling_id = None
            self.app.call_later(self._update_crawl_button_state)
            self.app.call_later(self._hide_loading_animation)
    
    def _remove_id_from_input(self, novel_id: str) -> None:
        """ä»è¾“å…¥æ¡†ä¸­ç§»é™¤æŒ‡å®šçš„ID"""
        try:
            # å¯¹è¦ç§»é™¤çš„IDè¿›è¡ŒURLè§£ç ï¼Œç¡®ä¿èƒ½æ­£ç¡®åŒ¹é…
            decoded_novel_id = unquote(novel_id)
            novel_id_input = self.query_one("#novel-id-input", Input)
            current_value = novel_id_input.value.strip()
            
            # åˆ†å‰²å¹¶å¯¹æ¯ä¸ªIDè¿›è¡ŒURLè§£ç ï¼Œç„¶åè¿‡æ»¤æ‰æŒ‡å®šçš„ID
            ids = [unquote(id.strip()) for id in current_value.split(',') if id.strip()]
            filtered_ids = [id for id in ids if id != decoded_novel_id]
            
            # é‡æ–°ç»„åˆå¹¶æ›´æ–°è¾“å…¥æ¡†
            if filtered_ids:
                # å¦‚æœè¿˜æœ‰å…¶ä»–IDï¼Œåœ¨æœ€åä¸€ä¸ªIDåé¢æ·»åŠ é€—å·ï¼Œæ–¹ä¾¿ç»§ç»­è¾“å…¥
                novel_id_input.value = ', '.join(filtered_ids) + ','
            else:
                # å¦‚æœæ²¡æœ‰å…¶ä»–IDäº†ï¼Œæ¸…ç©ºè¾“å…¥æ¡†
                novel_id_input.value = ''
                
            # å°†å…‰æ ‡ç§»åŠ¨åˆ°è¾“å…¥æ¡†æœ«å°¾
            novel_id_input.action_end()
        except Exception as e:
            logger.debug(f"ä»è¾“å…¥æ¡†ä¸­ç§»é™¤IDå¤±è´¥: {e}")
    
    def _update_crawl_button_state(self) -> None:
        """æ›´æ–°çˆ¬å–æŒ‰é’®çŠ¶æ€"""
        try:
            # ç¡®ä¿ç»„ä»¶å·²ç»æŒ‚è½½
            if not self.is_mounted_flag:
                logger.debug("ç»„ä»¶å°šæœªæŒ‚è½½ï¼Œå»¶è¿Ÿæ›´æ–°æŒ‰é’®çŠ¶æ€")
                # å»¶è¿Ÿ100msåé‡è¯•
                self.set_timer(0.1, self._update_crawl_button_state)
                return

            # ä½¿ç”¨æ­£ç¡®çš„CSSé€‰æ‹©å™¨è¯­æ³•ï¼Œéœ€è¦#å·
            start_crawl_button = self.query_one("#start-crawl-btn", Button)
            stop_crawl_button = self.query_one("#stop-crawl-btn", Button)
            
            if self.is_crawling:
                start_crawl_button.label = get_global_i18n().t('crawler.crawling_in_progress')
                start_crawl_button.disabled = True
                stop_crawl_button.disabled = False
            else:
                start_crawl_button.label = get_global_i18n().t('crawler.start_crawl')
                start_crawl_button.disabled = False
                stop_crawl_button.disabled = True
            
            logger.debug("çˆ¬å–æŒ‰é’®çŠ¶æ€æ›´æ–°æˆåŠŸ")
        except Exception as e:
            # å¦‚æœæŒ‰é’®ä¸å­˜åœ¨ï¼Œè®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
            logger.debug(f"æ›´æ–°çˆ¬å–æŒ‰é’®çŠ¶æ€å¤±è´¥: {e}")
            # å»¶è¿Ÿé‡è¯•
            self.set_timer(0.1, self._update_crawl_button_state)
    
    def _show_loading_animation(self) -> None:
        """æ˜¾ç¤ºåŠ è½½åŠ¨ç”»"""
        try:
            if self.loading_indicator:
                self.loading_indicator.styles.display = "block"
        except Exception as e:
            logger.error(f"æ˜¾ç¤ºåŠ è½½åŠ¨ç”»å¤±è´¥: {e}")
    
    def _hide_loading_animation(self) -> None:
        """éšè—åŠ è½½åŠ¨ç”»"""
        try:
            if self.loading_indicator:
                self.loading_indicator.styles.display = "none"
        except Exception as e:
            logger.error(f"éšè—åŠ è½½åŠ¨ç”»å¤±è´¥: {e}")
    
    def _reset_crawl_state(self) -> None:
        """é‡ç½®çˆ¬å–çŠ¶æ€"""
        try:
            # ç¡®ä¿ç»„ä»¶å·²ç»æŒ‚è½½
            if not self.is_mounted_flag:
                return
            
            # é‡ç½®æ‰€æœ‰çˆ¬å–ç›¸å…³çš„çŠ¶æ€
            self.is_crawling = False
            self.current_crawling_id = None
            self.current_task_id = None
            
            # æ›´æ–°UIçŠ¶æ€
            self._update_crawl_button_state()
            self._hide_loading_animation()
            
            # é‡ç½®çŠ¶æ€æ˜¾ç¤º
            self.app.call_later(self._update_status, get_global_i18n().t('crawler.ready'))
            
            logger.debug("çˆ¬å–çŠ¶æ€å·²é‡ç½®")
        except Exception as e:
            logger.error(f"é‡ç½®çˆ¬å–çŠ¶æ€å¤±è´¥: {e}")
    
    def _sync_ui_state_with_crawler(self) -> None:
        """åŒæ­¥UIçŠ¶æ€ä¸çˆ¬å–å™¨çŠ¶æ€"""
        try:
            from src.core.crawler_manager import CrawlStatus
            
            # æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            if self.current_task_id:
                task = self.crawler_manager.get_task_by_id(self.current_task_id)
                if task and task.status != CrawlStatus.COMPLETED and task.status != CrawlStatus.FAILED:
                    # ä»»åŠ¡ä»åœ¨è¿è¡Œï¼ŒåŒæ­¥çŠ¶æ€
                    self.is_crawling = True
                    self.current_crawling_id = task.current_novel_id
                else:
                    # ä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œé‡ç½®çŠ¶æ€
                    self._reset_crawl_state()
            else:
                # æ²¡æœ‰ä»»åŠ¡ï¼Œç¡®ä¿çŠ¶æ€æ­£ç¡®
                self._reset_crawl_state()
            
            # æ›´æ–°UI
            self._update_crawl_button_state()
            
        except Exception as e:
            logger.error(f"åŒæ­¥UIçŠ¶æ€å¤±è´¥: {e}")
            # å¦‚æœåŒæ­¥å¤±è´¥ï¼Œä¿å®ˆåœ°é‡ç½®çŠ¶æ€
            self._reset_crawl_state()
            self.set_timer(0.1, self._reset_crawl_state)
            return

            self.is_crawling = False
            self._update_crawl_button_state()
            self._hide_loading_animation()
            
            # è‡ªåŠ¨ç»§ç»­çˆ¬å–å‰©ä½™IDï¼ˆå¦‚æœè¾“å…¥æ¡†ä¸­è¿˜æœ‰ï¼‰
            try:
                novel_id_input = self.query_one("#novel-id-input", Input)
                raw = (novel_id_input.value or "").strip()
                remaining_ids = [i.strip() for i in raw.split(",") if i.strip()]
                if remaining_ids and not self.is_crawling:
                    # åœ¨UIåˆ·æ–°åè§¦å‘ä¸‹ä¸€è½®çˆ¬å–
                    self.call_after_refresh(self._start_crawl)
            except Exception as e:
                logger.debug(f"é‡ç½®çˆ¬å–çŠ¶æ€å¤±è´¥: {e}")
                # å»¶è¿Ÿé‡è¯•
                self.set_timer(0.1, self._reset_crawl_state)
    
    async def _async_parse_novel_detail(self, parser, novel_id: str) -> Dict[str, Any]:
        """å¼‚æ­¥è§£æå°è¯´è¯¦æƒ…
        
        Args:
            parser: è§£æå™¨å®ä¾‹
            novel_id: å°è¯´ID
            
        Returns:
            Dict[str, Any]: è§£æç»“æœ
        """
        import asyncio
        import aiohttp
        
        try:
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡Œç½‘ç»œè¯·æ±‚
            await asyncio.sleep(0.5)  # æ·»åŠ å°å»¶è¿Ÿé¿å…åŒæ—¶è¯·æ±‚è¿‡å¤š
            
            # å¼‚æ­¥è·å–å°è¯´å†…å®¹
            novel_url = parser.get_novel_url(novel_id)
            
            # å‡†å¤‡ä»£ç†é…ç½®
            proxies = None
            if parser.proxy_config.get('enabled', False):
                proxy_url = parser.proxy_config.get('proxy_url', '')
                if proxy_url:
                    proxies = proxy_url
            
            # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥è¯·æ±‚
            async with aiohttp.ClientSession(headers=parser.session.headers) as session:
                try:
                    # è®¾ç½®è¶…æ—¶æ—¶é—´
                    timeout = aiohttp.ClientTimeout(total=60, connect=15)
                    
                    # å‘é€å¼‚æ­¥è¯·æ±‚
                    async with session.get(novel_url, proxy=proxies, timeout=timeout) as response:
                        if response.status == 200:
                            # è¯»å–å†…å®¹ï¼Œå¤„ç†ç¼–ç é—®é¢˜
                            try:
                                # å…ˆå°è¯•ä½¿ç”¨UTF-8è§£ç 
                                content = await response.text()
                            except UnicodeDecodeError:
                                # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–ç¼–ç 
                                raw_content = await response.read()
                                # å°è¯•å¤šç§ç¼–ç 
                                encodings = ['utf-8', 'gbk', 'gb2312', 'big5', 'latin1']
                                content = None
                                for encoding in encodings:
                                    try:
                                        content = raw_content.decode(encoding)
                                        logger.debug(f"ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè§£ç å†…å®¹")
                                        break
                                    except UnicodeDecodeError:
                                        continue
                                if content is None:
                                    logger.warning("æ— æ³•è§£ç å†…å®¹ï¼Œä½¿ç”¨latin1ä½œä¸ºæœ€åæ‰‹æ®µ")
                                    content = raw_content.decode('latin1', errors='ignore')
                            
                            # æ£€æµ‹å¹¶å¤„ç†é«˜çº§åçˆ¬è™«æœºåˆ¶
                            if parser._detect_advanced_anti_bot(content):
                                logger.warning(f"æ£€æµ‹åˆ°é«˜çº§åçˆ¬è™«æœºåˆ¶ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•: {novel_url}")
                                # å¦‚æœæ£€æµ‹åˆ°é«˜çº§åçˆ¬è™«ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•
                                content = parser._get_url_content(novel_url)
                                if not content:
                                    return {
                                        'success': False,
                                        'error_message': f"æ— æ³•è·å–å°è¯´é¡µé¢: {novel_url}"
                                    }
                        else:
                            logger.warning(f"HTTP {response.status} è·å–å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•: {novel_url}")
                            # å¦‚æœå¼‚æ­¥è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•
                            content = parser._get_url_content(novel_url)
                            if not content:
                                return {
                                    'success': False,
                                    'error_message': f"æ— æ³•è·å–å°è¯´é¡µé¢: {novel_url}"
                                }
                except Exception as async_error:
                    logger.warning(f"å¼‚æ­¥è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•: {async_error}")
                    # å¦‚æœå¼‚æ­¥è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ–¹æ³•
                    content = parser._get_url_content(novel_url)
                    if not content:
                        return {
                            'success': False,
                            'error_message': f"æ— æ³•è·å–å°è¯´é¡µé¢: {novel_url}"
                        }
            
            # è‡ªåŠ¨æ£€æµ‹ä¹¦ç±ç±»å‹
            book_type = parser._detect_book_type(content)
            
            # æå–æ ‡é¢˜
            title = parser._extract_with_regex(content, parser.title_reg)
            if not title:
                return {
                    'success': False,
                    'error_message': "æ— æ³•æå–å°è¯´æ ‡é¢˜"
                }
            
            # æ ¹æ®ä¹¦ç±ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            if book_type == "å¤šç« èŠ‚":
                novel_content = parser._parse_multichapter_novel(content, novel_url, title)
            elif book_type == "å†…å®¹é¡µå†…åˆ†é¡µ":
                novel_content = parser._parse_content_pagination_novel(content, novel_url, title)
            else:
                novel_content = parser._parse_single_chapter_novel(content, novel_url, title)
            
            # è·å–å­˜å‚¨æ–‡ä»¶å¤¹
            storage_folder = self.novel_site.get('storage_folder', 'novels')
            # å±•å¼€è·¯å¾„ä¸­çš„ ~ ç¬¦å·
            storage_folder = os.path.expanduser(storage_folder)
            
            # ä¿å­˜å°è¯´åˆ°æ–‡ä»¶
            file_path = parser.save_to_file(novel_content, storage_folder)

            if file_path == 'already_exists':
                return {
                    'success': False,
                    'error_message': 'File exists'
                }
            
            return {
                'success': True,
                'title': novel_content.get('title', novel_id),
                'file_path': file_path
            }
            
        except Exception as e:
            logger.error(f"å¼‚æ­¥è§£æå°è¯´è¯¦æƒ…å¤±è´¥: {e}")
            return {
                'success': False,
                'error_message': str(e)
            }
    
    def _detect_advanced_anti_bot(self, content: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦å­˜åœ¨é«˜çº§åçˆ¬è™«æœºåˆ¶ï¼ˆå¦‚ Cloudflare Turnstileï¼‰
        
        Args:
            content: é¡µé¢å†…å®¹
            
        Returns:
            æ˜¯å¦å­˜åœ¨é«˜çº§åçˆ¬è™«æœºåˆ¶
        """
        try:
            import re
            turnstile_patterns = [
                r'challenges\.cloudflare\.com',
                r'cf-turnstile',
                r'data-sitekey',
                r'turnstile\.cloudflare\.com'
            ]
            
            for pattern in turnstile_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    return True
            
            return False
        except Exception:
            return False
    
    
    
    def _view_file(self, history_item: Dict[str, Any]) -> None:
        """æŸ¥çœ‹æ–‡ä»¶"""
        try:
            file_path = history_item.get('file_path')
            
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸ºç©ºæˆ–ä¸º"already_exists"ï¼Œå°è¯•ä»æ•°æ®åº“ä¸­é‡æ–°è·å–
            if not file_path or file_path == 'already_exists':
                site_id = self.novel_site.get('id')
                novel_id = history_item.get('novel_id')
                if site_id and novel_id:
                    crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
                    if crawl_history:
                        # è·å–æœ€æ–°çš„æˆåŠŸè®°å½•
                        for record in crawl_history:
                            if record.get('status') == 'success' and record.get('file_path') and record.get('file_path') != 'already_exists':
                                file_path = record.get('file_path')
                                # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                                history_item['file_path'] = file_path
                                break
            
            if not file_path:
                self._update_status(get_global_i18n().t('crawler.no_file_path'))
                return
                
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä»ç„¶æ˜¯"already_exists"ï¼Œå°è¯•æŸ¥æ‰¾å®é™…æ–‡ä»¶
            if file_path == 'already_exists':
                # å°è¯•æ ¹æ®å°è¯´æ ‡é¢˜æŸ¥æ‰¾æ–‡ä»¶
                novel_title = history_item.get('novel_title', '')
                storage_folder = self.novel_site.get('storage_folder', 'novels')
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
                storage_folder = os.path.expanduser(storage_folder)
                possible_files = glob.glob(os.path.join(storage_folder, f"*{novel_title}*"))
                if possible_files:
                    file_path = possible_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                    # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                    history_item['file_path'] = file_path
                else:
                    self._update_status(f"{get_global_i18n().t('crawler.file_not_exists')}{get_global_i18n().t('crawler.not_found')}")
                    return
            
            if not os.path.exists(file_path):
                # å°è¯•æ›´æ–°æ•°æ®åº“è®°å½•çŠ¶æ€ä¸ºå¤±è´¥
                site_id = self.novel_site.get('id')
                novel_id = history_item.get('novel_id')
                if site_id and novel_id:
                    # æŸ¥æ‰¾å¹¶æ›´æ–°è®°å½•
                    crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
                    for record in crawl_history:
                        if record.get('file_path') == file_path:
                            # æ›´æ–°çŠ¶æ€ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
                            self.db_manager.update_crawl_history_status(
                                site_id=site_id,
                                novel_id=novel_id,
                                status='failed',
                                file_path=file_path,
                                novel_title=history_item.get('novel_title', ''),
                                error_message='æ–‡ä»¶ä¸å­˜åœ¨'
                            )
                            # é‡æ–°åŠ è½½å†å²è®°å½•
                            self._load_crawl_history()
                            break
                
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return
                
            # åœ¨æ–‡ä»¶ç®¡ç†å™¨ä¸­æ˜¾ç¤ºæ–‡ä»¶
            import platform
            system = platform.system()
            
            if system == "Darwin":  # macOS
                os.system(f'open -R "{file_path}"')
            elif system == "Windows":
                os.system(f'explorer /select,"{file_path}"')
            elif system == "Linux":
                os.system(f'xdg-open "{os.path.dirname(file_path)}"')
                
            self._update_status(get_global_i18n().t('crawler.file_opened'))
            
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.open_file_failed')}: {str(e)}", "error")

    def _delete_file_only(self, history_item: Dict[str, Any]) -> None:
        """åªåˆ é™¤æ–‡ä»¶ï¼Œä¸åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆåŒæ—¶åˆ é™¤ä¹¦æ¶ä¸­çš„å¯¹åº”ä¹¦ç±ï¼‰"""
        try:
            file_path = history_item.get('file_path')
            if not file_path:
                self._update_status(get_global_i18n().t('crawler.no_file_path'))
                return
                
            import os
            if not os.path.exists(file_path):
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return
                
            # ç¡®è®¤åˆ é™¤
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_delete_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        # åªåˆ é™¤æ–‡ä»¶ï¼Œä¸åˆ é™¤æ•°æ®åº“è®°å½•
                        send2trash(file_path)
                        logger.info(f"æ–‡ä»¶å·²ç§»è‡³å›æ”¶ç«™: {file_path}")
                        
                        # åŒæ—¶åˆ é™¤ä¹¦æ¶ä¸­çš„å¯¹åº”ä¹¦ç±
                        try:
                            # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„åˆ é™¤ä¹¦æ¶ä¸­çš„ä¹¦ç±
                            if self.db_manager.delete_book(file_path):
                                # å‘é€å…¨å±€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯ï¼Œç¡®ä¿ä¹¦æ¶å±å¹•èƒ½å¤Ÿæ¥æ”¶
                                try:
                                    from src.ui.messages import RefreshBookshelfMessage
                                    self.app.post_message(RefreshBookshelfMessage())
                                    logger.info("å·²å‘é€ä¹¦æ¶åˆ·æ–°æ¶ˆæ¯ï¼Œä¹¦ç±å·²ä»ä¹¦æ¶åˆ é™¤")
                                    self._update_status(f"{get_global_i18n().t('crawler.file_deleted')}ï¼Œ{get_global_i18n().t('crawler.book_deleted')}")
                                except Exception as msg_error:
                                    logger.debug(f"å‘é€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯å¤±è´¥: {msg_error}")
                                    self._update_status(f"{get_global_i18n().t('crawler.file_deleted')}ï¼Œ{get_global_i18n().t('crawler.refresh_failed')}")
                            else:
                                # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œæ£€æŸ¥ä¹¦ç±æ˜¯å¦å­˜åœ¨äºä¹¦æ¶ä¸­
                                books = self.db_manager.get_all_books()
                                book_exists = any(book.path == file_path for book in books)
                                if book_exists:
                                    self._update_status(f"{get_global_i18n().t('crawler.file_deleted')}ï¼Œ{get_global_i18n().t('crawler.delete_failed')}")
                                else:
                                    self._update_status(get_global_i18n().t('crawler.file_deleted'))
                        except Exception as shelf_error:
                            logger.error(f"åˆ é™¤ä¹¦æ¶ä¹¦ç±å¤±è´¥: {shelf_error}")
                            self._update_status(f"{get_global_i18n().t('crawler.file_deleted')}ï¼Œ{get_global_i18n().t('crawler.delete_error')}")
                        
                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()
                    except Exception as e:
                        self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))
            
            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    f"{get_global_i18n().t('crawler.confirm_delete')}ï¼ˆ{get_global_i18n().t('crawler.delete_file_and_book')}ï¼‰",
                    f"{get_global_i18n().t('crawler.confirm_delete_message')} {get_global_i18n().t('crawler.delete_book_tip')}"
                ),
                handle_delete_confirmation
            )
            
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")

    def _delete_record_only(self, history_item: Dict[str, Any]) -> None:
        """åªåˆ é™¤æ•°æ®åº“è®°å½•ï¼Œä¸åˆ é™¤æ–‡ä»¶"""
        try:
            # ç¡®è®¤åˆ é™¤
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_delete_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        # åªåˆ é™¤æ•°æ®åº“è®°å½•ï¼Œä¸åˆ é™¤æ–‡ä»¶
                        history_id = history_item.get('id')
                        if history_id:
                            self.db_manager.delete_crawl_history(history_id)
                        
                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()
                        self._update_status(get_global_i18n().t('crawler.file_deleted'))
                    except Exception as e:
                        self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))
            
            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    f"{get_global_i18n().t('crawler.confirm_delete')}ï¼ˆ{get_global_i18n().t('crawler.only_delete')}ï¼‰",
                    f"{get_global_i18n().t('crawler.confirm_delete_message')}"
                ),
                handle_delete_confirmation
            )
            
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")
    
    def _delete_file(self, history_item: Dict[str, Any]) -> None:
        """åˆ é™¤æ–‡ä»¶ï¼ˆåŒæ—¶åˆ é™¤æ–‡ä»¶å’Œè®°å½•ï¼‰"""
        try:
            file_path = history_item.get('file_path')
            if not file_path:
                self._update_status(get_global_i18n().t('crawler.no_file_path'))
                return
                
            import os
            if not os.path.exists(file_path):
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return
                
            # ç¡®è®¤åˆ é™¤
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_delete_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        # å…ˆåˆ é™¤æ–‡ä»¶
                        send2trash(file_path)
                        logger.info(f"æ–‡ä»¶å·²ç§»è‡³å›æ”¶ç«™: {file_path}")
                        
                        # ä»æ•°æ®åº“ä¸­åˆ é™¤å¯¹åº”çš„è®°å½•
                        history_id = history_item.get('id')
                        if history_id:
                            self.db_manager.delete_crawl_history(history_id)
                        
                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()
                        self._update_status(get_global_i18n().t('crawler.file_deleted'))
                    except Exception as e:
                        self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))
            
            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    f"{get_global_i18n().t('crawler.confirm_delete')}ï¼ˆ{get_global_i18n().t('crawler.both_file_data')}ï¼‰",
                    f"{get_global_i18n().t('crawler.confirm_delete_message')}"
                ),
                handle_delete_confirmation
            )
            
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.delete_file_failed')}: {str(e)}", "error")
    
    def on_key(self, event: events.Key) -> None:
        """å¤„ç†é”®ç›˜äº‹ä»¶"""
        table = self.query_one("#crawl-history-table", DataTable)
        
        # åŠ¨æ€è®¡ç®—æ€»é¡µæ•°
        total_pages = max(1, (len(self.crawler_history) + self.items_per_page - 1) // self.items_per_page)
        
        # æ–¹å‘é”®ç¿»é¡µåŠŸèƒ½
        if event.key == "down":
            # ä¸‹é”®ï¼šå¦‚æœåˆ°è¾¾å½“å‰é¡µåº•éƒ¨ä¸”æœ‰ä¸‹ä¸€é¡µï¼Œåˆ™ç¿»åˆ°ä¸‹ä¸€é¡µ
            if (table.cursor_row == len(table.rows) - 1 and 
                self.current_page < total_pages):
                self._go_to_next_page()
                # å°†å…‰æ ‡ç§»åŠ¨åˆ°æ–°é¡µé¢çš„ç¬¬ä¸€è¡Œ
                table.move_cursor(row=0, column=0)  # ç›´æ¥ç§»åŠ¨åˆ°ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—
                event.prevent_default()
                event.stop()
                return
        elif event.key == "up":
            # ä¸Šé”®ï¼šå¦‚æœåˆ°è¾¾å½“å‰é¡µé¡¶éƒ¨ä¸”æœ‰ä¸Šä¸€é¡µï¼Œåˆ™ç¿»åˆ°ä¸Šä¸€é¡µ
            if table.cursor_row == 0 and self.current_page > 1:
                self._go_to_prev_page()
                # å°†å…‰æ ‡ç§»åŠ¨åˆ°æ–°é¡µé¢çš„æœ€åä¸€è¡Œ
                last_row_index = len(table.rows) - 1
                table.move_cursor(row=last_row_index, column=0)  # ç›´æ¥ç§»åŠ¨åˆ°æœ€åä¸€è¡Œç¬¬ä¸€åˆ—
                event.prevent_default()
                event.stop()
                return
        
        if event.key == "escape":
            # ESCé”®è¿”å› - çˆ¬å–ç»§ç»­åœ¨åå°è¿è¡Œ
            self.app.pop_screen()
            event.stop()
        
        # æ•°å­—é”®åŠŸèƒ½ - æ ¹æ®æ˜¯å¦æœ‰é€‰ä¸­é¡¹æ‰§è¡Œä¸åŒæ“ä½œ
        if event.key in ["1", "2", "3", "4", "5", "6", "7", "8", "9", "0"]:
            # 0é”®æ˜ å°„åˆ°ç¬¬10ä½
            target_position = 9 if event.key == "0" else int(event.key) - 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­é¡¹
            if self.selected_history:
                # æœ‰é€‰ä¸­é¡¹æ—¶ï¼Œå°†å½“å‰å…‰æ ‡æ‰€åœ¨è¡Œæ’åºåˆ°æŒ‡å®šä½ç½®
                self._move_to_position(target_position)
            else:
                # æ²¡æœ‰é€‰ä¸­é¡¹æ—¶ï¼Œå°†å…‰æ ‡ç§»åŠ¨åˆ°å½“å‰é¡µå¯¹åº”è¡Œ
                self._move_cursor_to_position(target_position)
            event.stop()
    
    def _view_reason(self, history_item: Dict[str, Any]) -> None:
        """æŸ¥çœ‹å¤±è´¥åŸå› """
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤±è´¥çŠ¶æ€
            if history_item.get("status") != get_global_i18n().t('crawler.status_failed'):
                self._update_status(get_global_i18n().t('crawler.no_reason_to_view'), "warning")
                return
                
            # è·å–é”™è¯¯ä¿¡æ¯
            error_message = history_item.get('error_message', '')
            
            if not error_message:
                self._update_status(get_global_i18n().t('crawler.no_error_message'), "information")
                return
                
            # åœ¨çŠ¶æ€ä¿¡æ¯åŒºåŸŸæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            self._update_status(f"{get_global_i18n().t('crawler.failure_reason')}: {error_message}", "error")
                
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.view_reason_failed')}: {str(e)}", "error")

    def _read_book(self, history_item: Dict[str, Any]) -> None:
        """é˜…è¯»ä¹¦ç±"""
        try:
            file_path = history_item.get('file_path')
            
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸ºç©ºæˆ–ä¸º"already_exists"ï¼Œå°è¯•ä»æ•°æ®åº“ä¸­é‡æ–°è·å–
            if not file_path or file_path == 'already_exists':
                site_id = self.novel_site.get('id')
                novel_id = history_item.get('novel_id')
                if site_id and novel_id:
                    crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
                    if crawl_history:
                        # è·å–æœ€æ–°çš„æˆåŠŸè®°å½•
                        for record in crawl_history:
                            if record.get('status') == 'success' and record.get('file_path') and record.get('file_path') != 'already_exists':
                                file_path = record.get('file_path')
                                # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                                history_item['file_path'] = file_path
                                break
            
            if not file_path:
                self._update_status(get_global_i18n().t('crawler.no_file_path'))
                return
                
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä»ç„¶æ˜¯"already_exists"ï¼Œå°è¯•æŸ¥æ‰¾å®é™…æ–‡ä»¶
            if file_path == 'already_exists':
                # å°è¯•æ ¹æ®å°è¯´æ ‡é¢˜æŸ¥æ‰¾æ–‡ä»¶
                novel_title = history_item.get('novel_title', '')
                storage_folder = self.novel_site.get('storage_folder', 'novels')
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
                storage_folder = os.path.expanduser(storage_folder)
                possible_files = glob.glob(os.path.join(storage_folder, f"*{novel_title}*"))
                if possible_files:
                    file_path = possible_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                    # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                    history_item['file_path'] = file_path
                else:
                    self._update_status(f"{get_global_i18n().t('crawler.file_not_exists')}get_global_i18n().t('crawler.not_found')")
                    return
            
            if not os.path.exists(file_path):
                # å°è¯•æ›´æ–°æ•°æ®åº“è®°å½•çŠ¶æ€ä¸ºå¤±è´¥
                site_id = self.novel_site.get('id')
                novel_id = history_item.get('novel_id')
                if site_id and novel_id:
                    # æŸ¥æ‰¾å¹¶æ›´æ–°è®°å½•
                    crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
                    for record in crawl_history:
                        if record.get('file_path') == file_path:
                            # æ›´æ–°çŠ¶æ€ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
                            self.db_manager.update_crawl_history_status(
                                site_id=site_id,
                                novel_id=novel_id,
                                status='failed',
                                file_path=file_path,
                                novel_title=history_item.get('novel_title', ''),
                                error_message='æ–‡ä»¶ä¸å­˜åœ¨'
                            )
                            # é‡æ–°åŠ è½½å†å²è®°å½•
                            self._load_crawl_history()
                            break
                
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return
            
            # ä»æ–‡ä»¶è·¯å¾„åˆ›å»ºä¹¦ç±å¯¹è±¡
            from src.core.book import Book
            book_title = history_item.get('novel_title', get_global_i18n().t('crawler.unknown_book'))
            book_source = self.novel_site.get('name', get_global_i18n().t('crawler.unknown_source'))
            book = Book(file_path, book_title, book_source)
            
            # æ£€æŸ¥ä¹¦ç±æ˜¯å¦æœ‰æ•ˆ
            if not book.path or not os.path.exists(book.path):
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return
            
            # ä½¿ç”¨ app çš„ open_book æ–¹æ³•æ‰“å¼€ä¹¦ç±ï¼ˆè¿è¡Œæ—¶å®‰å…¨æ£€æŸ¥ï¼Œé¿å…ç±»å‹æ£€æŸ¥å‘Šè­¦ï¼‰
            open_book = getattr(self.app, "open_book", None)
            if callable(open_book):
                open_book(file_path)  # type: ignore[misc]
                self._update_status(f"{get_global_i18n().t('crawler.on_reading')}: {book_title}", "success")
            else:
                self._update_status(get_global_i18n().t('crawler.cannot_open_book'), "error")
                
        except Exception as e:
            self._update_status(f"{get_global_i18n().t('crawler.open_failed')}: {str(e)}", "error")
    
    def _read_book_in_browser(self, history_item: Dict[str, Any]) -> None:
        """ä½¿ç”¨æµè§ˆå™¨é˜…è¯»ä¹¦ç±"""
        try:
            from src.utils.browser_reader import BrowserReader
            
            file_path = history_item.get('file_path')
            
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸ºç©ºæˆ–ä¸º"already_exists"ï¼Œå°è¯•ä»æ•°æ®åº“ä¸­é‡æ–°è·å–
            if not file_path or file_path == 'already_exists':
                site_id = self.novel_site.get('id')
                novel_id = history_item.get('novel_id')
                if site_id and novel_id:
                    crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
                    if crawl_history:
                        # è·å–æœ€æ–°çš„æˆåŠŸè®°å½•
                        for record in crawl_history:
                            if record.get('status') == 'success' and record.get('file_path') and record.get('file_path') != 'already_exists':
                                file_path = record.get('file_path')
                                # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                                history_item['file_path'] = file_path
                                break
            
            if not file_path:
                self._update_status(get_global_i18n().t('crawler.no_file_path'))
                return
                
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä»ç„¶æ˜¯"already_exists"ï¼Œå°è¯•æŸ¥æ‰¾å®é™…æ–‡ä»¶
            if file_path == 'already_exists':
                # å°è¯•æ ¹æ®å°è¯´æ ‡é¢˜æŸ¥æ‰¾æ–‡ä»¶
                novel_title = history_item.get('novel_title', '')
                storage_folder = self.novel_site.get('storage_folder', 'novels')
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
                storage_folder = os.path.expanduser(storage_folder)
                possible_files = glob.glob(os.path.join(storage_folder, f"*{novel_title}*"))
                if possible_files:
                    file_path = possible_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
                    # æ›´æ–°å†…å­˜ä¸­çš„è®°å½•
                    history_item['file_path'] = file_path
                else:
                    self._update_status(f"{get_global_i18n().t('crawler.file_not_exists')}get_global_i18n().t('crawler.not_found')")
                    return
            
            if not os.path.exists(file_path):
                self._update_status(get_global_i18n().t('crawler.file_not_exists'))
                return

            # æ•è·æ–‡ä»¶è·¯å¾„ï¼Œé¿å…é—­åŒ…é—®é¢˜
            captured_file_path = file_path

            # ä¿å­˜è¿›åº¦å›è°ƒ
            logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æ³¨å†Œè¿›åº¦ä¿å­˜å›è°ƒå‡½æ•°: file_path={captured_file_path}")
            def on_progress_save(progress: float, scroll_top: int, scroll_height: int,
                                 current_page: Optional[int] = None, total_pages: Optional[int] = None,
                                 word_count: Optional[int] = None) -> None:
                """ä¿å­˜é˜…è¯»è¿›åº¦"""
                logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æ”¶åˆ°ä¿å­˜è¿›åº¦å›è°ƒ: progress={progress:.4f} (å°æ•°), scrollTop={scroll_top}px, scrollHeight={scroll_height}px")
                if current_page is not None:
                    logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] current_page={current_page}, total_pages={total_pages}")
                if word_count is not None:
                    logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] word_count={word_count}")

                try:
                    # ä¿å­˜é˜…è¯»è¿›åº¦åˆ°æ•°æ®åº“
                    from src.core.bookmark import BookmarkManager
                    bookmark_manager = BookmarkManager()

                    # å¦‚æœå‰ç«¯æ²¡æœ‰ä¼ é€’é¡µæ•°ä¿¡æ¯ï¼Œæ ¹æ®è¿›åº¦ä¼°ç®—
                    if total_pages is None or total_pages <= 0:
                        total_pages = int(scroll_height / 1000)  # å‡è®¾æ¯é¡µ1000px
                    if current_page is None:
                        current_page = int(progress * total_pages)

                    logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] å‡†å¤‡ä¿å­˜åˆ°æ•°æ®åº“: book_path={captured_file_path}, current_page={current_page}, total_pages={total_pages}")

                    # ä¿å­˜é˜…è¯»ä¿¡æ¯
                    success = bookmark_manager.save_reading_info(
                        captured_file_path,
                        current_page=current_page,
                        total_pages=total_pages,
                        reading_progress=progress,
                        scroll_top=scroll_top,
                        scroll_height=scroll_height,
                        word_count=word_count if word_count is not None else None
                    )

                    if success:
                        logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] ä¿å­˜æµè§ˆå™¨é˜…è¯»è¿›åº¦æˆåŠŸ: {progress:.4f} ({progress*100:.2f}%), ä½ç½®: {scroll_top}px")
                    else:
                        logger.error(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] ä¿å­˜æµè§ˆå™¨é˜…è¯»è¿›åº¦å¤±è´¥: save_reading_info è¿”å› False")
                except Exception as e:
                    logger.error(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] ä¿å­˜é˜…è¯»è¿›åº¦å¼‚å¸¸: {e}", exc_info=True)

            # åŠ è½½è¿›åº¦å›è°ƒ
            logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æ³¨å†Œè¿›åº¦åŠ è½½å›è°ƒå‡½æ•°: file_path={captured_file_path}")
            def on_progress_load() -> Optional[Dict[str, Any]]:
                """åŠ è½½é˜…è¯»è¿›åº¦"""
                try:
                    from src.core.bookmark import BookmarkManager
                    bookmark_manager = BookmarkManager()

                    reading_info = bookmark_manager.get_reading_info(captured_file_path)
                    logger.debug(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] ä»æ•°æ®åº“è·å–åˆ°é˜…è¯»ä¿¡æ¯: {reading_info}")

                    if reading_info:
                        progress = reading_info.get('progress', 0)
                        scroll_top = reading_info.get('scrollTop', 0)
                        scroll_height = reading_info.get('scrollHeight', 0)

                        # åªè¦æœ‰ progress æ•°æ®å°±è¿”å›ï¼Œå³ä½¿ scroll_top ä¸º 0 ä¹Ÿè¿”å›
                        if progress > 0:
                            logger.debug(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] è¿”å›é˜…è¯»è¿›åº¦: {progress:.2f}%, ä½ç½®: {scroll_top}px, é«˜åº¦: {scroll_height}px")
                            return {
                                'progress': progress,
                                'scrollTop': scroll_top,
                                'scrollHeight': scroll_height if scroll_height > 0 else 10000
                            }
                        else:
                            logger.debug("[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] é˜…è¯»è¿›åº¦ä¸º 0ï¼Œä¸è¿”å›è¿›åº¦æ•°æ®")
                    else:
                        logger.debug("[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æ•°æ®åº“ä¸­æ²¡æœ‰é˜…è¯»è¿›åº¦æ•°æ®")
                except Exception as e:
                    logger.error(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] åŠ è½½é˜…è¯»è¿›åº¦å¤±è´¥: {e}")

                return None

            # ä½¿ç”¨è‡ªå®šä¹‰æµè§ˆå™¨é˜…è¯»å™¨æ‰“å¼€ï¼Œæ”¯æŒè¿›åº¦åŒæ­¥
            logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æ­£åœ¨æ‰“å¼€æµè§ˆå™¨é˜…è¯»å™¨: file_path={file_path}")
            success, message = BrowserReader.open_book_in_browser(
                file_path,
                on_progress_save=on_progress_save,
                on_progress_load=on_progress_load
            )
            logger.info(f"[çˆ¬å–ç®¡ç†-æµè§ˆå™¨é˜…è¯»] æµè§ˆå™¨é˜…è¯»å™¨æ‰“å¼€ç»“æœ: success={success}, message={message}")

            if success:
                book_title = history_item.get('novel_title', get_global_i18n().t('crawler.unknown_book'))
                self._update_status(f"{message}: {book_title}", "success")
            else:
                self._update_status(message, "error")
                
        except Exception as e:
            self._update_status(f"æµè§ˆå™¨æ‰“å¼€å¤±è´¥: {str(e)}", "error")
    
    def _retry_crawl(self, history_item: Dict[str, Any]) -> None:
        """é‡è¯•çˆ¬å–å¤±è´¥çš„è®°å½•"""
        try:
            # æ£€æŸ¥æƒé™ï¼šæ‰§è¡Œçˆ¬å–ä»»åŠ¡éœ€ crawler.run
            if not getattr(self.app, "has_permission", lambda k: True)("crawler.run"):
                self._update_status(get_global_i18n().t('crawler.np_crawler'), "error")
                return
                
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨çˆ¬å–
            if self.is_crawling:
                self._update_status(get_global_i18n().t('crawler.crawling_in_progress'), "error")
                return
                
            # è·å–å°è¯´ID
            novel_id = history_item.get('novel_id')
            if not novel_id:
                self._update_status(get_global_i18n().t('crawler.invalid_novel_id'), "error")
                return
                
            # æ£€æŸ¥è®°å½•æ˜¯å¦ä¸ºå¤±è´¥çŠ¶æ€
            if history_item.get('status') != get_global_i18n().t('crawler.status_failed'):
                self._update_status(get_global_i18n().t('crawler.only_retry_failed'), "error")
                return
            
            # æ³¨æ„ï¼šæ‰‹åŠ¨é‡è¯•æ—¶ä¸æ£€æŸ¥å†å²å¤±è´¥æ¬¡æ•°ï¼Œå…è®¸ç”¨æˆ·ä¸»åŠ¨é‡è¯•
            # åªæœ‰åœ¨è‡ªåŠ¨çˆ¬å–æ—¶æ‰ä¼šæ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°é™åˆ¶
                
            # æ£€æŸ¥ä»£ç†è¦æ±‚
            proxy_check_result = self._check_proxy_requirements_sync()
            if not proxy_check_result['can_proceed']:
                self._update_status(proxy_check_result['message'], "error")
                return
                
            proxy_config = proxy_check_result['proxy_config']
            
            # è®¾ç½®çˆ¬å–çŠ¶æ€
            self.is_crawling = True
            self.current_crawling_id = novel_id
            
            # æ›´æ–°æŒ‰é’®çŠ¶æ€å’Œæ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            self._update_crawl_button_state()
            self._show_loading_animation()
            
            # æ˜¾ç¤ºé‡è¯•çŠ¶æ€
            self._update_status(f"{get_global_i18n().t('crawler.retrying')} ID: {novel_id}")
            
            # å¼‚æ­¥æ‰§è¡Œé‡è¯•çˆ¬å–
            self.app.run_worker(self._retry_crawl_single(novel_id, proxy_config, history_item), name="crawl-retry-worker")
            
        except Exception as e:
            logger.error(f"é‡è¯•çˆ¬å–å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.retry_failed')}: {str(e)}", "error")
            # é‡ç½®çˆ¬å–çŠ¶æ€
            self._reset_crawl_state()
    
    async def _retry_crawl_single(self, novel_id: str, proxy_config: Dict[str, Any], history_item: Dict[str, Any]) -> None:
        """å¼‚æ­¥é‡è¯•å•ä¸ªå°è¯´çš„çˆ¬å–"""
        import asyncio
        import time
        
        try:
            # è·å–è§£æå™¨åç§°
            parser_name = self.novel_site.get('parser')
            if not parser_name:
                self.app.call_later(self._update_status, get_global_i18n().t('crawler.no_parser'), "error")
                return
            
            # å¯¼å…¥è§£æå™¨
            from src.spiders import create_parser
            
            # åˆ›å»ºè§£æå™¨å®ä¾‹ï¼Œä¼ é€’æ•°æ®åº“ä¸­çš„ç½‘ç«™åç§°ä½œä¸ºä½œè€…ä¿¡æ¯å’Œç½‘ç«™URL
            parser_instance = create_parser(parser_name, proxy_config, self.novel_site.get('name'), self.novel_site.get('url'))
            
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡Œç½‘ç»œè¯·æ±‚
            await asyncio.sleep(0.5)  # æ·»åŠ å°å»¶è¿Ÿé¿å…åŒæ—¶è¯·æ±‚è¿‡å¤š
            
            # è§£æå°è¯´è¯¦æƒ…
            novel_content = await self._async_parse_novel_detail(parser_instance, novel_id)
            
            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if not novel_content.get('success', False):
                error_msg = novel_content.get('error_message', get_global_i18n().t('crawler.parse_failed'))
                logger.warning(f"é‡è¯•è§£æå¤±è´¥: {error_msg}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™ç­‰å¾…åé‡è¯•
                if any(keyword in error_msg.lower() for keyword in ['timeout', 'connection', 'network', 'ssl', 'verify', 'nonetype', 'string or bytes-like']):
                    logger.info(f"æ£€æµ‹åˆ°ä¸´æ—¶é”™è¯¯ï¼Œ5ç§’åè‡ªåŠ¨é‡è¯•...")
                    await asyncio.sleep(5)
                    
                    # é‡è¯•ä¸€æ¬¡
                    try:
                        logger.info(f"å¼€å§‹ç¬¬äºŒæ¬¡é‡è¯•: novel_id={novel_id}")
                        novel_content = await self._async_parse_novel_detail(parser_instance, novel_id)
                        if novel_content.get('success', False):
                            logger.info("ç¬¬äºŒæ¬¡é‡è¯•æˆåŠŸï¼")
                        else:
                            logger.error(f"ç¬¬äºŒæ¬¡é‡è¯•ä»ç„¶å¤±è´¥: {novel_content.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                    except Exception as retry_error:
                        logger.error(f"ç¬¬äºŒæ¬¡é‡è¯•å¼‚å¸¸: {retry_error}")
                        novel_content = {'success': False, 'error_message': f'é‡è¯•å¼‚å¸¸: {str(retry_error)}'}
                
                # å¦‚æœæœ€ç»ˆè¿˜æ˜¯å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
                if not novel_content.get('success', False):
                    site_id = self.novel_site.get('id')
                    if site_id:
                        self.db_manager.update_crawl_history_status(
                            site_id=site_id,
                            novel_id=novel_id,
                            status='failed',
                            novel_title=history_item.get('novel_title', ''),
                            error_message=novel_content.get('error_message', get_global_i18n().t('crawler.parse_failed'))
                        )
                    
                    # æ›´æ–°å†…å­˜ä¸­çš„å†å²è®°å½•
                    for i, item in enumerate(self.crawler_history):
                        if item.get('novel_id') == novel_id and item.get('status') == get_global_i18n().t('crawler.status_failed'):
                            self.crawler_history[i] = {
                                "novel_id": novel_id,
                                "novel_title": history_item.get('novel_title', ''),
                                "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                                "status": get_global_i18n().t('crawler.status_failed'),
                                "error_message": novel_content.get('error_message', 'è§£æå¤±è´¥')
                            }
                            break
                    
                    # æ›´æ–°çŠ¶æ€
                    self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.retry_failed')}: {novel_content.get('error_message', 'è§£æå¤±è´¥')}", "error")
                    return
            
            # è§£ææˆåŠŸï¼Œè·å–æ ‡é¢˜
            novel_title = novel_content.get('title', history_item.get('novel_title', ''))
            file_path = novel_content.get('file_path', '')
            
            logger.info(f"=== é‡è¯•æˆåŠŸ === novel_id={novel_id}, novel_title={novel_title}, file_path={file_path}")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„è®°å½•
            site_id = self.novel_site.get('id')
            logger.info(f"å‡†å¤‡æ›´æ–°æ•°æ®åº“çŠ¶æ€: site_id={site_id}, novel_id={novel_id}, novel_title={novel_title}")
            if site_id:
                logger.info(f"è°ƒç”¨ update_crawl_history_status æ›´æ–°çŠ¶æ€ä¸º success")
                success = self.db_manager.update_crawl_history_status(
                    site_id=site_id,
                    novel_id=novel_id,
                    status='success',
                    novel_title=novel_title,
                    file_path=file_path,
                    error_message=''
                )
                logger.info(f"update_crawl_history_status è¿”å›ç»“æœ: {success}")
            else:
                logger.error(f"æ— æ³•è·å– site_idï¼Œnovel_site={self.novel_site}")
            
            # æ›´æ–°å†…å­˜ä¸­çš„å†å²è®°å½•
            for i, item in enumerate(self.crawler_history):
                if item.get('novel_id') == novel_id and item.get('status') == get_global_i18n().t('crawler.status_failed'):
                    self.crawler_history[i] = {
                        "novel_id": novel_id,
                        "novel_title": novel_title,
                        "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "status": get_global_i18n().t('crawler.status_success'),
                        "file_path": file_path
                    }
                    break
            
            # è‡ªåŠ¨å°†ä¹¦ç±åŠ å…¥ä¹¦æ¶
            try:
                # å°†æ–°ä¹¦åŠ å…¥ä¹¦æ¶ï¼ˆä¼˜å…ˆä½¿ç”¨å†…å­˜ä¹¦æ¶ä»¥ä¾¿ç«‹åˆ»å¯è¯»ï¼Œå¤±è´¥æ—¶é€€å›ç›´æ¥å†™DBï¼‰
                try:
                    bs = getattr(self.app, "bookshelf", None)
                    book = None
                    if bs and hasattr(bs, "add_book"):
                        # ä½¿ç”¨"æœªçŸ¥ä½œè€…"è€Œä¸æ˜¯ç¡¬ç¼–ç çš„ç½‘ç«™åç§°ä½œä¸ºauthor
                        author = "æœªçŸ¥ä½œè€…"
                        # è·å–ç½‘ç«™æ ‡ç­¾
                        site_tags = self.novel_site.get('tags', '')
                        book = bs.add_book(file_path, author=author, tags=site_tags)
                    if not book:
                        from src.core.book import Book
                        # ä½¿ç”¨"æœªçŸ¥ä½œè€…"è€Œä¸æ˜¯ç¡¬ç¼–ç çš„ç½‘ç«™åç§°ä½œä¸ºauthor
                        author = "æœªçŸ¥ä½œè€…"
                        # è·å–ç½‘ç«™æ ‡ç­¾
                        site_tags = self.novel_site.get('tags', '')
                        book = Book(file_path, novel_title, author, tags=site_tags)
                        self.db_manager.add_book(book)
                        
                    # å‘é€å…¨å±€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯
                    try:
                        from src.ui.messages import RefreshBookshelfMessage
                        self.app.post_message(RefreshBookshelfMessage())
                        logger.info(f"å·²å‘é€ä¹¦æ¶åˆ·æ–°æ¶ˆæ¯ï¼Œä¹¦ç±å·²æ·»åŠ åˆ°ä¹¦æ¶: {novel_title}")
                    except Exception as msg_error:
                        logger.debug(f"å‘é€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯å¤±è´¥: {msg_error}")
                        
                except Exception as add_err:
                    logger.error(f"æ·»åŠ ä¹¦ç±åˆ°ä¹¦æ¶å¤±è´¥: {add_err}")
                    logger.warning(f"æ·»åŠ ä¹¦ç±åˆ°ä¹¦æ¶å¤±è´¥: {novel_title}")
                    
            except Exception as e:
                logger.error(f"æ·»åŠ ä¹¦ç±åˆ°ä¹¦æ¶å¤±è´¥: {e}")
            
            # é‡æ–°åŠ è½½æ•°æ®åº“ä¸­çš„å†å²è®°å½•
            self.app.call_later(self._load_crawl_history)
            
            # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
            self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.retry_success')}: {novel_title}", "success")
            
            # å‘é€å…¨å±€çˆ¬å–å®Œæˆé€šçŸ¥
            try:
                from src.ui.messages import CrawlCompleteNotification
                self.app.post_message(CrawlCompleteNotification(
                    success=True,
                    novel_title=novel_title,
                    message=f"{get_global_i18n().t('crawler.retry_success')}: {novel_title}"
                ))
            except Exception as msg_error:
                logger.debug(f"å‘é€çˆ¬å–å®Œæˆé€šçŸ¥å¤±è´¥: {msg_error}")
            
            # é‡ç½®çˆ¬å–çŠ¶æ€
            self.app.call_later(self._reset_crawl_state)
        except Exception as e:
            logger.error(f"é‡è¯•çˆ¬å–è¿‡ç¨‹å¤±è´¥: {e}")
            self.app.call_later(self._update_status, f"{get_global_i18n().t('crawler.retry_failed')}: {str(e)}", "error")
    
    def _check_and_continue_crawl(self) -> None:
        """æ£€æŸ¥è¾“å…¥æ¡†ä¸­æ˜¯å¦è¿˜æœ‰æ–°IDï¼Œå¦‚æœæœ‰åˆ™ç»§ç»­çˆ¬å–"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨çˆ¬å–
            if self.is_crawling:
                return
            
            # è·å–è¾“å…¥æ¡†å†…å®¹
            novel_id_input = self.query_one("#novel-id-input", Input)
            novel_ids_input = novel_id_input.value.strip()
            
            if not novel_ids_input:
                # è¾“å…¥æ¡†ä¸ºç©ºï¼Œåœæ­¢çˆ¬å–
                self._update_status(get_global_i18n().t('crawler.crawl_finished'))
                return
            
            # åˆ†å‰²å¤šä¸ªå°è¯´ID
            novel_ids = [id.strip() for id in novel_ids_input.split(',') if id.strip()]
            
            if not novel_ids:
                # æ²¡æœ‰æœ‰æ•ˆçš„IDï¼Œåœæ­¢çˆ¬å–
                self._update_status(get_global_i18n().t('crawler.crawl_finished'))
                return
            
            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„IDå’Œè¿ç»­å¤±è´¥3æ¬¡ä»¥ä¸Šçš„ID
            site_id = self.novel_site.get('id')
            if site_id:
                valid_novel_ids = []
                skipped_novel_ids = []
                for novel_id in novel_ids:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if self.db_manager.check_novel_exists(site_id, novel_id):
                        continue
                    
                    # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                    consecutive_failures = self.db_manager.get_consecutive_failure_count(site_id, novel_id)
                    if consecutive_failures >= 3:
                        skipped_novel_ids.append(novel_id)
                        logger.info(f"è‡ªåŠ¨è·³è¿‡å°è¯´ {novel_id}ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°å·²è¾¾ {consecutive_failures} æ¬¡")
                        continue
                    
                    valid_novel_ids.append(novel_id)
                
                # æ›´æ–°è¾“å…¥æ¡†å†…å®¹ï¼Œåªä¿ç•™æœ‰æ•ˆçš„ID
                novel_id_input.value = ', '.join(valid_novel_ids)
                novel_ids = valid_novel_ids
                
                # å¦‚æœæœ‰è¢«è·³è¿‡çš„IDï¼Œæ˜¾ç¤ºä¿¡æ¯
                if skipped_novel_ids:
                    self._update_status(f"å·²è·³è¿‡ {len(skipped_novel_ids)} ä¸ªè¿ç»­å¤±è´¥3æ¬¡ä»¥ä¸Šçš„å°è¯´", "warning")
                
                if not valid_novel_ids:
                    # æ²¡æœ‰æœ‰æ•ˆçš„IDï¼Œåœæ­¢çˆ¬å–
                    if skipped_novel_ids:
                        self._update_status(get_global_i18n().t('crawler.no_valid_novels'), "warning")
                    else:
                        self._update_status(get_global_i18n().t('crawler.all_novels_exist'))
                    novel_id_input.value = ""  # æ¸…ç©ºè¾“å…¥æ¡†
                    return
            
            # æ£€æŸ¥ä»£ç†è¦æ±‚
            proxy_check_result = self._check_proxy_requirements_sync()
            if not proxy_check_result['can_proceed']:
                self._update_status(proxy_check_result['message'], "error")
                return
            
            proxy_config = proxy_check_result['proxy_config']
            
            # è®¾ç½®çˆ¬å–çŠ¶æ€
            self.is_crawling = True
            self._update_crawl_button_state()
            
            # ä½¿ç”¨åå°çˆ¬å–ç®¡ç†å™¨å¯åŠ¨ä»»åŠ¡
            site_id = self.novel_site.get('id')
            if not site_id:
                self._update_status(get_global_i18n().t('crawler.no_site_id'), "error")
                return
            
            # å¯åŠ¨åå°çˆ¬å–ä»»åŠ¡
            task_id = self.crawler_manager.start_crawl_task(site_id, novel_ids, proxy_config)
            self.current_task_id = task_id
            
            # æ˜¾ç¤ºå¯åŠ¨çŠ¶æ€
            self._update_status(f"{get_global_i18n().t('crawler.continuing_crawl')} ({len(novel_ids)} {get_global_i18n().t('crawler.books')})")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨ç»§ç»­çˆ¬å–æ£€æŸ¥å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.continue_crawl_failed')}: {str(e)}", "error")
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """
        æŒ‰é’®æŒ‰ä¸‹æ—¶çš„å›è°ƒ
        
        Args:
            event: æŒ‰é’®æŒ‰ä¸‹äº‹ä»¶
        """
        button_id = event.button.id
        
        if button_id == "open-browser-btn":
            self._open_browser()
        elif button_id == "view-history-btn":
            self._view_history()
        elif button_id == "note-btn":
            self._open_note_dialog()
        elif button_id == "view-logs-btn":
            self._open_log_viewer()
        elif button_id == "clear-invalid-btn":
            self._clear_invalid_records()
        elif button_id == "delete-file-btn":
            self._batch_delete_files()
        elif button_id == "delete-record-btn":
            self._batch_delete_records()
        elif button_id == "back-btn":
            self.app.pop_screen()
        elif button_id == "search-btn":
            self._perform_search()
            # æ‰§è¡Œæœç´¢åï¼Œä¿æŒç„¦ç‚¹åœ¨æœç´¢æ¡†
            self.set_timer(0.1, lambda: self._focus_search_input())
        elif button_id == "clear-search-btn":
            self._clear_search()
            # æ¸…é™¤æœç´¢åï¼Œä¿æŒç„¦ç‚¹åœ¨æœç´¢æ¡†
            self.set_timer(0.1, lambda: self._focus_search_input())
        elif button_id == "start-crawl-btn":
            self._start_crawl()
        elif button_id == "stop-crawl-btn":
            self._stop_crawl()
        elif button_id == "copy-ids-btn":
            self._copy_novel_ids()
        elif button_id == "choose-books-btn":
            self._open_select_books_dialog()
        elif button_id == "first-page-btn":
            self._go_to_first_page()
        elif button_id == "prev-page-btn":
            self._go_to_prev_page()
        elif button_id == "next-page-btn":
            self._go_to_next_page()
        elif button_id == "last-page-btn":
            self._go_to_last_page()
        elif button_id == "jump-page-btn":
            self._show_jump_dialog()
        elif button_id == "select-all-btn":
            self._select_all_rows()
        elif button_id == "invert-selection-btn":
            self._invert_selection()
        elif button_id == "deselect-all-btn":
            self._deselect_all_rows()
        elif button_id == "move-up-btn":
            self._move_selected_up()
        elif button_id == "move-down-btn":
            self._move_selected_down()
        elif button_id == "merge-btn":
            self._merge_selected()
        elif button_id == "toggle-monitor-btn":
            self._toggle_browser_monitor()
    
    def _batch_delete_files(self) -> None:
        """æ‰¹é‡åˆ é™¤é€‰ä¸­çš„æ–‡ä»¶"""
        try:
            # è·å–é€‰ä¸­çš„è®°å½•ID
            selected_ids = self.selected_history
            if not selected_ids:
                self._update_status(get_global_i18n().t('batch_ops.no_selected_rows'), "warning")
                return
            
            # ä»crawler_historyä¸­è·å–å¯¹åº”çš„å®Œæ•´è¡Œæ•°æ®
            selected_rows = []
            for item in self.crawler_history:
                if str(item.get("id")) in selected_ids:
                    selected_rows.append(item)
            
            if not selected_rows:
                self._update_status(get_global_i18n().t('batch_ops.no_data'), "warning")
                return
            
            # æ£€æŸ¥æƒé™
            if not getattr(self.app, "has_permission", lambda k: True)("crawler.delete_file"):
                self._update_status(get_global_i18n().t('crawler.np_delete_file'), "error")
                return
            
            # ç¡®è®¤åˆ é™¤
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_delete_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        deleted_count = 0
                        failed_count = 0
                        
                        for row_data in selected_rows:
                            file_path = row_data.get('file_path')
                            if not file_path:
                                continue
                                
                            try:
                                import os
                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                                if os.path.exists(file_path):
                                    # åˆ é™¤æ–‡ä»¶
                                    send2trash(file_path)
                                    logger.info(f"æ–‡ä»¶å·²ç§»è‡³å›æ”¶ç«™: {file_path}")
                                    
                                    # åŒæ—¶åˆ é™¤ä¹¦æ¶ä¸­çš„å¯¹åº”ä¹¦ç±
                                    try:
                                        if self.db_manager.delete_book(file_path):
                                            # å‘é€å…¨å±€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯
                                            try:
                                                from src.ui.messages import RefreshBookshelfMessage
                                                self.app.post_message(RefreshBookshelfMessage())
                                            except Exception as msg_error:
                                                logger.debug(f"å‘é€åˆ·æ–°ä¹¦æ¶æ¶ˆæ¯å¤±è´¥: {msg_error}")
                                    except Exception as shelf_error:
                                        logger.error(f"åˆ é™¤ä¹¦æ¶ä¹¦ç±å¤±è´¥: {shelf_error}")
                                        
                                    deleted_count += 1
                                else:
                                    failed_count += 1
                            except Exception as e:
                                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                                failed_count += 1
                        
                        # æ¸…é™¤é€‰ä¸­çŠ¶æ€
                        self.selected_history.clear()
                        
                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()
                        
                        # æ˜¾ç¤ºç»“æœ
                        if failed_count > 0:
                            self._update_status(get_global_i18n().t('crawler.multi_delete_file_success', deletes=deleted_count, fails=failed_count), "warning")
                        else:
                            self._update_status(get_global_i18n().t('crawler.multi_delete_file_success2', deletes=deleted_count), "success")
                        
                        
                    except Exception as e:
                        self._update_status(get_global_i18n().t('crawler.multi_delete_file_failed', err=str(e)), "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))
            
            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    get_global_i18n().t('crawler.confirm_title', counts=len(selected_rows)),
                    get_global_i18n().t('crawler.confirm_desc', counts=len(selected_rows))
                ),
                handle_delete_confirmation
            )
            
        except Exception as e:
            self._update_status(get_global_i18n().t('crawler.multi_delete_file_failed', err=str(e)), "error")
    
    def _batch_delete_records(self) -> None:
        """æ‰¹é‡åˆ é™¤é€‰ä¸­çš„æ•°æ®åº“è®°å½•"""
        try:
            # è·å–é€‰ä¸­çš„è®°å½•ID
            selected_ids = self.selected_history
            if not selected_ids:
                self._update_status(get_global_i18n().t('batch_ops.no_selected_rows'), "warning")
                return
            
            # æ£€æŸ¥æƒé™
            if not getattr(self.app, "has_permission", lambda k: True)("crawler.delete_record"):
                self._update_status(get_global_i18n().t('crawler.np_delete_record'), "error")
                return
            
            # ç¡®è®¤åˆ é™¤
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_delete_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        deleted_count = 0
                        failed_count = 0
                        
                        for record_id in selected_ids:
                            try:
                                # åˆ é™¤æ•°æ®åº“è®°å½•ï¼Œå°†å­—ç¬¦ä¸²IDè½¬æ¢ä¸ºæ•´æ•°
                                if self.db_manager.delete_crawl_history(int(record_id)):
                                    deleted_count += 1
                                else:
                                    failed_count += 1
                            except Exception as e:
                                logger.error(f"åˆ é™¤è®°å½•å¤±è´¥: {record_id}, é”™è¯¯: {e}")
                                failed_count += 1
                        
                        # æ¸…é™¤é€‰ä¸­çŠ¶æ€
                        self.selected_history.clear()
                        
                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()
                        
                        # æ˜¾ç¤ºç»“æœ
                        if failed_count > 0:
                            self._update_status(get_global_i18n().t('crawler.multi_delete_record_success', deletes=deleted_count, fails=failed_count), "warning")
                        else:
                            self._update_status(get_global_i18n().t('crawler.multi_delete_record_success2', deletes=deleted_count), "success")
                        
                          
                    except Exception as e:
                        self._update_status(get_global_i18n().t('crawler.multi_delete_record_failed', err=str(e)), "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))
            
            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    get_global_i18n().t('crawler.confirm_record_title', counts=len(selected_ids)),
                    get_global_i18n().t('crawler.confirm_record_desc', counts=len(selected_ids))
                ),
                handle_delete_confirmation
            )

        except Exception as e:
            self._update_status(get_global_i18n().t('crawler.confirm_record_title', err=str(e)), "error")

    def _clear_invalid_records(self) -> None:
        """æ¸…ç†æ— æ•ˆè®°å½•ï¼ˆæ²¡æœ‰æ–‡ä»¶çš„è®°å½•å’Œçˆ¬å–å¤±è´¥çš„è®°å½•ï¼‰"""
        try:
            # è·å–ç½‘ç«™ID
            site_id = self.novel_site.get('id')
            if not site_id:
                self._update_status(get_global_i18n().t('crawler.no_site_id'), "error")
                return

            # æ£€æŸ¥æƒé™
            if not getattr(self.app, "has_permission", lambda k: True)("crawler.delete_record"):
                self._update_status(get_global_i18n().t('crawler.np_clear_invalid'), "error")
                return

            # æŸ¥è¯¢æ— æ•ˆè®°å½•
            import sqlite3
            invalid_records = []
            try:
                with sqlite3.connect(self.db_manager.db_path) as conn:
                    conn.row_factory = sqlite3.Row
                    cursor = conn.cursor()

                    # 1. æŸ¥è¯¢æ‰€æœ‰å¤±è´¥çš„è®°å½•
                    cursor.execute("""
                        SELECT id, novel_id, novel_title, file_path, status, error_message
                        FROM crawl_history
                        WHERE site_id = ? AND status = 'failed'
                    """, (site_id,))
                    failed_rows = cursor.fetchall()
                    for row in failed_rows:
                        invalid_records.append(dict(row))

                    # 2. æŸ¥è¯¢æ‰€æœ‰æˆåŠŸçš„è®°å½•ï¼Œç„¶åæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    cursor.execute("""
                        SELECT id, novel_id, novel_title, file_path, status, error_message
                        FROM crawl_history
                        WHERE site_id = ? AND status = 'success'
                    """, (site_id,))
                    success_rows = cursor.fetchall()
                    for row in success_rows:
                        record = dict(row)
                        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ºç©ºæˆ–æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        if not record.get('file_path') or not os.path.exists(record['file_path']):
                            invalid_records.append(record)

            except Exception as e:
                logger.error(f"æŸ¥è¯¢æ— æ•ˆè®°å½•å¤±è´¥: {e}")
                self._update_status(get_global_i18n().t('crawler.clear_invalid_failed', error=str(e)), "error")
                return

            # å¦‚æœæ²¡æœ‰æ— æ•ˆè®°å½•
            if not invalid_records:
                self._update_status(get_global_i18n().t('crawler.clear_invalid_no_data'), "information")
                return

            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            from src.ui.dialogs.confirm_dialog import ConfirmDialog
            def handle_clear_confirmation(confirmed: bool | None) -> None:
                if confirmed:
                    try:
                        deleted_count = 0
                        failed_count = 0

                        # åˆ é™¤æ‰€æœ‰æ— æ•ˆè®°å½•
                        for record in invalid_records:
                            try:
                                record_id = record['id']
                                if self.db_manager.delete_crawl_history(record_id):
                                    deleted_count += 1
                                else:
                                    failed_count += 1
                            except Exception as e:
                                logger.error(f"åˆ é™¤è®°å½•å¤±è´¥: {record['id']}, é”™è¯¯: {e}")
                                failed_count += 1

                        # åˆ·æ–°å†å²è®°å½•
                        self._load_crawl_history()

                        # æ˜¾ç¤ºç»“æœ
                        if failed_count > 0:
                            self._update_status(
                                get_global_i18n().t('crawler.clear_invalid_success', success=deleted_count, fail=failed_count),
                                "warning"
                            )
                        else:
                            self._update_status(
                                get_global_i18n().t('crawler.clear_invalid_success', success=deleted_count, fail=failed_count),
                                "success"
                            )
                    except Exception as e:
                        logger.error(f"æ¸…ç†æ— æ•ˆè®°å½•å¤±è´¥: {e}")
                        self._update_status(get_global_i18n().t('crawler.clear_invalid_failed', error=str(e)), "error")
                elif confirmed is False:
                    self._update_status(get_global_i18n().t('crawler.delete_cancelled'))

            # æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†
            self.app.push_screen(
                ConfirmDialog(
                    self.theme_manager,
                    get_global_i18n().t('crawler.clear_invalid_confirm'),
                    get_global_i18n().t('crawler.clear_invalid_desc', count=len(invalid_records))
                ),
                handle_clear_confirmation
            )

        except Exception as e:
            logger.error(f"æ¸…ç†æ— æ•ˆè®°å½•æ“ä½œå¤±è´¥: {e}")
            self._update_status(get_global_i18n().t('crawler.clear_invalid_failed', error=str(e)), "error")

    # Chromeæ ‡ç­¾é¡µç›‘å¬å™¨ç›¸å…³æ–¹æ³•
    def _init_browser_monitor(self) -> None:
        """åˆå§‹åŒ–æµè§ˆå™¨æ ‡ç­¾é¡µç›‘å¬å™¨"""
        try:
            # åˆå§‹åŒ–æµè§ˆå™¨ç›‘å¬å™¨ï¼ˆAppleScriptæ¨¡å¼ï¼‰ - åªç›‘å¬å½“å‰ç½‘ç«™
            # æ ¹æ®é€‰æ‹©ç¡®å®šæµè§ˆå™¨ç±»å‹
            if self.selected_browser == "safari":
                browser_type = BrowserType.SAFARI
            elif self.selected_browser == "brave":
                browser_type = BrowserType.BRAVE
            elif self.selected_browser == "firefox":
                browser_type = BrowserType.FIREFOX
            else:  # chrome
                browser_type = BrowserType.CHROME

            try:
                self.browser_monitor = BrowserTabMonitor(
                    novel_sites=[self.novel_site],  # åªä¼ å…¥å½“å‰ç½‘ç«™ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ç½‘ç«™
                    on_url_detected=self._on_browser_url_detected,
                    headless=False,  # AppleScriptæ¨¡å¼ä¸éœ€è¦headless
                    browser_type=browser_type  # ä¼ é€’æµè§ˆå™¨ç±»å‹
                )
                logger.info(f"åˆå§‹åŒ–ç›‘å¬å™¨: selected_browser={self.selected_browser}, browser_type={browser_type}, BrowserType.SAFARI={BrowserType.SAFARI}")
                logger.info(f"{self.selected_browser}æµè§ˆå™¨æ ‡ç­¾é¡µç›‘å¬å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"{self.selected_browser}ç›‘å¬å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.browser_monitor = None

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æµè§ˆå™¨ç›‘å¬å™¨å¤±è´¥: {e}")

    def _reinit_monitor_with_browser(self) -> None:
        """ä½¿ç”¨æ–°æµè§ˆå™¨ç±»å‹é‡æ–°åˆå§‹åŒ–ç›‘å¬å™¨"""
        try:
            # åœæ­¢å¹¶æ¸…ç†æ—§ç›‘å¬å™¨
            if self.browser_monitor and hasattr(self.browser_monitor, 'stop_monitoring'):
                self.browser_monitor.stop_monitoring()

            # å°†æ—§ç›‘å¬å™¨è®¾ç½®ä¸ºNone
            self.browser_monitor = None

            # é‡æ–°åˆå§‹åŒ–ç›‘å¬å™¨
            self._init_browser_monitor()
            self.browser_monitor_active = False

            logger.info(f"ç›‘å¬å™¨å·²é‡æ–°åˆå§‹åŒ–,ä½¿ç”¨æµè§ˆå™¨ç±»å‹: {self.selected_browser}")

        except Exception as e:
            logger.error(f"é‡æ–°åˆå§‹åŒ–ç›‘å¬å™¨å¤±è´¥: {e}")
            self._update_status(f"åˆå§‹åŒ–æµè§ˆå™¨ç›‘å¬å™¨å¤±è´¥: {str(e)}", "error")

    def _on_browser_url_detected(self, result: Dict[str, Any]) -> None:
        """
        Chromeç›‘å¬å™¨æ£€æµ‹åˆ°URLæ—¶çš„å›è°ƒå‡½æ•°
        
        Args:
            result: åŒ…å«ç½‘ç«™ä¿¡æ¯ã€å°è¯´IDå’ŒURLçš„å­—å…¸
        """
        try:
            site = result.get('site_config')
            novel_id = result['novel_id']
            url = result['url']
            
            logger.info(f"æ£€æµ‹åˆ°å°è¯´URL: {url}, å°è¯´ID: {novel_id}")
            
            # éªŒè¯ä¹¦ç±IDæ˜¯å¦å·²çˆ¬å–è¿‡
            if self._is_novel_already_crawled(site.get('id'), novel_id):
                logger.info(f"å°è¯´ {novel_id} å·²çˆ¬å–è¿‡ï¼Œè·³è¿‡")
                # å…³é—­å¯¹åº”çš„æ ‡ç­¾é¡µ
                if self.browser_monitor:
                    self.browser_monitor.close_tab(url)
                return
            
            # å°†å°è¯´IDæ·»åŠ åˆ°è¾“å…¥æ¡†
            self._add_novel_id_to_input(novel_id)
            
            # å…³é—­å¯¹åº”çš„æ ‡ç­¾é¡µ
            if self.browser_monitor:
                self.browser_monitor.close_tab(url)
            
            # æ›´æ–°çŠ¶æ€
            novel_title = site.get('name', 'æœªçŸ¥ç½‘ç«™')
            self._update_status(f"è‡ªåŠ¨æ£€æµ‹åˆ°å°è¯´: {novel_id} ({novel_title})", "success")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ£€æµ‹åˆ°çš„URLå¤±è´¥: {e}")

    def _get_log_file_path(self) -> str:
        """
        è·å–å½“å‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
        Returns:
            str: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        try:
            # è·å–é…ç½®ç®¡ç†å™¨
            config_manager = ConfigManager.get_instance()
            
            # è·å–æ—¥å¿—ç›®å½•
            log_dir = os.path.join(os.path.expanduser("~"), ".config", "new_preader", "logs")
            
            # æ ¹æ®è°ƒè¯•æ¨¡å¼ç¡®å®šæ—¥å¿—æ–‡ä»¶å
            if config_manager.get_debug_mode():
                # å¼€å‘æ¨¡å¼ä½¿ç”¨å¸¦æ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶
                from datetime import date
                log_file = os.path.join(log_dir, f'application_{date.today()}.log')
            else:
                # ç”Ÿäº§æ¨¡å¼ä½¿ç”¨åº”ç”¨ä¸»æ—¥å¿—æ–‡ä»¶
                log_file = os.path.join(log_dir, 'application.log')
            
            # å¦‚æœä¸»æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
            if not os.path.exists(log_file):
                if os.path.exists(log_dir):
                    # æŸ¥æ‰¾ç›®å½•ä¸‹æœ€æ–°çš„.logæ–‡ä»¶
                    log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]
                    if log_files:
                        log_files.sort(key=lambda x: os.path.getmtime(os.path.join(log_dir, x)), reverse=True)
                        log_file = os.path.join(log_dir, log_files[0])
            
            return log_file
            
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è·¯å¾„
            return os.path.join(os.path.expanduser("~"), ".config", "new_preader", "logs", "application.log")

    def _open_log_viewer(self) -> None:
        """æ‰“å¼€æ—¥å¿—æŸ¥çœ‹å™¨å¼¹çª—"""
        try:
            log_file_path = self._get_log_file_path()
            
            # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(log_file_path):
                self._update_status(get_global_i18n().t('crawler.log_file_not_found'), "warning")
                self.app.notify(f"ğŸ“ {get_global_i18n().t('crawler.log_file_not_found')}")
                return
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ—¥å¿—æŸ¥çœ‹å™¨å¼¹çª—å­˜åœ¨
            existing_screens = self.app.screen_stack
            log_viewer_exists = any(isinstance(screen, LogViewerPopup) for screen in existing_screens)
            
            if log_viewer_exists:
                # å¦‚æœå¼¹çª—å·²å­˜åœ¨ï¼Œä¸é‡å¤æ˜¾ç¤ºæ¶ˆæ¯
                return
            
            # åˆ›å»ºå¹¶æ˜¾ç¤ºæ—¥å¿—æŸ¥çœ‹å™¨å¼¹çª—
            log_viewer = LogViewerPopup(log_file_path)
            self.app.push_screen(log_viewer)
            
            self._update_status(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_opened')}", "information")
            self.app.notify(f"ğŸ“‹ {get_global_i18n().t('crawler.log_viewer_opened')}")
            
        except Exception as e:
            logger.error(f"æ‰“å¼€æ—¥å¿—æŸ¥çœ‹å™¨å¤±è´¥: {e}")
            self._update_status(f"{get_global_i18n().t('crawler.open_log_viewer_failed')}: {e}", "error")
            self.app.notify(f"âŒ {get_global_i18n().t('crawler.open_log_viewer_failed')}: {e}")

    def _is_novel_already_crawled(self, site_id: str, novel_id: str) -> bool:
        """
        æ£€æŸ¥å°è¯´æ˜¯å¦å·²ç»çˆ¬å–è¿‡
        
        Args:
            site_id: ç½‘ç«™ID
            novel_id: å°è¯´ID
            
        Returns:
            æ˜¯å¦å·²çˆ¬å–è¿‡
        """
        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢
            crawl_history = self.db_manager.get_crawl_history_by_novel_id(site_id, novel_id)
            if crawl_history:
                # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„è®°å½•
                for record in crawl_history:
                    if record.get('status') == 'success':
                        return True
            return False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å°è¯´çˆ¬å–çŠ¶æ€å¤±è´¥: {e}")
            return False

    def _add_novel_id_to_input(self, novel_id: str) -> None:
        """
        å°†å°è¯´IDæ·»åŠ åˆ°è¾“å…¥æ¡†ï¼ˆè¿½åŠ å½¢å¼ï¼‰
        
        Args:
            novel_id: å°è¯´ID
        """
        try:
            input_field = self.query_one("#novel-id-input", Input)
            current_value = input_field.value.strip()
            
            # URLè§£ç å°è¯´ID
            from urllib.parse import unquote
            decoded_novel_id = unquote(novel_id)
            
            if current_value:
                # å¦‚æœå·²æœ‰å†…å®¹ï¼Œç”¨é€—å·è¿½åŠ åˆ°æœ«å°¾
                new_value = f"{current_value},{decoded_novel_id}"
            else:
                # å¦‚æœä¸ºç©ºï¼Œç›´æ¥è®¾ç½®
                new_value = decoded_novel_id
            
            input_field.value = new_value
            logger.info(f"å·²å°†å°è¯´ID {decoded_novel_id} æ·»åŠ åˆ°è¾“å…¥æ¡†")
            
        except Exception as e:
            logger.error(f"æ·»åŠ å°è¯´IDåˆ°è¾“å…¥æ¡†å¤±è´¥: {e}")

    def _toggle_browser_monitor(self) -> None:
            """åˆ‡æ¢ç›‘å¬çŠ¶æ€"""
            try:
                if self.browser_monitor_active:
                    self._stop_browser_monitor()
                else:
                    self._start_browser_monitor()
            except Exception as e:
                logger.error(f"åˆ‡æ¢ç›‘å¬çŠ¶æ€å¤±è´¥: {e}")
                self._update_status(f"åˆ‡æ¢ç›‘å¬çŠ¶æ€å¤±è´¥: {str(e)}", "error")

    def _start_browser_monitor(self) -> None:
        """å¼€å§‹ç›‘å¬"""
        try:
            if not self.browser_monitor:
                self._update_status("ç›‘å¬å™¨æœªåˆå§‹åŒ–", "error")
                return
            
            success = self.browser_monitor.start_monitoring()
            if success:
                self.browser_monitor_active = True
                self._update_status(get_global_i18n().t('crawler.monitor_started'), "success")
                self._update_monitor_button_state()
            else:
                self._update_status(get_global_i18n().t('crawler.monitor_start_failed'), "error")
                
        except Exception as e:
            logger.error(f"å¯åŠ¨ç›‘å¬å¤±è´¥: {e}")
            self._update_status(f"å¯åŠ¨ç›‘å¬å¤±è´¥: {str(e)}", "error")

    def _stop_browser_monitor(self) -> None:
        """åœæ­¢ç›‘å¬"""
        try:
            # åœæ­¢Chromeç›‘å¬
            if self.browser_monitor:
                success = self.browser_monitor.stop_monitoring()
                if not success:
                    logger.warning("åœæ­¢ç›‘å¬å¯èƒ½æœªå®Œå…¨æˆåŠŸ")
            
            self.browser_monitor_active = False
            self._update_status(get_global_i18n().t('crawler.monitor_stopped'), "information")
            self._update_monitor_button_state()
            
        except Exception as e:
            logger.error(f"åœæ­¢ç›‘å¬å¤±è´¥: {e}")
            self._update_status(f"åœæ­¢ç›‘å¬å¤±è´¥: {str(e)}", "error")
    def _update_monitor_button_state(self) -> None:
        """æ›´æ–°ç›‘å¬æŒ‰é’®çŠ¶æ€"""
        try:
            toggle_btn = self.query_one("#toggle-monitor-btn", Button)
            if self.browser_monitor_active:
                toggle_btn.label = get_global_i18n().t("crawler.stop_monitor")
                toggle_btn.variant = "error"
            else:
                toggle_btn.label = get_global_i18n().t("crawler.start_monitor")
                toggle_btn.variant = "success"
        except Exception:
            pass

    @on(Select.Changed)
    def on_select_changed(self, event: Select.Changed) -> None:
        """
        ä¸‹æ‹‰é€‰æ‹©æ¡†å€¼å˜åŒ–æ—¶çš„å›è°ƒ

        Args:
            event: ä¸‹æ‹‰é€‰æ‹©æ¡†å˜åŒ–äº‹ä»¶
        """
        if event.select.id == "browser-select" and event.value is not None:
            # æ›´æ–°å½“å‰é€‰æ‹©çš„æµè§ˆå™¨
            self.selected_browser = event.value
            browser_name = {
                "chrome": "Chrome",
                "safari": "Safari",
                "brave": "Brave"
            }.get(self.selected_browser, "Chrome")
            logger.info(f"æµè§ˆå™¨é€‰æ‹©å·²æ›´æ”¹ä¸º: {browser_name}")
            self._update_status(get_global_i18n().t("crawler.browser_changed", browser=browser_name), "information")
            # ä¿å­˜æµè§ˆå™¨é…ç½®
            self._save_browser_config()

            # æ£€æŸ¥å½“å‰ç›‘å¬å™¨çš„æµè§ˆå™¨ç±»å‹æ˜¯å¦ä¸æ–°é€‰æ‹©ä¸€è‡´
            logger.info(f"æµè§ˆå™¨åˆ‡æ¢æ£€æŸ¥: self.selected_browser={self.selected_browser}, self.browser_monitor_active={self.browser_monitor_active}")

            # æ— è®ºç›‘å¬å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ,éƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–ä»¥ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æµè§ˆå™¨ç±»å‹
            if self.browser_monitor:
                logger.info("æ£€æµ‹åˆ°ç›‘å¬å™¨å¯¹è±¡å­˜åœ¨,å‡†å¤‡é‡æ–°åˆå§‹åŒ–...")
                # å¦‚æœç›‘å¬å™¨æ­£åœ¨è¿è¡Œ,å…ˆåœæ­¢
                if self.browser_monitor_active:
                    self._stop_browser_monitor()
                # é‡æ–°åˆå§‹åŒ–ç›‘å¬å™¨ä»¥ä½¿ç”¨æ–°æµè§ˆå™¨ç±»å‹
                self._reinit_monitor_with_browser()
            else:
                logger.info("ç›‘å¬å™¨å¯¹è±¡ä¸å­˜åœ¨,åˆ›å»ºæ–°ç›‘å¬å™¨...")
                # ç›´æ¥åˆå§‹åŒ–ç›‘å¬å™¨
                self._init_browser_monitor()



    def on_unmount(self) -> None:
        """å±å¹•å¸è½½æ—¶çš„å›è°ƒ"""
        # åœæ­¢Chromeç›‘å¬å™¨
        if self.browser_monitor and self.browser_monitor_active:
            try:
                self.browser_monitor.stop_monitoring()
                self.browser_monitor_active = False
                logger.info("é¡µé¢å¸è½½ï¼ŒChromeç›‘å¬å™¨å·²åœæ­¢")
            except Exception as e:
                logger.error(f"åœæ­¢Chromeç›‘å¬å™¨å¤±è´¥: {e}")
        
        # è°ƒç”¨çˆ¶ç±»çš„å¸è½½æ–¹æ³•
        self.is_mounted_flag = False
        # ç¡®ä¿çˆ¬å–çŠ¶æ€è¢«æ­£ç¡®æ¸…ç†
        self.is_crawling = False
        self.current_crawling_id = None
        # æ³¨æ„ï¼šè¿™é‡Œä¸åœæ­¢çˆ¬å–å·¥ä½œçº¿ç¨‹ï¼Œè®©çˆ¬å–ç»§ç»­åœ¨åå°è¿è¡Œ
        # çˆ¬å–å·¥ä½œçº¿ç¨‹ä¼šé€šè¿‡app.call_laterå’Œapp.post_messageæ¥æ›´æ–°UI
        # å³ä½¿é¡µé¢å¸è½½ï¼Œè¿™äº›æ¶ˆæ¯ä¹Ÿä¼šè¢«æ­£ç¡®å¤„ç†
        logger.debug("çˆ¬å–ç®¡ç†é¡µé¢å¸è½½ï¼Œçˆ¬å–å·¥ä½œçº¿ç¨‹ç»§ç»­åœ¨åå°è¿è¡Œ")

