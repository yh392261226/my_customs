"""
canovel.comè§£æå™¨ - åŸºäºé…ç½®é©±åŠ¨ç‰ˆæœ¬
æ”¯æŒå†…å®¹é¡µå†…åˆ†é¡µç±»å‹å°è¯´
"""

import re
import time
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin
from .base_parser_v2 import BaseParser
from src.utils.logger import get_logger

logger = get_logger(__name__)

class CanovelParser(BaseParser):
    """canovel.comè§£æå™¨ - é…ç½®é©±åŠ¨ç‰ˆæœ¬"""
    
    # åŸºæœ¬ä¿¡æ¯
    name = "canovel.com"
    description = "canovel.comå°è¯´çˆ¬å–è§£æå™¨ï¼ˆæ”¯æŒå†…å®¹é¡µåˆ†é¡µç±»å‹ï¼‰"
    base_url = "https://canovel.com"
    
    # ç¼–ç é…ç½® - canovel.comä½¿ç”¨UTF-8ç¼–ç 
    encoding = "utf-8"
    
    # æ­£åˆ™è¡¨è¾¾å¼é…ç½® - æ ‡é¢˜æå–
    title_reg = [
        r'<h1[^>]*class="post-title entry-title"[^>]*>(.*?)</h1>',
        r'<h1[^>]*>(.*?)</h1>'
    ]
    
    # æ­£åˆ™è¡¨è¾¾å¼é…ç½® - å†…å®¹æå–ï¼ˆä½¿ç”¨è´ªå©ªæ¨¡å¼åŒ¹é…åµŒå¥—divï¼‰
    content_reg = [
        r'<div[^>]*class="entry-inner"[^>]*>(.*?)</div>\s*<div[^>]*class="'
    ]
    
    status_reg = [
        r'<span[^>]*class="posted-on"[^>]*>(.*?)</span>',
        r'<time[^>]*class="entry-date"[^>]*>(.*?)</time>'
    ]
    
    # ä¹¦ç±ç±»å‹é…ç½® - æ”¯æŒå†…å®¹é¡µåˆ†é¡µ
    book_type = ["å†…å®¹é¡µå†…åˆ†é¡µ"]
    
    # å†…å®¹é¡µå†…åˆ†é¡µç›¸å…³é…ç½® - ä¸‹ä¸€é¡µé“¾æ¥æå–
    next_page_link_reg = [
        r'<a[^>]*class="nextpostslink"[^>]*href="([^"]*)"[^>]*>'
    ]
    
    # å¤„ç†å‡½æ•°é…ç½®
    after_crawler_func = [
        "_clean_html_content",  # å…ˆæ¸…ç†HTML
        "_extract_balanced_content",  # ä½¿ç”¨å¹³è¡¡ç®—æ³•æå–å†…å®¹
        "_remove_ads",  # å¹¿å‘Šç§»é™¤
        "_convert_traditional_to_simplified" # ç¹ä½“è½¬ç®€ä½“
    ]

    def __init__(self, proxy_config: Optional[Dict[str, Any]] = None, novel_site_name: Optional[str] = None):
        """
        åˆå§‹åŒ–è§£æå™¨
        
        Args:
            proxy_config: ä»£ç†é…ç½®
            novel_site_name: ç½‘ç«™åç§°ï¼Œå¦‚æœæä¾›åˆ™è¦†ç›–é»˜è®¤åç§°
        """
        super().__init__(proxy_config, novel_site_name)
    
    def _detect_book_type(self, content: str) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹ä¹¦ç±ç±»å‹
        å¯¹äºcanovel.comç½‘ç«™ï¼Œå›ºå®šè¿”å›"å†…å®¹é¡µå†…åˆ†é¡µ"
        
        Args:
            content: é¡µé¢å†…å®¹
            
        Returns:
            ä¹¦ç±ç±»å‹
        """
        return "å†…å®¹é¡µå†…åˆ†é¡µ"
    
    def parse_novel_detail(self, novel_id: str) -> Dict[str, Any]:
        """
        é‡å†™è§£æå°è¯´è¯¦æƒ…æ–¹æ³•ï¼Œç›´æ¥å¤„ç†å†…å®¹é¡µå†…åˆ†é¡µ
        å¯¹äºcanovel.comç½‘ç«™ï¼Œä¹¦ç±URLç›´æ¥æ˜¯å†…å®¹é¡µ
        
        Args:
            novel_id: å°è¯´ID
            
        Returns:
            å°è¯´è¯¦æƒ…ä¿¡æ¯
        """
        # æ„å»ºå°è¯´URLï¼Œä¾‹å¦‚ï¼šhttps://canovel.com/article/1384
        novel_url = f"{self.base_url}/article/{novel_id}"
        
        # ç›´æ¥ä»å†…å®¹é¡µå¼€å§‹æŠ“å–
        return self._parse_content_pagination_novel_direct(novel_url, novel_id)
    
    def _parse_content_pagination_novel_direct(self, start_url: str, novel_id: str) -> Dict[str, Any]:
        """
        ç›´æ¥è§£æå†…å®¹é¡µå†…åˆ†é¡µæ¨¡å¼çš„å°è¯´
        ä¸éœ€è¦å…ˆè·å–é¦–é¡µï¼Œç›´æ¥ä»ç¬¬ä¸€é¡µå†…å®¹å¼€å§‹
        
        Args:
            start_url: èµ·å§‹å†…å®¹é¡µé¢URL
            novel_id: å°è¯´ID
            
        Returns:
            å°è¯´è¯¦æƒ…ä¿¡æ¯
        """
        # è·å–ç¬¬ä¸€é¡µå†…å®¹
        content = self._get_url_content(start_url)
        if not content:
            raise Exception(f"æ— æ³•è·å–å°è¯´é¡µé¢: {start_url}")
        
        # æå–æ ‡é¢˜
        title = self._extract_with_regex(content, self.title_reg)
        if not title:
            raise Exception("æ— æ³•æå–å°è¯´æ ‡é¢˜")
        
        print(f"å¼€å§‹å¤„ç† [ {title} ] - ç±»å‹: å†…å®¹é¡µå†…åˆ†é¡µ")
        
        # åˆ›å»ºå°è¯´å†…å®¹ç»“æ„
        novel_content = {
            'title': title,
            'author': self.novel_site_name,
            'novel_id': novel_id,
            'url': start_url,
            'chapters': []
        }
        
        # æŠ“å–æ‰€æœ‰å†…å®¹é¡µ
        self._get_all_content_pages_direct(start_url, novel_content)
        
        print(f'[ {title} ] å®Œæˆ')
        return novel_content
    
    def _get_all_content_pages_direct(self, start_url: str, novel_content: Dict[str, Any]) -> None:
        """
        ç›´æ¥æŠ“å–æ‰€æœ‰å†…å®¹é¡µé¢ï¼ˆä»ç¬¬ä¸€é¡µå¼€å§‹ï¼‰
        
        Args:
            start_url: èµ·å§‹å†…å®¹é¡µé¢URL
            novel_content: å°è¯´å†…å®¹å­—å…¸
        """
        current_url = start_url
        self.chapter_count = 0
        
        while current_url:
            self.chapter_count += 1
            print(f"æ­£åœ¨æŠ“å–ç¬¬ {self.chapter_count} é¡µ: {current_url}")
            
            # è·å–é¡µé¢å†…å®¹
            page_content = self._get_url_content(current_url)
            
            if page_content:
                # æå–å†…å®¹
                chapter_content = self._extract_with_regex(page_content, self.content_reg)
                
                if chapter_content:
                    # ç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„å†…å®¹æ¸…ç†æ–¹æ³•å¤„ç†å†…å®¹
                    processed_content = self._extract_balanced_content(chapter_content)
                    # æ‰§è¡Œçˆ¬å–åå¤„ç†å‡½æ•°
                    processed_content = self._execute_after_crawler_funcs(processed_content)
                    
                    novel_content['chapters'].append({
                        'chapter_number': self.chapter_count,
                        'title': f"ç¬¬ {self.chapter_count} é¡µ",
                        'content': processed_content,
                        'url': current_url
                    })
                    print(f"âˆš ç¬¬ {self.chapter_count} é¡µæŠ“å–æˆåŠŸ")
                else:
                    print(f"Ã— ç¬¬ {self.chapter_count} é¡µå†…å®¹æå–å¤±è´¥")
            else:
                print(f"Ã— ç¬¬ {self.chapter_count} é¡µæŠ“å–å¤±è´¥")
            
            # è·å–ä¸‹ä¸€é¡µURL
            next_url = self._get_next_page_url_direct(page_content, current_url)
            current_url = next_url
            
            # é¡µé¢é—´å»¶è¿Ÿ
            time.sleep(1)
    
    def _get_next_page_url_direct(self, content: str, current_url: str) -> Optional[str]:
        """
        è·å–ä¸‹ä¸€é¡µURL - é€‚é…canovel.comç½‘ç«™ç»“æ„
        
        Args:
            content: å½“å‰é¡µé¢å†…å®¹
            current_url: å½“å‰é¡µé¢URL
            
        Returns:
            ä¸‹ä¸€é¡µURLæˆ–None
        """
        if not content:
            return None
            
        # ä½¿ç”¨é…ç½®çš„æ­£åˆ™è¡¨è¾¾å¼æå–ä¸‹ä¸€é¡µé“¾æ¥
        if self.next_page_link_reg:
            for pattern in self.next_page_link_reg:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    next_url = match.group(1)
                    # æ„å»ºå®Œæ•´URL
                    if next_url.startswith('http'):
                        return next_url
                    else:
                        return next_url
        
        return None
    
    def _extract_balanced_content(self, content: str) -> str:
        """
        æå–å¹³è¡¡çš„å†…å®¹ï¼Œå¤„ç†åµŒå¥—divæ ‡ç­¾
        ä½¿ç”¨è´ªå©ªæ¨¡å¼åŒ¹é…ï¼Œç¡®ä¿å†…å®¹æŠ“å–å®Œæ•´
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            å¤„ç†åçš„å†…å®¹
        """
        # æ¸…ç†HTMLå†…å®¹
        content = self._clean_html_content(content)
        
        # æ‰¾åˆ°"ğŸŒ± æ±¤ç±³ä»”ä¼˜é€‰å¥½ç«™"çš„ä½ç½®ï¼Œæˆªæ–­ä¹‹åçš„æ‰€æœ‰å†…å®¹
        emoji_pattern = r'ğŸŒ± æ±¤ç±³ä»”ä¼˜é€‰å¥½ç«™'
        if emoji_pattern in content:
            content = content.split(emoji_pattern)[0]
            content = content.strip()
        
        # æ‰¾åˆ°"æ±¤ç±³ä»”ä¼˜é€‰å¥½ç«™"çš„ä½ç½®ï¼Œæˆªæ–­ä¹‹åçš„æ‰€æœ‰å†…å®¹
        pattern = r'æ±¤ç±³ä»”ä¼˜é€‰å¥½ç«™'
        if pattern in content:
            content = content.split(pattern)[0]
            content = content.strip()
        
        # ç§»é™¤"ğŸŒ±"ä¹‹åçš„æ‰€æœ‰å†…å®¹
        emoji_single_pattern = r'ğŸŒ±'
        if emoji_single_pattern in content:
            content = content.split(emoji_single_pattern)[0]
            content = content.strip()
        
        # ç§»é™¤é¡µç å’Œå¯¼èˆªä¿¡æ¯
        content = re.sub(r'ç¬¬\s*\d+\s*é¡µ\s*/\s*å…±\s*\d+\s*é¡µ', '', content)
        
        # ç§»é™¤"You may also like"ç›¸å…³å†…å®¹
        content = re.sub(r'You may also like.*$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤è­¦å‘Šä¿¡æ¯
        content = re.sub(r'è­¦å‘Šï¼šæœ¬ç½‘åªä¾›18å²ä»¥ä¸Šäººå£«æµè§ˆ.*$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤ç‰ˆæƒä¿¡æ¯å’Œå‹ç«™é“¾æ¥
        content = re.sub(r'Â© 2025.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        content = re.sub(r'å‹ç«™è¿ç»“.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        content = re.sub(r'æ ‡ç±¤äº‘.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤"Tags:"ä¹‹åçš„å†…å®¹
        content = re.sub(r'Tags:.*$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤"Previous story"ä¹‹åçš„å†…å®¹
        content = re.sub(r'Previous story.*$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤æœ«å°¾çš„ç©ºæ ¼å’Œæ¢è¡Œ
        content = content.strip()
        
        return content